{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_datasets = ['GSM8K', 'date', 'MultiArith', 'ASDiv', 'SVAMP', 'AQUA', 'StrategyQA','p_GSM8K', 'commonsenseQA','SPARTQA']\n",
    "json_datasets = ['logical_deduction_seven_objects', 'reasoning_about_colored_objects']\n",
    "all_datasets = jsonl_datasets + json_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dataset: logical_deduction_seven_objects\n",
      "Input CSV file not found: /Users/log/Github/textual_grounding/logan/results/final/VanillaCoT/logical_deduction_seven_objects/llama3.170b/zero_shot_vanilla_cot_None_logical_deduction_seven_objects_llama3.170b.csv\n",
      "\n",
      "Processing dataset: reasoning_about_colored_objects\n",
      "Total QA Pairs Parsed: 75\n",
      "Total Ground Truth Entries: 250\n",
      "\n",
      "--- reasoning_about_colored_objects Summary ---\n",
      "Accuracy: 100.00% (75/75 correct)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "\n",
    "def parse_csv_file(file_path):\n",
    "    qa_pairs = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            question = row.get('question', 'No question found.').strip()\n",
    "            answer_text = row.get('answer', 'No answer found.').strip()\n",
    "            id_ = row.get('id')\n",
    "            if id_ is not None:\n",
    "                id_str = str(id_).strip()  # Ensure ID is treated as a string\n",
    "                if id_str:\n",
    "                    qa_pairs.append((id_str, question, answer_text))\n",
    "                else:\n",
    "                    print(f\"Skipping a row due to empty 'id': {row}\")\n",
    "            else:\n",
    "                # Handle cases without 'id' by skipping\n",
    "                print(f\"Skipping a row due to missing 'id': {row}\")\n",
    "    return qa_pairs\n",
    "\n",
    "def read_ground_truth(path):\n",
    "    ground_truth = {}\n",
    "    file_extension = os.path.splitext(path)[1].lower()\n",
    "    \n",
    "    try:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            # If the file is a JSON array (.json), load it directly\n",
    "            if file_extension == '.json':\n",
    "                data_list = json.load(f)\n",
    "            # If the file is JSONL, read line-by-line\n",
    "            elif file_extension == '.jsonl':\n",
    "                data_list = [json.loads(line) for line in f]\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported file format. Please provide a .json or .jsonl file.\")\n",
    "        \n",
    "        # Process each entry in data_list (which is a list of dicts)\n",
    "        for data in data_list:\n",
    "            id_ = data.get('id')\n",
    "            answer = data.get('answer')\n",
    "            if id_ is not None and answer is not None:\n",
    "                id_str = str(id_).strip()  # Ensure ID is treated as a string\n",
    "                answer_str = str(answer).lower().strip()\n",
    "                # GSM ONLY: Assuming the answer is split by '####' and the relevant part is after it\n",
    "                if '####' in answer_str:\n",
    "                    answer_str = answer_str.split('####')[1].strip()\n",
    "                # Extract numbers only, as per original code\n",
    "                numbers_only = re.sub(r'[^0-9]', '', answer_str)\n",
    "                ground_truth[id_str] = numbers_only\n",
    "            else:\n",
    "                print(f\"Invalid ground truth entry (missing 'id' or 'answer'): {data}\")\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "    except ValueError as ve:\n",
    "        print(ve)\n",
    "    \n",
    "    return ground_truth\n",
    "\n",
    "def extract_final_answer(answer_text):\n",
    "    \"\"\"\n",
    "    Extracts the final answer from the answer_text.\n",
    "    Assumes the final answer is within the last pair of curly braces {}.\n",
    "    \"\"\"\n",
    "    final_answer_match = re.search(r'\\{([^}]+)\\}(?=[^}]*$)', answer_text, re.DOTALL)\n",
    "    if final_answer_match:\n",
    "        extracted = final_answer_match.group(1)\n",
    "        # Remove commas, dollar signs, and trim whitespace\n",
    "        final_answer = re.sub(r'[^\\d.]', '', extracted).strip().lower()\n",
    "        # Handle boolean answers if applicable\n",
    "        if \"no\" in final_answer or \"false\" in final_answer:\n",
    "            final_answer = \"false\"\n",
    "        elif \"yes\" in final_answer or \"true\" in final_answer:\n",
    "            final_answer = \"true\"\n",
    "        return final_answer\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def compute_accuracy(qa_pairs, ground_truth):\n",
    "    correct_answers = 0\n",
    "    total_answers = 0\n",
    "    mismatches = []  # Optional: To store mismatched cases for further inspection\n",
    "\n",
    "    for id_, question, answer_text in qa_pairs:\n",
    "        final_answer = extract_final_answer(answer_text)\n",
    "        gt_answer = ground_truth.get(id_)\n",
    "\n",
    "        if gt_answer is None:\n",
    "            print(f\"Ground truth not available for ID '{id_}'. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        is_correct = final_answer == gt_answer\n",
    "        if is_correct:\n",
    "            correct_answers += 1\n",
    "        else:\n",
    "            mismatches.append((id_, question, final_answer, gt_answer))\n",
    "        total_answers += 1\n",
    "\n",
    "    accuracy_percentage = (correct_answers / total_answers * 100) if total_answers > 0 else 0\n",
    "    return correct_answers, total_answers, accuracy_percentage, mismatches\n",
    "\n",
    "def main():\n",
    "    for dataset in json_datasets:\n",
    "        input_csv = f'/Users/log/Github/textual_grounding/logan/results/final/VanillaCoT/{dataset}/llama3.170b/zero_shot_vanilla_cot_None_{dataset}_llama3.170b.csv' \n",
    "        \n",
    "        if dataset in jsonl_datasets: \n",
    "            ground_truth_file = f'/Users/log/Github/textual_grounding/data/{dataset}/test.jsonl'  \n",
    "        else:\n",
    "            ground_truth_file = f'/Users/log/Github/textual_grounding/data/{dataset}/test.json'\n",
    "\n",
    "        print(f\"\\nProcessing dataset: {dataset}\")\n",
    "\n",
    "        # Check if input files exist\n",
    "        if not os.path.isfile(input_csv):\n",
    "            print(f\"Input CSV file not found: {input_csv}\")\n",
    "            continue  # Continue to the next dataset instead of exiting\n",
    "        if not os.path.isfile(ground_truth_file):\n",
    "            print(f\"Ground truth JSONL file not found: {ground_truth_file}\")\n",
    "            continue  # Continue to the next dataset instead of exiting\n",
    "\n",
    "        # Parse the input CSV file to extract IDs, questions, and answers\n",
    "        qa_pairs = parse_csv_file(input_csv)\n",
    "        print(f\"Total QA Pairs Parsed: {len(qa_pairs)}\")  # Debug: Print the number of QA pairs parsed\n",
    "\n",
    "        # Read the ground truth answers\n",
    "        ground_truth = read_ground_truth(ground_truth_file)\n",
    "        print(f\"Total Ground Truth Entries: {len(ground_truth)}\")  # Debug: Print the number of ground truth entries\n",
    "\n",
    "        # Check if any QA pairs were found\n",
    "        if not qa_pairs:\n",
    "            print(\"No question-answer pairs were found in the input file.\")\n",
    "            continue  # Continue to the next dataset\n",
    "\n",
    "        # Compute accuracy\n",
    "        correct, total, accuracy, mismatches = compute_accuracy(qa_pairs, ground_truth)\n",
    "\n",
    "        # Print the accuracy\n",
    "        print(f\"\\n--- {dataset} Summary ---\")\n",
    "        print(f\"Accuracy: {accuracy:.2f}% ({correct}/{total} correct)\")\n",
    "\n",
    "        # Optional: Print mismatched cases for debugging\n",
    "        # if mismatches:\n",
    "        #     print(\"\\n--- Mismatched Cases ---\")\n",
    "        #     for id_, question, predicted, actual in mismatches:\n",
    "        #         print(f\"ID: {id_}\")\n",
    "        #         print(f\"Question: {question}\")\n",
    "        #         print(f\"Predicted Answer: {predicted}\")\n",
    "        #         print(f\"Ground Truth Answer: {actual}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tin Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  logical_deduction_seven_objects\n",
      "216 250\n",
      "Accuracy:  0.864\n",
      "------------------------------------\n",
      "Dataset:  reasoning_about_colored_objects\n",
      "249 250\n",
      "Accuracy:  0.996\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import importlib\n",
    "\n",
    "# Add the directory containing eval_mmlu.py to the Python path\n",
    "sys.path.append('/Users/log/Github/textual_grounding/utils') \n",
    "\n",
    "# Import eval_mmlu as a module\n",
    "import eval_mmlu\n",
    "import utils\n",
    "import mmlu\n",
    "# import eval_da_and_cot\n",
    "\n",
    "# Reload the eval_mmlu module to ensure the latest changes are loaded (optional)\n",
    "importlib.reload(eval_mmlu)\n",
    "importlib.reload(utils)\n",
    "importlib.reload(mmlu)\n",
    "# importlib.reload(eval_da_and_cot)\n",
    "\n",
    "# Now, you can access the evaluate_model function from eval_mmlu\n",
    "llm_model = \"gpt-4o-2024-08-06\"\n",
    "data_mode = \"longest\"\n",
    "answer_mode = \"cot\"\n",
    "dataset = \"AQUA\"\n",
    "\n",
    "# Call the function from the module\n",
    "for dataset in json_datasets:\n",
    "    eval_mmlu.evaluate_model(llm_model, data_mode, answer_mode, dataset)\n",
    "    # eval_da_and_cot.evaluate_model(llm_model, data_mode, answer_mode, dataset)\n",
    "# print(eval_mmlu.evaluate_model(llm_model, data_mode, answer_mode, 'AQUA'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
