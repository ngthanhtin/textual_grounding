{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_datasets = ['GSM8K', 'p_GSM8K', 'MultiArith', 'ASDiv', 'SVAMP', 'AQUA', 'StrategyQA', 'commonsenseQA','SPARTQA']\n",
    "json_datasets = ['logical_deduction_seven_objects', 'reasoning_about_colored_objects']\n",
    "all_datasets = jsonl_datasets + json_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dataset: logical_deduction_seven_objects\n",
      "Input CSV file not found: /Users/log/Github/textual_grounding/logan/results/final/VanillaCoT/logical_deduction_seven_objects/llama3.170b/zero_shot_vanilla_cot_None_logical_deduction_seven_objects_llama3.170b.csv\n",
      "\n",
      "Processing dataset: reasoning_about_colored_objects\n",
      "Total QA Pairs Parsed: 75\n",
      "Total Ground Truth Entries: 250\n",
      "\n",
      "--- reasoning_about_colored_objects Summary ---\n",
      "Accuracy: 100.00% (75/75 correct)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "\n",
    "def parse_csv_file(file_path):\n",
    "    qa_pairs = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            question = row.get('question', 'No question found.').strip()\n",
    "            answer_text = row.get('answer', 'No answer found.').strip()\n",
    "            id_ = row.get('id')\n",
    "            if id_ is not None:\n",
    "                id_str = str(id_).strip()  # Ensure ID is treated as a string\n",
    "                if id_str:\n",
    "                    qa_pairs.append((id_str, question, answer_text))\n",
    "                else:\n",
    "                    print(f\"Skipping a row due to empty 'id': {row}\")\n",
    "            else:\n",
    "                # Handle cases without 'id' by skipping\n",
    "                print(f\"Skipping a row due to missing 'id': {row}\")\n",
    "    return qa_pairs\n",
    "\n",
    "def read_ground_truth(path):\n",
    "    ground_truth = {}\n",
    "    file_extension = os.path.splitext(path)[1].lower()\n",
    "    \n",
    "    try:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            # If the file is a JSON array (.json), load it directly\n",
    "            if file_extension == '.json':\n",
    "                data_list = json.load(f)\n",
    "            # If the file is JSONL, read line-by-line\n",
    "            elif file_extension == '.jsonl':\n",
    "                data_list = [json.loads(line) for line in f]\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported file format. Please provide a .json or .jsonl file.\")\n",
    "        \n",
    "        # Process each entry in data_list (which is a list of dicts)\n",
    "        for data in data_list:\n",
    "            id_ = data.get('id')\n",
    "            answer = data.get('answer')\n",
    "            if id_ is not None and answer is not None:\n",
    "                id_str = str(id_).strip()  # Ensure ID is treated as a string\n",
    "                answer_str = str(answer).lower().strip()\n",
    "                # GSM ONLY: Assuming the answer is split by '####' and the relevant part is after it\n",
    "                if '####' in answer_str:\n",
    "                    answer_str = answer_str.split('####')[1].strip()\n",
    "                # Extract numbers only, as per original code\n",
    "                numbers_only = re.sub(r'[^0-9]', '', answer_str)\n",
    "                ground_truth[id_str] = numbers_only\n",
    "            else:\n",
    "                print(f\"Invalid ground truth entry (missing 'id' or 'answer'): {data}\")\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "    except ValueError as ve:\n",
    "        print(ve)\n",
    "    \n",
    "    return ground_truth\n",
    "\n",
    "def extract_final_answer(answer_text):\n",
    "    \"\"\"\n",
    "    Extracts the final answer from the answer_text.\n",
    "    Assumes the final answer is within the last pair of curly braces {}.\n",
    "    \"\"\"\n",
    "    final_answer_match = re.search(r'\\{([^}]+)\\}(?=[^}]*$)', answer_text, re.DOTALL)\n",
    "    if final_answer_match:\n",
    "        extracted = final_answer_match.group(1)\n",
    "        # Remove commas, dollar signs, and trim whitespace\n",
    "        final_answer = re.sub(r'[^\\d.]', '', extracted).strip().lower()\n",
    "        # Handle boolean answers if applicable\n",
    "        if \"no\" in final_answer or \"false\" in final_answer:\n",
    "            final_answer = \"false\"\n",
    "        elif \"yes\" in final_answer or \"true\" in final_answer:\n",
    "            final_answer = \"true\"\n",
    "        return final_answer\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def compute_accuracy(qa_pairs, ground_truth):\n",
    "    correct_answers = 0\n",
    "    total_answers = 0\n",
    "    mismatches = []  # Optional: To store mismatched cases for further inspection\n",
    "\n",
    "    for id_, question, answer_text in qa_pairs:\n",
    "        final_answer = extract_final_answer(answer_text)\n",
    "        gt_answer = ground_truth.get(id_)\n",
    "\n",
    "        if gt_answer is None:\n",
    "            print(f\"Ground truth not available for ID '{id_}'. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        is_correct = final_answer == gt_answer\n",
    "        if is_correct:\n",
    "            correct_answers += 1\n",
    "        else:\n",
    "            mismatches.append((id_, question, final_answer, gt_answer))\n",
    "        total_answers += 1\n",
    "\n",
    "    accuracy_percentage = (correct_answers / total_answers * 100) if total_answers > 0 else 0\n",
    "    return correct_answers, total_answers, accuracy_percentage, mismatches\n",
    "\n",
    "def main():\n",
    "    for dataset in json_datasets:\n",
    "        input_csv = f'/Users/log/Github/textual_grounding/logan/results/final/VanillaCoT/{dataset}/llama3.170b/zero_shot_vanilla_cot_None_{dataset}_llama3.170b.csv' \n",
    "        \n",
    "        if dataset in jsonl_datasets: \n",
    "            ground_truth_file = f'/Users/log/Github/textual_grounding/data/{dataset}/test.jsonl'  \n",
    "        else:\n",
    "            ground_truth_file = f'/Users/log/Github/textual_grounding/data/{dataset}/test.json'\n",
    "\n",
    "        print(f\"\\nProcessing dataset: {dataset}\")\n",
    "\n",
    "        # Check if input files exist\n",
    "        if not os.path.isfile(input_csv):\n",
    "            print(f\"Input CSV file not found: {input_csv}\")\n",
    "            continue  # Continue to the next dataset instead of exiting\n",
    "        if not os.path.isfile(ground_truth_file):\n",
    "            print(f\"Ground truth JSONL file not found: {ground_truth_file}\")\n",
    "            continue  # Continue to the next dataset instead of exiting\n",
    "\n",
    "        # Parse the input CSV file to extract IDs, questions, and answers\n",
    "        qa_pairs = parse_csv_file(input_csv)\n",
    "        print(f\"Total QA Pairs Parsed: {len(qa_pairs)}\")  # Debug: Print the number of QA pairs parsed\n",
    "\n",
    "        # Read the ground truth answers\n",
    "        ground_truth = read_ground_truth(ground_truth_file)\n",
    "        print(f\"Total Ground Truth Entries: {len(ground_truth)}\")  # Debug: Print the number of ground truth entries\n",
    "\n",
    "        # Check if any QA pairs were found\n",
    "        if not qa_pairs:\n",
    "            print(\"No question-answer pairs were found in the input file.\")\n",
    "            continue  # Continue to the next dataset\n",
    "\n",
    "        # Compute accuracy\n",
    "        correct, total, accuracy, mismatches = compute_accuracy(qa_pairs, ground_truth)\n",
    "\n",
    "        # Print the accuracy\n",
    "        print(f\"\\n--- {dataset} Summary ---\")\n",
    "        print(f\"Accuracy: {accuracy:.2f}% ({correct}/{total} correct)\")\n",
    "\n",
    "        # Optional: Print mismatched cases for debugging\n",
    "        # if mismatches:\n",
    "        #     print(\"\\n--- Mismatched Cases ---\")\n",
    "        #     for id_, question, predicted, actual in mismatches:\n",
    "        #         print(f\"ID: {id_}\")\n",
    "        #         print(f\"Question: {question}\")\n",
    "        #         print(f\"Predicted Answer: {predicted}\")\n",
    "        #         print(f\"Ground Truth Answer: {actual}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tin Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: GSM8K\n",
      "Accuracy:  0.49373433583959897\n",
      "------------------------------------\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/log/Github/textual_grounding/data/p_GSM8K/r-gsm.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Call the function from the module\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m all_datasets:\n\u001b[0;32m---> 21\u001b[0m     eval_mmlu\u001b[38;5;241m.\u001b[39mevaluate_model(llm_model, data_mode, answer_mode, dataset)\n",
      "File \u001b[0;32m~/Github/textual_grounding/utils/eval_mmlu.py:395\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(llm_model, data_mode, answer_mode, dataset)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdesign_1_v4\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;66;03m# remove all the tags in the answer\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     answers \u001b[38;5;241m=\u001b[39m [re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m</?fact\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m answers]\n\u001b[0;32m--> 395\u001b[0m gts \u001b[38;5;241m=\u001b[39m retrieve_gts(data_path, ids, dataset)\n\u001b[1;32m    397\u001b[0m compute_acc(questions, answers, gts, dataset)\n",
      "File \u001b[0;32m~/Github/textual_grounding/utils/utils.py:341\u001b[0m, in \u001b[0;36mretrieve_gts\u001b[0;34m(data_path, ids, dataset)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve_gts\u001b[39m(data_path, ids, dataset):\n\u001b[1;32m    336\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m    data_path: path to the whole data file\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;124;03m    ids: ids of the predictions (list)\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m    dataset: which dataset to retrieve the ground truth\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 341\u001b[0m     data \u001b[38;5;241m=\u001b[39m read_jsonl_file(data_path)\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;66;03m# read gt\u001b[39;00m\n\u001b[1;32m    343\u001b[0m     gts \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Github/textual_grounding/utils/utils.py:72\u001b[0m, in \u001b[0;36mread_jsonl_file\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_jsonl_file\u001b[39m(filepath):\n\u001b[1;32m     71\u001b[0m     data \u001b[38;5;241m=\u001b[39m [] \n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filepath, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[1;32m     74\u001b[0m             json_obj \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(line)  \u001b[38;5;66;03m# Parse the JSON data from the line\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/log/Github/textual_grounding/data/p_GSM8K/r-gsm.jsonl'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import importlib\n",
    "\n",
    "# Add the directory containing eval_mmlu.py to the Python path\n",
    "sys.path.append('/Users/log/Github/textual_grounding/utils')  # Ensure this path is correct\n",
    "\n",
    "# Import eval_mmlu as a module\n",
    "import eval_mmlu\n",
    "\n",
    "# Reload the eval_mmlu module to ensure the latest changes are loaded (optional)\n",
    "importlib.reload(eval_mmlu)\n",
    "\n",
    "# Now, you can access the evaluate_model function from eval_mmlu\n",
    "llm_model = \"llama3.170b\"\n",
    "data_mode = \"longest\"\n",
    "answer_mode = \"cot\"\n",
    "dataset = \"AQUA\"\n",
    "\n",
    "# Call the function from the module\n",
    "for dataset in all_datasets:\n",
    "    eval_mmlu.evaluate_model(llm_model, data_mode, answer_mode, dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
