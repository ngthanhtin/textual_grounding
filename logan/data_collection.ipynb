{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import os\n",
    "import sys\n",
    "import anthropic\n",
    "import ollama\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from google.generativeai.types import RequestOptions\n",
    "from google.api_core import retry\n",
    "from typing import List, Tuple\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import datetime\n",
    "import openai\n",
    "import time\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, TimeoutError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Shot - Vanilla CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_vanilla_cot = \"\"\"\n",
    "Think through your answer step by step. Put the concise form of your final answer in curly brackets e.g. {A}, {True} or {False}.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(prompt_type: str, few_shot_prompt: str, question: str) -> str:\n",
    "    prompts = {\n",
    "        \"zero_shot_vanilla_cot\": f\"{question}\\n{zero_shot_vanilla_cot}\",\n",
    "    }\n",
    "    return prompts[prompt_type]\n",
    "\n",
    "def save_results(save_path: str, ids: List[str], questions: List[str], answers: List[str], append: bool = False):\n",
    "    df = pd.DataFrame({'id': ids, 'question': questions, 'answer': answers})\n",
    "    if append and os.path.exists(save_path):\n",
    "        df.to_csv(save_path, mode='a', index=False, header=False)\n",
    "    else:\n",
    "        df.to_csv(save_path, index=False)\n",
    "\n",
    "def read_jsonl_file(filepath: str) -> List[dict]:\n",
    "    data = []\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            json_obj = json.loads(line)\n",
    "            data.append(json_obj)\n",
    "    return data\n",
    "\n",
    "def load_data_size_specific(data_path: str, sample_size: int = 0, random_seed: int = 0):\n",
    "    random.seed(random_seed)\n",
    "\n",
    "    if data_path.endswith('.jsonl'):\n",
    "        data = read_jsonl_file(data_path)\n",
    "    elif data_path.endswith('.json'):\n",
    "        with open(data_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "    \n",
    "    question_length = 0\n",
    "    eligible_data = [x for x in data if len(x[\"question\"]) >= question_length]\n",
    "    \n",
    "    if sample_size > 0 and sample_size < len(eligible_data):\n",
    "        sampled_data = random.sample(eligible_data, sample_size)\n",
    "    else:\n",
    "        sampled_data = eligible_data\n",
    "    \n",
    "    ids = [x[\"id\"] for x in sampled_data]\n",
    "    questions = [x[\"question\"] for x in sampled_data]\n",
    "    \n",
    "    return ids, questions\n",
    "\n",
    "def load_already_answered_ids(save_path: str) -> set:\n",
    "    if os.path.exists(save_path):\n",
    "        df = pd.read_csv(save_path)\n",
    "        answered_ids = set(df['id'].tolist())\n",
    "        # print(f\"Loaded {len(answered_ids)} already answered IDs from: {save_path}\")\n",
    "        print(f\"Already answered IDs: {answered_ids}\")\n",
    "        return answered_ids\n",
    "    else:\n",
    "        print(f\"No existing save file found at: {save_path}. Starting fresh.\")\n",
    "        return set()\n",
    "\n",
    "def initialize_save_file(save_path: str):\n",
    "    if not os.path.exists(save_path):\n",
    "        # Create an empty DataFrame with headers and save\n",
    "        df = pd.DataFrame(columns=['id', 'question', 'answer'])\n",
    "        df.to_csv(save_path, index=False)\n",
    "        print(f\"Initialized new save file with headers at: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_4o(prompt: str) -> str:\n",
    "    client = OpenAI()\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-2024-08-06\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{prompt}\"\n",
    "            }\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "def query_llama(prompt):\n",
    "    client = openai.OpenAI(\n",
    "        api_key=os.environ.get(\"SAMBANOVA_API_KEY\"),\n",
    "        base_url=\"https://api.sambanova.ai/v1\",\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model='Meta-Llama-3.1-8B-Instruct',\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "        temperature=0.6, # Meta default\n",
    "        top_p = 0.9 # Meta default\n",
    "    )\n",
    "    time.sleep(2)  # Pause execution for 2 seconds\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def query_llama_70b(prompt):\n",
    "    client = openai.OpenAI(\n",
    "        api_key=os.environ.get(\"SAMBANOVA_API_KEY\"),\n",
    "        base_url=\"https://api.sambanova.ai/v1\",\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model='Meta-Llama-3.1-70B-Instruct',\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "        temperature=0.6, # Meta default\n",
    "        top_p = 0.9 # Meta default\n",
    "    )\n",
    "    time.sleep(2)  # Pause execution for 2 seconds\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm(llm_model: str, ids: List[str], questions: List[str], few_shot_prompt: str, prompt_type: str, save_path: str, already_answered_ids: set) -> Tuple[List[str], List[str], List[str]]:\n",
    "    answers = []\n",
    "    ids_can_be_answered = []\n",
    "    questions_can_be_answered = []\n",
    "    \n",
    "    for id, q in tqdm(zip(ids, questions), total=len(ids)):\n",
    "        # print(q)\n",
    "        # print(f\"Processing ID: {id}\")\n",
    "        if id in already_answered_ids:\n",
    "            print(f\"Skipping: {id}\", end=' ')\n",
    "            continue\n",
    "        if id == 1146: # weird ID that breaks llama\n",
    "            continue\n",
    "        \n",
    "        prompt = get_prompt(prompt_type, few_shot_prompt, q)\n",
    "        try:\n",
    "            if llm_model == 'gemini':\n",
    "                answer = query_gemini(prompt, id)\n",
    "            elif llm_model == 'claude':\n",
    "                answer = query_claude(prompt)\n",
    "            elif llm_model == '4o':\n",
    "                # answer = query_4o_multiturn(prompt)\n",
    "                if prompt_type == 'multi_convo':\n",
    "                    fact_prompt = get_prompt(prompt_type=\"fact_prompt\", few_shot_prompt=\"\", question=q)\n",
    "                    \n",
    "                    answer_prompt = get_prompt(prompt_type=\"answer_prompt_data\", few_shot_prompt=\"\", question=q)\n",
    "                    answer = query_4o_multiconvo(fact_prompt=fact_prompt, answer_prompt=answer_prompt, extracted_question=q)\n",
    "                else:\n",
    "                    answer = query_4o(prompt)\n",
    "                \n",
    "            elif llm_model == 'llama3.18b':\n",
    "                answer = query_llama(prompt)\n",
    "            elif llm_model == 'llama3.170b':\n",
    "                answer = query_llama_70b(prompt)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported LLM model: {llm_model}\")\n",
    "            # print(f\"Answer for ID {id}: {answer}\")\n",
    "            \n",
    "            answers.append(answer)\n",
    "            questions_can_be_answered.append(q)\n",
    "            ids_can_be_answered.append(id)\n",
    "\n",
    "            # Save after each answer\n",
    "            save_results(save_path, [id], [q], [answer], append=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question {id}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return ids_can_be_answered, questions_can_be_answered, answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_datasets = ['logical_deduction_seven_objects','reasoning_about_colored_objects', 'wikimultihopQA']\n",
    "jsonl_datasets = ['GSM8K', 'date', 'MultiArith', 'ASDiv', 'SVAMP', 'AQUA', 'p_GSM8K', 'StrategyQA', 'commonsenseQA','SPARTQA']\n",
    "all_datasets = json_datasets + jsonl_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(llm_model, prompt_type, few_shot_txt, sample_size, project_root):\n",
    "    for dataset in all_datasets:\n",
    "        print(f\"------Processing dataset: {dataset}-------\")\n",
    "        if few_shot_txt:\n",
    "            fewshot_prompt_path = os.path.join(project_root, \"prompt\", dataset, few_shot_txt)\n",
    "            \n",
    "        save_dir = os.path.join(project_root, 'logan/results/final/VanillaCoT', dataset, f'{llm_model}')\n",
    "        os.makedirs(save_dir, exist_ok=True)  # Ensure the directory exists\n",
    "        save_path = os.path.join(save_dir, f'{prompt_type}_{few_shot_txt}_{dataset}_{llm_model}.csv')\n",
    "\n",
    "        if dataset in json_datasets:\n",
    "            data_path = os.path.join(project_root, 'data', dataset, 'test.json')\n",
    "        else:\n",
    "            data_path = os.path.join(project_root, 'data', dataset, 'test.jsonl')\n",
    "\n",
    "\n",
    "        ids, questions = load_data_size_specific(data_path, sample_size=sample_size)\n",
    "        if few_shot_txt:\n",
    "            with open(fewshot_prompt_path, 'r') as file:\n",
    "                few_shot_prompt = file.read()\n",
    "        else:\n",
    "            few_shot_prompt = \"\"\n",
    "\n",
    "        initialize_save_file(save_path)\n",
    "        already_answered_ids = load_already_answered_ids(save_path)\n",
    "\n",
    "        ids_answered, questions_answered, answers = query_llm(\n",
    "            llm_model=llm_model,\n",
    "            ids=ids,\n",
    "            questions=questions,\n",
    "            few_shot_prompt=few_shot_prompt,\n",
    "            prompt_type=prompt_type,\n",
    "            save_path=save_path,\n",
    "            already_answered_ids=already_answered_ids\n",
    "        )\n",
    "\n",
    "        print(f\"Processing complete for {dataset}. {len(ids_answered)} new answers saved to {save_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Processing dataset: logical_deduction_seven_objects-------\n",
      "Already answered IDs: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:00<00:00, 593757.64it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping: 0 Skipping: 1 Skipping: 2 Skipping: 3 Skipping: 4 Skipping: 5 Skipping: 6 Skipping: 7 Skipping: 8 Skipping: 9 Skipping: 10 Skipping: 11 Skipping: 12 Skipping: 13 Skipping: 14 Skipping: 15 Skipping: 16 Skipping: 17 Skipping: 18 Skipping: 19 Skipping: 20 Skipping: 21 Skipping: 22 Skipping: 23 Skipping: 24 Skipping: 25 Skipping: 26 Skipping: 27 Skipping: 28 Skipping: 29 Skipping: 30 Skipping: 31 Skipping: 32 Skipping: 33 Skipping: 34 Skipping: 35 Skipping: 36 Skipping: 37 Skipping: 38 Skipping: 39 Skipping: 40 Skipping: 41 Skipping: 42 Skipping: 43 Skipping: 44 Skipping: 45 Skipping: 46 Skipping: 47 Skipping: 48 Skipping: 49 Skipping: 50 Skipping: 51 Skipping: 52 Skipping: 53 Skipping: 54 Skipping: 55 Skipping: 56 Skipping: 57 Skipping: 58 Skipping: 59 Skipping: 60 Skipping: 61 Skipping: 62 Skipping: 63 Skipping: 64 Skipping: 65 Skipping: 66 Skipping: 67 Skipping: 68 Skipping: 69 Skipping: 70 Skipping: 71 Skipping: 72 Skipping: 73 Skipping: 74 Skipping: 75 Skipping: 76 Skipping: 77 Skipping: 78 Skipping: 79 Skipping: 80 Skipping: 81 Skipping: 82 Skipping: 83 Skipping: 84 Skipping: 85 Skipping: 86 Skipping: 87 Skipping: 88 Skipping: 89 Skipping: 90 Skipping: 91 Skipping: 92 Skipping: 93 Skipping: 94 Skipping: 95 Skipping: 96 Skipping: 97 Skipping: 98 Skipping: 99 Skipping: 100 Skipping: 101 Skipping: 102 Skipping: 103 Skipping: 104 Skipping: 105 Skipping: 106 Skipping: 107 Skipping: 108 Skipping: 109 Skipping: 110 Skipping: 111 Skipping: 112 Skipping: 113 Skipping: 114 Skipping: 115 Skipping: 116 Skipping: 117 Skipping: 118 Skipping: 119 Skipping: 120 Skipping: 121 Skipping: 122 Skipping: 123 Skipping: 124 Skipping: 125 Skipping: 126 Skipping: 127 Skipping: 128 Skipping: 129 Skipping: 130 Skipping: 131 Skipping: 132 Skipping: 133 Skipping: 134 Skipping: 135 Skipping: 136 Skipping: 137 Skipping: 138 Skipping: 139 Skipping: 140 Skipping: 141 Skipping: 142 Skipping: 143 Skipping: 144 Skipping: 145 Skipping: 146 Skipping: 147 Skipping: 148 Skipping: 149 Skipping: 150 Skipping: 151 Skipping: 152 Skipping: 153 Skipping: 154 Skipping: 155 Skipping: 156 Skipping: 157 Skipping: 158 Skipping: 159 Skipping: 160 Skipping: 161 Skipping: 162 Skipping: 163 Skipping: 164 Skipping: 165 Skipping: 166 Skipping: 167 Skipping: 168 Skipping: 169 Skipping: 170 Skipping: 171 Skipping: 172 Skipping: 173 Skipping: 174 Skipping: 175 Skipping: 176 Skipping: 177 Skipping: 178 Skipping: 179 Skipping: 180 Skipping: 181 Skipping: 182 Skipping: 183 Skipping: 184 Skipping: 185 Skipping: 186 Skipping: 187 Skipping: 188 Skipping: 189 Skipping: 190 Skipping: 191 Skipping: 192 Skipping: 193 Skipping: 194 Skipping: 195 Skipping: 196 Skipping: 197 Skipping: 198 Skipping: 199 Skipping: 200 Skipping: 201 Skipping: 202 Skipping: 203 Skipping: 204 Skipping: 205 Skipping: 206 Skipping: 207 Skipping: 208 Skipping: 209 Skipping: 210 Skipping: 211 Skipping: 212 Skipping: 213 Skipping: 214 Skipping: 215 Skipping: 216 Skipping: 217 Skipping: 218 Skipping: 219 Skipping: 220 Skipping: 221 Skipping: 222 Skipping: 223 Skipping: 224 Skipping: 225 Skipping: 226 Skipping: 227 Skipping: 228 Skipping: 229 Skipping: 230 Skipping: 231 Skipping: 232 Skipping: 233 Skipping: 234 Skipping: 235 Skipping: 236 Skipping: 237 Skipping: 238 Skipping: 239 Skipping: 240 Skipping: 241 Skipping: 242 Skipping: 243 Skipping: 244 Skipping: 245 Skipping: 246 Skipping: 247 Skipping: 248 Skipping: 249 Processing complete for logical_deduction_seven_objects. 0 new answers saved to /Users/log/Github/textual_grounding/logan/results/final/VanillaCoT/logical_deduction_seven_objects/llama3.18b/zero_shot_vanilla_cot_None_logical_deduction_seven_objects_llama3.18b.csv.\n",
      "------Processing dataset: reasoning_about_colored_objects-------\n",
      "Already answered IDs: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:00<00:00, 662816.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping: 0 Skipping: 1 Skipping: 2 Skipping: 3 Skipping: 4 Skipping: 5 Skipping: 6 Skipping: 7 Skipping: 8 Skipping: 9 Skipping: 10 Skipping: 11 Skipping: 12 Skipping: 13 Skipping: 14 Skipping: 15 Skipping: 16 Skipping: 17 Skipping: 18 Skipping: 19 Skipping: 20 Skipping: 21 Skipping: 22 Skipping: 23 Skipping: 24 Skipping: 25 Skipping: 26 Skipping: 27 Skipping: 28 Skipping: 29 Skipping: 30 Skipping: 31 Skipping: 32 Skipping: 33 Skipping: 34 Skipping: 35 Skipping: 36 Skipping: 37 Skipping: 38 Skipping: 39 Skipping: 40 Skipping: 41 Skipping: 42 Skipping: 43 Skipping: 44 Skipping: 45 Skipping: 46 Skipping: 47 Skipping: 48 Skipping: 49 Skipping: 50 Skipping: 51 Skipping: 52 Skipping: 53 Skipping: 54 Skipping: 55 Skipping: 56 Skipping: 57 Skipping: 58 Skipping: 59 Skipping: 60 Skipping: 61 Skipping: 62 Skipping: 63 Skipping: 64 Skipping: 65 Skipping: 66 Skipping: 67 Skipping: 68 Skipping: 69 Skipping: 70 Skipping: 71 Skipping: 72 Skipping: 73 Skipping: 74 Skipping: 75 Skipping: 76 Skipping: 77 Skipping: 78 Skipping: 79 Skipping: 80 Skipping: 81 Skipping: 82 Skipping: 83 Skipping: 84 Skipping: 85 Skipping: 86 Skipping: 87 Skipping: 88 Skipping: 89 Skipping: 90 Skipping: 91 Skipping: 92 Skipping: 93 Skipping: 94 Skipping: 95 Skipping: 96 Skipping: 97 Skipping: 98 Skipping: 99 Skipping: 100 Skipping: 101 Skipping: 102 Skipping: 103 Skipping: 104 Skipping: 105 Skipping: 106 Skipping: 107 Skipping: 108 Skipping: 109 Skipping: 110 Skipping: 111 Skipping: 112 Skipping: 113 Skipping: 114 Skipping: 115 Skipping: 116 Skipping: 117 Skipping: 118 Skipping: 119 Skipping: 120 Skipping: 121 Skipping: 122 Skipping: 123 Skipping: 124 Skipping: 125 Skipping: 126 Skipping: 127 Skipping: 128 Skipping: 129 Skipping: 130 Skipping: 131 Skipping: 132 Skipping: 133 Skipping: 134 Skipping: 135 Skipping: 136 Skipping: 137 Skipping: 138 Skipping: 139 Skipping: 140 Skipping: 141 Skipping: 142 Skipping: 143 Skipping: 144 Skipping: 145 Skipping: 146 Skipping: 147 Skipping: 148 Skipping: 149 Skipping: 150 Skipping: 151 Skipping: 152 Skipping: 153 Skipping: 154 Skipping: 155 Skipping: 156 Skipping: 157 Skipping: 158 Skipping: 159 Skipping: 160 Skipping: 161 Skipping: 162 Skipping: 163 Skipping: 164 Skipping: 165 Skipping: 166 Skipping: 167 Skipping: 168 Skipping: 169 Skipping: 170 Skipping: 171 Skipping: 172 Skipping: 173 Skipping: 174 Skipping: 175 Skipping: 176 Skipping: 177 Skipping: 178 Skipping: 179 Skipping: 180 Skipping: 181 Skipping: 182 Skipping: 183 Skipping: 184 Skipping: 185 Skipping: 186 Skipping: 187 Skipping: 188 Skipping: 189 Skipping: 190 Skipping: 191 Skipping: 192 Skipping: 193 Skipping: 194 Skipping: 195 Skipping: 196 Skipping: 197 Skipping: 198 Skipping: 199 Skipping: 200 Skipping: 201 Skipping: 202 Skipping: 203 Skipping: 204 Skipping: 205 Skipping: 206 Skipping: 207 Skipping: 208 Skipping: 209 Skipping: 210 Skipping: 211 Skipping: 212 Skipping: 213 Skipping: 214 Skipping: 215 Skipping: 216 Skipping: 217 Skipping: 218 Skipping: 219 Skipping: 220 Skipping: 221 Skipping: 222 Skipping: 223 Skipping: 224 Skipping: 225 Skipping: 226 Skipping: 227 Skipping: 228 Skipping: 229 Skipping: 230 Skipping: 231 Skipping: 232 Skipping: 233 Skipping: 234 Skipping: 235 Skipping: 236 Skipping: 237 Skipping: 238 Skipping: 239 Skipping: 240 Skipping: 241 Skipping: 242 Skipping: 243 Skipping: 244 Skipping: 245 Skipping: 246 Skipping: 247 Skipping: 248 Skipping: 249 Processing complete for reasoning_about_colored_objects. 0 new answers saved to /Users/log/Github/textual_grounding/logan/results/final/VanillaCoT/reasoning_about_colored_objects/llama3.18b/zero_shot_vanilla_cot_None_reasoning_about_colored_objects_llama3.18b.csv.\n",
      "------Processing dataset: wikimultihopQA-------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[111], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m few_shot_txt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      5\u001b[0m sample_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m400\u001b[39m\n\u001b[0;32m----> 7\u001b[0m run_model(llm_model, prompt_type, few_shot_txt, sample_size, project_root)\n",
      "Cell \u001b[0;32mIn[110], line 17\u001b[0m, in \u001b[0;36mrun_model\u001b[0;34m(llm_model, prompt_type, few_shot_txt, sample_size, project_root)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     data_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(project_root, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, dataset, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest.jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m ids, questions \u001b[38;5;241m=\u001b[39m load_data_size_specific(data_path, sample_size\u001b[38;5;241m=\u001b[39msample_size)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m few_shot_txt:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(fewshot_prompt_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "Cell \u001b[0;32mIn[82], line 39\u001b[0m, in \u001b[0;36mload_data_size_specific\u001b[0;34m(data_path, sample_size, random_seed)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m     sampled_data \u001b[38;5;241m=\u001b[39m eligible_data\n\u001b[0;32m---> 39\u001b[0m ids \u001b[38;5;241m=\u001b[39m [x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m sampled_data]\n\u001b[1;32m     40\u001b[0m questions \u001b[38;5;241m=\u001b[39m [x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m sampled_data]\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids, questions\n",
      "\u001b[0;31mKeyError\u001b[0m: 'id'"
     ]
    }
   ],
   "source": [
    "llm_model = 'llama3.18b'\n",
    "project_root = '/Users/log/Github/textual_grounding/'\n",
    "prompt_type = 'zero_shot_vanilla_cot'\n",
    "few_shot_txt = None\n",
    "sample_size = 400\n",
    "\n",
    "run_model(llm_model, prompt_type, few_shot_txt, sample_size, project_root)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
