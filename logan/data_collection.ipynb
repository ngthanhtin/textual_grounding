{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import os\n",
    "import sys\n",
    "import anthropic\n",
    "import ollama\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from google.generativeai.types import RequestOptions\n",
    "from google.api_core import retry\n",
    "from typing import List, Tuple\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import datetime\n",
    "import openai\n",
    "import time\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, TimeoutError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Shot - Vanilla CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_vanilla_cot = \"\"\"\n",
    "Think through your answer step by step. Put the concise form of your final answer in curly brackets e.g. {A}, {True} or {3.0}.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(prompt_type: str, few_shot_prompt: str, question: str) -> str:\n",
    "    prompts = {\n",
    "        \"zero_shot_vanilla_cot\": f\"{question}\\n{zero_shot_vanilla_cot}\",\n",
    "    }\n",
    "    return prompts[prompt_type]\n",
    "\n",
    "def save_results(save_path: str, ids: List[str], questions: List[str], answers: List[str], append: bool = False):\n",
    "    df = pd.DataFrame({'id': ids, 'question': questions, 'answer': answers})\n",
    "    if append and os.path.exists(save_path):\n",
    "        df.to_csv(save_path, mode='a', index=False, header=False)\n",
    "    else:\n",
    "        df.to_csv(save_path, index=False)\n",
    "\n",
    "def read_jsonl_file(filepath: str) -> List[dict]:\n",
    "    data = []\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            json_obj = json.loads(line)\n",
    "            data.append(json_obj)\n",
    "    return data\n",
    "\n",
    "def load_data_size_specific(data_path: str, sample_size: int = 0, random_seed: int = 0):\n",
    "    random.seed(random_seed)\n",
    "\n",
    "    if data_path.endswith('.jsonl'):\n",
    "        data = read_jsonl_file(data_path)\n",
    "    elif data_path.endswith('.json'):\n",
    "        with open(data_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "    \n",
    "    question_length = 0\n",
    "    eligible_data = [x for x in data if len(x[\"question\"]) >= question_length]\n",
    "    \n",
    "    if sample_size > 0 and sample_size < len(eligible_data):\n",
    "        sampled_data = random.sample(eligible_data, sample_size)\n",
    "    else:\n",
    "        sampled_data = eligible_data\n",
    "    \n",
    "    ids = [x[\"id\"] for x in sampled_data]\n",
    "    questions = [x[\"question\"] for x in sampled_data]\n",
    "    \n",
    "    return ids, questions\n",
    "\n",
    "def load_already_answered_ids(save_path: str) -> set:\n",
    "    if os.path.exists(save_path):\n",
    "        df = pd.read_csv(save_path)\n",
    "        answered_ids = set(df['id'].tolist())\n",
    "        # print(f\"Loaded {len(answered_ids)} already answered IDs from: {save_path}\")\n",
    "        print(f\"Already answered IDs: {answered_ids}\")\n",
    "        return answered_ids\n",
    "    else:\n",
    "        print(f\"No existing save file found at: {save_path}. Starting fresh.\")\n",
    "        return set()\n",
    "\n",
    "def initialize_save_file(save_path: str):\n",
    "    if not os.path.exists(save_path):\n",
    "        # Create an empty DataFrame with headers and save\n",
    "        df = pd.DataFrame(columns=['id', 'question', 'answer'])\n",
    "        df.to_csv(save_path, index=False)\n",
    "        print(f\"Initialized new save file with headers at: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_4o(prompt: str) -> str:\n",
    "    client = OpenAI()\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-2024-08-06\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{prompt}\"\n",
    "            }\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "def query_llama(prompt):\n",
    "    client = openai.OpenAI(\n",
    "        api_key=os.environ.get(\"SAMBANOVA_API_KEY\"),\n",
    "        base_url=\"https://api.sambanova.ai/v1\",\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model='Meta-Llama-3.1-8B-Instruct',\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "        temperature=0.6, # Meta default\n",
    "        top_p = 0.9 # Meta default\n",
    "    )\n",
    "    time.sleep(2)  # Pause execution for 2 seconds\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def query_llama_70b(prompt):\n",
    "    client = openai.OpenAI(\n",
    "        api_key=os.environ.get(\"SAMBANOVA_API_KEY\"),\n",
    "        base_url=\"https://api.sambanova.ai/v1\",\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model='Meta-Llama-3.1-70B-Instruct',\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "        temperature=0.6, # Meta default\n",
    "        top_p = 0.9 # Meta default\n",
    "    )\n",
    "    time.sleep(2)  # Pause execution for 2 seconds\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm(llm_model: str, ids: List[str], questions: List[str], few_shot_prompt: str, prompt_type: str, save_path: str, already_answered_ids: set) -> Tuple[List[str], List[str], List[str]]:\n",
    "    answers = []\n",
    "    ids_can_be_answered = []\n",
    "    questions_can_be_answered = []\n",
    "    \n",
    "    for id, q in tqdm(zip(ids, questions), total=len(ids)):\n",
    "        # print(q)\n",
    "        # print(f\"Processing ID: {id}\")\n",
    "        if id in already_answered_ids:\n",
    "            print(f\"Skipping: {id}\", end=' ')\n",
    "            continue\n",
    "        if id == 1146: # weird ID that breaks llama\n",
    "            continue\n",
    "        \n",
    "        prompt = get_prompt(prompt_type, few_shot_prompt, q)\n",
    "        try:\n",
    "            if llm_model == 'gemini':\n",
    "                answer = query_gemini(prompt, id)\n",
    "            elif llm_model == 'claude':\n",
    "                answer = query_claude(prompt)\n",
    "            elif llm_model == '4o':\n",
    "                # answer = query_4o_multiturn(prompt)\n",
    "                if prompt_type == 'multi_convo':\n",
    "                    fact_prompt = get_prompt(prompt_type=\"fact_prompt\", few_shot_prompt=\"\", question=q)\n",
    "                    \n",
    "                    answer_prompt = get_prompt(prompt_type=\"answer_prompt_data\", few_shot_prompt=\"\", question=q)\n",
    "                    answer = query_4o_multiconvo(fact_prompt=fact_prompt, answer_prompt=answer_prompt, extracted_question=q)\n",
    "                else:\n",
    "                    answer = query_4o(prompt)\n",
    "                \n",
    "            elif llm_model == 'llama3.18b':\n",
    "                answer = query_llama(prompt)\n",
    "            elif llm_model == 'llama3.170b':\n",
    "                answer = query_llama_70b(prompt)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported LLM model: {llm_model}\")\n",
    "            # print(f\"Answer for ID {id}: {answer}\")\n",
    "            \n",
    "            answers.append(answer)\n",
    "            questions_can_be_answered.append(q)\n",
    "            ids_can_be_answered.append(id)\n",
    "\n",
    "            # Save after each answer\n",
    "            save_results(save_path, [id], [q], [answer], append=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question {id}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return ids_can_be_answered, questions_can_be_answered, answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_datasets = ['logical_deduction_seven_objects','reasoning_about_colored_objects']\n",
    "jsonl_datasets = ['GSM8K', 'date', 'GSM_Plus', 'MultiArith', 'ASDiv', 'SVAMP', 'AQUA', 'p_GSM8K', 'StrategyQA', 'commonsenseQA','SPARTQA']\n",
    "all_datasets = jsonl_datasets + json_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(llm_model, prompt_type, few_shot_txt, sample_size, project_root):\n",
    "    for dataset in all_datasets:\n",
    "        print(f\"------Processing dataset: {dataset}-------\")\n",
    "        if few_shot_txt:\n",
    "            fewshot_prompt_path = os.path.join(project_root, \"prompt\", dataset, few_shot_txt)\n",
    "            \n",
    "        save_dir = os.path.join(project_root, 'logan/results/final/VanillaCoT', dataset, f'{llm_model}')\n",
    "        os.makedirs(save_dir, exist_ok=True)  # Ensure the directory exists\n",
    "        save_path = os.path.join(save_dir, f'{prompt_type}_{few_shot_txt}_{dataset}_{llm_model}.csv')\n",
    "\n",
    "        if dataset in json_datasets:\n",
    "            data_path = os.path.join(project_root, 'data', dataset, 'test.json')\n",
    "        else:\n",
    "            data_path = os.path.join(project_root, 'data', dataset, 'test.jsonl')\n",
    "\n",
    "\n",
    "        ids, questions = load_data_size_specific(data_path, sample_size=sample_size)\n",
    "        if few_shot_txt:\n",
    "            with open(fewshot_prompt_path, 'r') as file:\n",
    "                few_shot_prompt = file.read()\n",
    "        else:\n",
    "            few_shot_prompt = \"\"\n",
    "\n",
    "        initialize_save_file(save_path)\n",
    "        already_answered_ids = load_already_answered_ids(save_path)\n",
    "\n",
    "        ids_answered, questions_answered, answers = query_llm(\n",
    "            llm_model=llm_model,\n",
    "            ids=ids,\n",
    "            questions=questions,\n",
    "            few_shot_prompt=few_shot_prompt,\n",
    "            prompt_type=prompt_type,\n",
    "            save_path=save_path,\n",
    "            already_answered_ids=already_answered_ids\n",
    "        )\n",
    "\n",
    "        print(f\"Processing complete for {dataset}. {len(ids_answered)} new answers saved to {save_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = 'llama3.170b'\n",
    "project_root = '/Users/log/Github/textual_grounding/'\n",
    "prompt_type = 'zero_shot_vanilla_cot'\n",
    "few_shot_txt = None\n",
    "sample_size = 400\n",
    "\n",
    "# run_model(llm_model, prompt_type, few_shot_txt, sample_size, project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 17:46:09,175 - INFO - Batch input file created at: /Users/log/Github/textual_grounding/logan/batch_files/VanillaCoT/MultiArith/claude-3-5-sonnet-20240620/zero_shot_vanilla_cot_None_MultiArith_claude-3-5-sonnet-20240620.jsonl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'custom_id': '397', 'params': {'model': 'claude-3-5-sonnet-20240620', 'max_tokens': 1024, 'messages': [{'role': 'user', 'content': \"Debby's class is going on a field trip to the zoo. If each van can hold 9 people and there are 40 students and 14 adults going, how many vans will they need?\\n\\nThink through your answer step by step. Put the concise form of your final answer in curly brackets e.g. {A}, {True} or {3.0}.\\n\"}]}}, {'custom_id': '433', 'params': {'model': 'claude-3-5-sonnet-20240620', 'max_tokens': 1024, 'messages': [{'role': 'user', 'content': 'John had 5 action figures, but needed 7 total for a complete collection. If each one costs $5, how much money would he need to finish his collection?\\n\\nThink through your answer step by step. Put the concise form of your final answer in curly brackets e.g. {A}, {True} or {3.0}.\\n'}]}}]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 17:46:09,633 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages/batches?beta=true \"HTTP/1.1 200 OK\"\n",
      "2024-11-10 17:46:09,637 - INFO - Created batch with ID: msgbatch_014xdLrN6YFLTrqHmYTxghjf\n",
      "2024-11-10 17:46:09,767 - INFO - HTTP Request: GET https://api.anthropic.com/v1/messages/batches/msgbatch_014xdLrN6YFLTrqHmYTxghjf?beta=true \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 55\u001b[0m\n\u001b[1;32m     45\u001b[0m     prompts\u001b[38;5;241m.\u001b[39mappend(prompt)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# tasks = batch_agents.prepare_batch_input(\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m#     llm_model=llm_model,\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m#     ids=ids,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m \n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m#Execute the batch processing with GPT-4 and save results\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m batch_agents\u001b[38;5;241m.\u001b[39mbatch_api_agent(\n\u001b[1;32m     56\u001b[0m     llm_model\u001b[38;5;241m=\u001b[39mllm_model,\n\u001b[1;32m     57\u001b[0m     ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[1;32m     58\u001b[0m     prompts\u001b[38;5;241m=\u001b[39mprompts,\n\u001b[1;32m     59\u001b[0m     batch_output_file\u001b[38;5;241m=\u001b[39mbatch_output_file,\n\u001b[1;32m     60\u001b[0m     batch_results_file\u001b[38;5;241m=\u001b[39mbatch_results_path\n\u001b[1;32m     61\u001b[0m )\n",
      "File \u001b[0;32m~/Github/textual_grounding/agents/batch_api_agents.py:264\u001b[0m, in \u001b[0;36mbatch_api_agent\u001b[0;34m(llm_model, ids, prompts, temperature, max_tokens, batch_output_file, batch_results_file, poll_interval, max_wait_time)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m message_batch\u001b[38;5;241m.\u001b[39mprocessing_status \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch processing failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage_batch\u001b[38;5;241m.\u001b[39merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 264\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(poll_interval)\n\u001b[1;32m    265\u001b[0m wait_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m poll_interval\n\u001b[1;32m    266\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch Status: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage_batch\u001b[38;5;241m.\u001b[39mprocessing_status\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Time elapsed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwait_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from agents.batch_api_agents import prepare_batch_input, batch_api_agent\n",
    "from openai import OpenAI\n",
    "import importlib\n",
    "import agents.batch_api_agents as batch_agents\n",
    "\n",
    "# Reload the module to ensure the latest changes are loaded\n",
    "importlib.reload(batch_agents)\n",
    "\n",
    "llm_model = 'claude-3-5-sonnet-20240620'\n",
    "project_root = '/Users/log/Github/textual_grounding/'\n",
    "prompt_type = 'zero_shot_vanilla_cot'\n",
    "few_shot_txt = None\n",
    "sample_size = 2\n",
    "json_datasets = ['logical_deduction_seven_objects','reasoning_about_colored_objects']\n",
    "jsonl_datasets = ['GSM_Plus', 'MultiArith', 'SVAMP', 'p_GSM8K', 'StrategyQA', 'commonsenseQA','SPARTQA']\n",
    "all_datasets = jsonl_datasets + json_datasets\n",
    "all_datasets = ['MultiArith']\n",
    "for dataset in all_datasets:\n",
    "    if dataset in json_datasets:\n",
    "        data_path = os.path.join(project_root, 'data', dataset, 'test.json')\n",
    "    else:\n",
    "        data_path = os.path.join(project_root, 'data', dataset, 'test.jsonl')\n",
    "        \n",
    "    save_dir = os.path.join(project_root, 'logan/results/final/VanillaCoT', dataset, f'{llm_model}')\n",
    "    batch_dir = os.path.join(project_root, 'logan/batch_files/VanillaCoT', dataset, f'{llm_model}')\n",
    "    os.makedirs(save_dir, exist_ok=True)  # Ensure the directory exists\n",
    "    os.makedirs(batch_dir, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "    batch_results_path = os.path.join(save_dir, f'{prompt_type}_{few_shot_txt}_{dataset}_{llm_model}.jsonl')   \n",
    "    batch_output_file = os.path.join(batch_dir, f'{prompt_type}_{few_shot_txt}_{dataset}_{llm_model}.jsonl')   \n",
    "\n",
    "    ids, questions = load_data_size_specific(data_path, sample_size=sample_size)\n",
    "    if few_shot_txt:\n",
    "        with open(fewshot_prompt_path, 'r') as file:\n",
    "            few_shot_prompt = file.read()\n",
    "    else:\n",
    "        few_shot_prompt = \"\"\n",
    "    prompts = []\n",
    "    for question in questions:\n",
    "        prompt = get_prompt(prompt_type, few_shot_prompt, question)\n",
    "        # print(prompt)\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    # tasks = batch_agents.prepare_batch_input(\n",
    "    #     llm_model=llm_model,\n",
    "    #     ids=ids,\n",
    "    #     prompts=prompts,\n",
    "    #     batch_output_file=batch_output_file\n",
    "    # )\n",
    "\n",
    "    #Execute the batch processing with GPT-4 and save results\n",
    "    batch_agents.batch_api_agent(\n",
    "        llm_model=llm_model,\n",
    "        ids=ids,\n",
    "        prompts=prompts,\n",
    "        batch_output_file=batch_output_file,\n",
    "        batch_results_file=batch_results_path\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
