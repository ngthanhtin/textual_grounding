{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import os\n",
    "import sys\n",
    "import anthropic\n",
    "import ollama\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from google.generativeai.types import RequestOptions\n",
    "from google.api_core import retry\n",
    "from typing import List, Tuple\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import datetime\n",
    "import openai\n",
    "import time\n",
    "import re\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, TimeoutError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Shot - Vanilla CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_vanilla_cot = \"\"\"\n",
    "Think through your answer step by step. Put the concise form of your final answer in curly brackets e.g. {A}, {True} or {3.0}.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_last_sentence(text):\n",
    "    # Split the text at the point where multiple-choice options start (e.g., \"A)\", \"B)\", etc.)\n",
    "    parts = re.split(r'\\n[A-Z]\\)', text, flags=re.MULTILINE)\n",
    "    \n",
    "    if parts:\n",
    "        # The first part contains everything before the options\n",
    "        pre_options = parts[0]\n",
    "        \n",
    "        # Use a regex to find all sentences ending with ., ?, or !\n",
    "        sentences = re.findall(r'[^.!?]*[.!?]', pre_options, re.DOTALL)\n",
    "        \n",
    "        if sentences:\n",
    "            # Return the last sentence after stripping any extra spaces\n",
    "            return sentences[-1].strip()\n",
    "    \n",
    "    # If splitting or finding sentences fails, return the original text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# def extract_last_sentence(text):\n",
    "#     # Split the text into sentences, excluding option markers like \"A)\", \"B)\", etc.\n",
    "#     sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|!)(?=\\s[A-Z]\\))', text)\n",
    "    \n",
    "#     # Return the last sentence after stripping any extra spaces.\n",
    "#     return sentences[-1].strip() if sentences else text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(prompt_type: str, few_shot_prompt: str, question: str) -> str:\n",
    "    # print(question)\n",
    "    last_sentence = extract_last_sentence(question)\n",
    "    \n",
    "    # instruction = f\"I want you to answer this question but your explanation should contain references referring back to the information in the question. To do that, first, re-generate the question with proper tags for key phrases, the key phrases that are most relevant to answering the question {last_sentence} and then generate your answers. The output format is as follow:\\n\\\"\n",
    "    \n",
    "        # CHANGE FOR MATH\n",
    "    instruction = f\"I want you to answer this question but your explanation should contain references referring back to the information in the question. To do that, first, re-generate the question with proper tags for key phrases, the key phrases that are most relevant to answering the question and then generate your answers. The output format is as follow:\\n\\\n",
    "                Reformatted Question: \\\n",
    "                    Answer:\"\n",
    "                    \n",
    "    prompts = {\n",
    "        \"zero_shot_vanilla_cot\": f\"{question}\\n{zero_shot_vanilla_cot}\",\n",
    "        \"gcot\": f\"{few_shot_prompt}\\n{question}\\n{instruction}\",\n",
    "        \"cot\": f\"{few_shot_prompt}\\n{question}\\nPlease generate your explanation first, then generate the final answer in the bracket as follow:\\nAnswer: {{}}\"\n",
    "    }\n",
    "    return prompts[prompt_type]\n",
    "\n",
    "def save_results(save_path: str, ids: List[str], questions: List[str], answers: List[str], append: bool = False):\n",
    "    df = pd.DataFrame({'id': ids, 'question': questions, 'answer': answers})\n",
    "    if append and os.path.exists(save_path):\n",
    "        df.to_csv(save_path, mode='a', index=False, header=False)\n",
    "    else:\n",
    "        df.to_csv(save_path, index=False)\n",
    "\n",
    "def read_jsonl_file(filepath: str) -> List[dict]:\n",
    "    data = []\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            json_obj = json.loads(line)\n",
    "            data.append(json_obj)\n",
    "    return data\n",
    "\n",
    "def load_already_answered_ids(save_path: str) -> set:\n",
    "    if os.path.exists(save_path):\n",
    "        df = pd.read_csv(save_path)\n",
    "        answered_ids = set(df['id'].tolist())\n",
    "        # print(f\"Loaded {len(answered_ids)} already answered IDs from: {save_path}\")\n",
    "        print(f\"Already answered IDs: {answered_ids}\")\n",
    "        return answered_ids\n",
    "    else:\n",
    "        print(f\"No existing save file found at: {save_path}. Starting fresh.\")\n",
    "        return set()\n",
    "\n",
    "def initialize_save_file(save_path: str):\n",
    "    if not os.path.exists(save_path):\n",
    "        # Create an empty DataFrame with headers and save\n",
    "        df = pd.DataFrame(columns=['id', 'question', 'answer'])\n",
    "        df.to_csv(save_path, index=False)\n",
    "        print(f\"Initialized new save file with headers at: {save_path}\")\n",
    "\n",
    "def load_data_size_specific(data_path: str, sample_size: int = 0, random_seed: int = 0):\n",
    "    random.seed(random_seed)\n",
    "\n",
    "    if data_path.endswith('.jsonl'):\n",
    "        data = read_jsonl_file(data_path)\n",
    "    elif data_path.endswith('.json'):\n",
    "        with open(data_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "    \n",
    "    question_length = 0\n",
    "    eligible_data = [x for x in data if len(x[\"question\"]) >= question_length]\n",
    "    \n",
    "    if sample_size > 0 and sample_size < len(eligible_data):\n",
    "        sampled_data = random.sample(eligible_data, sample_size)\n",
    "    else:\n",
    "        sampled_data = eligible_data\n",
    "    \n",
    "    ids = [x[\"id\"] for x in sampled_data]\n",
    "    questions = [x[\"question\"] for x in sampled_data]\n",
    "    \n",
    "    return ids, questions\n",
    "\n",
    "        \n",
    "def get_longest_questions_and_ids(data_path, sample_size=200):\n",
    "    data = read_jsonl_file(data_path)\n",
    "    full_ids = [x[\"id\"] for x in data]\n",
    "    full_questions = [x[\"question\"] for x in data]\n",
    "\n",
    "    # Combine questions and IDs into a list of tuples\n",
    "    full_questions_ids = list(zip(full_questions, full_ids))\n",
    "    \n",
    "    # Sort the tuples by the length of the questions\n",
    "    sorted_full_questions_ids = sorted(full_questions_ids, key=lambda x: len(x[0]), reverse=True)\n",
    "    \n",
    "    # Select the shortest questions and their IDs\n",
    "    sampled_data = random.sample(data, sample_size)\n",
    "    longest_ = sorted_full_questions_ids[:min(sample_size, len(sorted_full_questions_ids))]\n",
    "\n",
    "    # Separate them back into two lists\n",
    "    longest_questions, longest_ids = zip(*longest_)\n",
    "\n",
    "    # Convert to lists if necessary\n",
    "    longest_questions = list(longest_questions)\n",
    "    longest_ids = list(longest_ids)\n",
    "    \n",
    "    return longest_ids, longest_questions\n",
    "\n",
    "def get_shortest_questions_and_ids(data_path, sample_size=200):\n",
    "    data = read_jsonl_file(data_path)\n",
    "    full_ids = [x[\"id\"] for x in data]\n",
    "    full_questions = [x[\"question\"] for x in data]\n",
    "\n",
    "    # Combine questions and IDs into a list of tuples\n",
    "    full_questions_ids = list(zip(full_questions, full_ids))\n",
    "    \n",
    "    # Sort the tuples by the length of the questions in ascending order\n",
    "    sorted_full_questions_ids = sorted(full_questions_ids, key=lambda x: len(x[0]))\n",
    "    \n",
    "    # Select the shortest questions and their IDs\n",
    "    sampled_data = random.sample(data, sample_size)\n",
    "    shortest_ = sorted_full_questions_ids[:min(sample_size, len(sorted_full_questions_ids))]\n",
    "\n",
    "    # Separate them back into two lists\n",
    "    shortest_questions, shortest_ids = zip(*shortest_)\n",
    "\n",
    "    # Convert to lists if necessary\n",
    "    shortest_questions = list(shortest_questions)\n",
    "    shortest_ids = list(shortest_ids)\n",
    "    \n",
    "    return shortest_ids, shortest_questions\n",
    "\n",
    "    \n",
    "def get_random_questions_and_ids(data_path, sample_size=200):\n",
    "\n",
    "    data = read_jsonl_file(data_path)\n",
    "\n",
    "    longest_questions, longest_ids = get_longest_questions_and_ids(data_path, sample_size)\n",
    "    \n",
    "    result_ids = []\n",
    "    result_questions = []\n",
    "    # Select the shortest questions and their IDs\n",
    "    sampled_data = random.sample(data, sample_size*2)\n",
    "    totalQuestions = 0\n",
    "    for x in sampled_data:\n",
    "        if totalQuestions >= sample_size:\n",
    "            break\n",
    "        if x[\"question\"] not in longest_questions:\n",
    "            result_ids.append(x[\"id\"])\n",
    "            result_questions.append(x[\"question\"])\n",
    "            totalQuestions += 1\n",
    "    \n",
    "    return result_ids, result_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_4o(prompt: str) -> str:\n",
    "    client = OpenAI()\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-2024-08-06\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{prompt}\"\n",
    "            }\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "def query_llama(prompt):\n",
    "    client = openai.OpenAI(\n",
    "        api_key=os.environ.get(\"SAMBANOVA_API_KEY\"),\n",
    "        base_url=\"https://api.sambanova.ai/v1\",\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model='Meta-Llama-3.1-8B-Instruct',\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "        temperature=0.6, # Meta default\n",
    "        top_p = 0.9 # Meta default\n",
    "    )\n",
    "    time.sleep(2)  # Pause execution for 2 seconds\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def query_llama_70b(prompt):\n",
    "    client = openai.OpenAI(\n",
    "        api_key=os.environ.get(\"SAMBANOVA_API_KEY\"),\n",
    "        base_url=\"https://api.sambanova.ai/v1\",\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model='Meta-Llama-3.1-70B-Instruct',\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "        temperature=0.6, # Meta default\n",
    "        top_p = 0.9 # Meta default\n",
    "    )\n",
    "    time.sleep(2)  # Pause execution for 2 seconds\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def query_llama_405b(prompt):\n",
    "    client = openai.OpenAI(\n",
    "        api_key=os.environ.get(\"SAMBANOVA_API_KEY\"),\n",
    "        base_url=\"https://api.sambanova.ai/v1\",\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model='Meta-Llama-3.1-405B-Instruct',\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "        temperature=0.6, # Meta default\n",
    "        top_p = 0.9 # Meta default\n",
    "    )\n",
    "    time.sleep(5)  # Pause execution for 2 seconds\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm(llm_model: str, ids: List[str], questions: List[str], few_shot_prompt: str, prompt_type: str, save_path: str, already_answered_ids: set) -> Tuple[List[str], List[str], List[str]]:\n",
    "    answers = []\n",
    "    ids_can_be_answered = []\n",
    "    questions_can_be_answered = []\n",
    "    \n",
    "    for id, q in tqdm(zip(ids, questions), total=len(ids)):\n",
    "        # print(q)\n",
    "        # print(f\"Processing ID: {id}\")\n",
    "        if id in already_answered_ids:\n",
    "            print(f\"Skipping: {id}\", end=' ')\n",
    "            continue\n",
    "        # if id == 1146: # weird ID that breaks llama\n",
    "        #     continue\n",
    "        \n",
    "        # print(few_shot_prompt)\n",
    "        prompt = get_prompt(prompt_type, few_shot_prompt, q)\n",
    "        # print(prompt)\n",
    "        try:\n",
    "            # print(prompt)\n",
    "            if llm_model == 'gemini':\n",
    "                answer = query_gemini(prompt, id)\n",
    "            elif llm_model == 'claude':\n",
    "                answer = query_claude(prompt)\n",
    "            elif llm_model == '4o':\n",
    "                # answer = query_4o_multiturn(prompt)\n",
    "                if prompt_type == 'multi_convo':\n",
    "                    fact_prompt = get_prompt(prompt_type=\"fact_prompt\", few_shot_prompt=\"\", question=q)\n",
    "                    \n",
    "                    answer_prompt = get_prompt(prompt_type=\"answer_prompt_data\", few_shot_prompt=\"\", question=q)\n",
    "                    answer = query_4o_multiconvo(fact_prompt=fact_prompt, answer_prompt=answer_prompt, extracted_question=q)\n",
    "                else:\n",
    "                    answer = query_4o(prompt)\n",
    "                \n",
    "            elif llm_model == 'llama3.18b':\n",
    "                answer = query_llama(prompt)\n",
    "            elif llm_model == 'llama3.170b':\n",
    "                answer = query_llama_70b(prompt)\n",
    "            elif llm_model == 'llama3.1405b':\n",
    "                answer = query_llama_405b(prompt)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported LLM model: {llm_model}\")\n",
    "            # print(f\"Answer for ID {id}: {answer}\")\n",
    "            \n",
    "            answers.append(answer)\n",
    "            questions_can_be_answered.append(q)\n",
    "            ids_can_be_answered.append(id)\n",
    "\n",
    "            # Save after each answer\n",
    "            save_results(save_path, [id], [q], [answer], append=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question {id}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return ids_can_be_answered, questions_can_be_answered, answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_datasets = ['logical_deduction_seven_objects','reasoning_about_colored_objects']\n",
    "jsonl_datasets = ['GSM8K', 'date', 'GSM_Plus', 'MultiArith', 'ASDiv', 'SVAMP', 'AQUA', 'p_GSM8K', 'StrategyQA', 'commonsenseQA','SPARTQA']\n",
    "all_datasets = jsonl_datasets + json_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(llm_model, prompt_type, few_shot_txt, sample_size, project_root, identifier, isRandom = False, isLongest = False, isShortest = False):\n",
    "    # for dataset in all_datasets:\n",
    "    for dataset in ['MATH']:\n",
    "        print(f\"------Processing dataset: {dataset}-------\")\n",
    "        if few_shot_txt:\n",
    "            fewshot_prompt_path = os.path.join(project_root, \"prompt\", dataset, few_shot_txt)\n",
    "        # print(fewshot_prompt_path)\n",
    "        # continue \n",
    "        if prompt_type == 'zero_shot_vanilla_cot':\n",
    "            save_dir = os.path.join(project_root, 'logan/results/final/VanillaCoT', dataset, f'{llm_model}')\n",
    "        elif prompt_type == 'gcot':\n",
    "            save_dir = os.path.join(project_root, 'logan/results/final/GCoT', dataset, f'{llm_model}')\n",
    "        elif prompt_type == 'cot':\n",
    "            save_dir = os.path.join(project_root, 'logan/results/final/fewshot_CoT', dataset, f'{llm_model}')\n",
    "        os.makedirs(save_dir, exist_ok=True)  # Ensure the directory exists\n",
    "        save_path = os.path.join(save_dir, f'{prompt_type}_{identifier}_{few_shot_txt}_{dataset}_{llm_model}.csv')\n",
    "\n",
    "        if dataset in json_datasets:\n",
    "            data_path = os.path.join(project_root, 'data', dataset, 'test.json')\n",
    "        else:\n",
    "            data_path = os.path.join(project_root, 'data', dataset, 'test.jsonl')\n",
    "\n",
    "\n",
    "        if isRandom:\n",
    "            ids, questions = get_random_questions_and_ids(data_path, sample_size=sample_size)\n",
    "        elif isLongest:\n",
    "            ids, questions = get_longest_questions_and_ids(data_path, sample_size=sample_size)\n",
    "        elif isShortest:\n",
    "            ids, questions = load_data_size_specific(data_path, sample_size=sample_size)\n",
    "        else:\n",
    "            ids, questions = load_data_size_specific(data_path, sample_size=sample_size)\n",
    "        print(sorted(ids))\n",
    "\n",
    "        if few_shot_txt:\n",
    "            with open(fewshot_prompt_path, 'r') as file:\n",
    "                few_shot_prompt = file.read()\n",
    "        else:\n",
    "            few_shot_prompt = \"\"\n",
    "\n",
    "        initialize_save_file(save_path)\n",
    "        already_answered_ids = load_already_answered_ids(save_path)\n",
    "\n",
    "        ids_answered, questions_answered, answers = query_llm(\n",
    "            llm_model=llm_model,\n",
    "            ids=ids,\n",
    "            questions=questions,\n",
    "            few_shot_prompt=few_shot_prompt,\n",
    "            prompt_type=prompt_type,\n",
    "            save_path=save_path,\n",
    "            already_answered_ids=already_answered_ids\n",
    "        )\n",
    "\n",
    "        print(f\"Processing complete for {dataset}. {len(ids_answered)} new answers saved to {save_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Processing dataset: MATH-------\n",
      "Already answered IDs: set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[223], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# models = ['llama3.170b']\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[0;32m---> 14\u001b[0m     run_model(model, prompt_type, few_shot_txt, sample_size, project_root, \n\u001b[1;32m     15\u001b[0m               identifier\u001b[38;5;241m=\u001b[39midentifier, \n\u001b[1;32m     16\u001b[0m               isLongest\u001b[38;5;241m=\u001b[39misLongest, \n\u001b[1;32m     17\u001b[0m               isRandom\u001b[38;5;241m=\u001b[39misRandom,\n\u001b[1;32m     18\u001b[0m               isShortest\u001b[38;5;241m=\u001b[39misShortest)\n",
      "Cell \u001b[0;32mIn[218], line 43\u001b[0m, in \u001b[0;36mrun_model\u001b[0;34m(llm_model, prompt_type, few_shot_txt, sample_size, project_root, identifier, isRandom, isLongest, isShortest)\u001b[0m\n\u001b[1;32m     40\u001b[0m initialize_save_file(save_path)\n\u001b[1;32m     41\u001b[0m already_answered_ids \u001b[38;5;241m=\u001b[39m load_already_answered_ids(save_path)\n\u001b[0;32m---> 43\u001b[0m ids_answered, questions_answered, answers \u001b[38;5;241m=\u001b[39m query_llm(\n\u001b[1;32m     44\u001b[0m     llm_model\u001b[38;5;241m=\u001b[39mllm_model,\n\u001b[1;32m     45\u001b[0m     ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[1;32m     46\u001b[0m     questions\u001b[38;5;241m=\u001b[39mquestions,\n\u001b[1;32m     47\u001b[0m     few_shot_prompt\u001b[38;5;241m=\u001b[39mfew_shot_prompt,\n\u001b[1;32m     48\u001b[0m     prompt_type\u001b[38;5;241m=\u001b[39mprompt_type,\n\u001b[1;32m     49\u001b[0m     save_path\u001b[38;5;241m=\u001b[39msave_path,\n\u001b[1;32m     50\u001b[0m     already_answered_ids\u001b[38;5;241m=\u001b[39malready_answered_ids\n\u001b[1;32m     51\u001b[0m )\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing complete for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(ids_answered)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m new answers saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[216], line 39\u001b[0m, in \u001b[0;36mquery_llm\u001b[0;34m(llm_model, ids, questions, few_shot_prompt, prompt_type, save_path, already_answered_ids)\u001b[0m\n\u001b[1;32m     37\u001b[0m     answer \u001b[38;5;241m=\u001b[39m query_llama_70b(prompt)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m llm_model \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllama3.1405b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 39\u001b[0m     answer \u001b[38;5;241m=\u001b[39m query_llama_405b(prompt)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported LLM model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mllm_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[206], line 63\u001b[0m, in \u001b[0;36mquery_llama_405b\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquery_llama_405b\u001b[39m(prompt):\n\u001b[1;32m     58\u001b[0m     client \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mOpenAI(\n\u001b[1;32m     59\u001b[0m         api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSAMBANOVA_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     60\u001b[0m         base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api.sambanova.ai/v1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     61\u001b[0m     )\n\u001b[0;32m---> 63\u001b[0m     response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     64\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMeta-Llama-3.1-405B-Instruct\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     65\u001b[0m             messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     66\u001b[0m                 {\n\u001b[1;32m     67\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     68\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt,\n\u001b[1;32m     69\u001b[0m                 }\n\u001b[1;32m     70\u001b[0m             ],\n\u001b[1;32m     71\u001b[0m         temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m, \u001b[38;5;66;03m# Meta default\u001b[39;00m\n\u001b[1;32m     72\u001b[0m         top_p \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.9\u001b[39m \u001b[38;5;66;03m# Meta default\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     )\n\u001b[1;32m     74\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m5\u001b[39m)  \u001b[38;5;66;03m# Pause execution for 2 seconds\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/openai/_utils/_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py:646\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    644\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    645\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 646\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    647\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    648\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m    649\u001b[0m             {\n\u001b[1;32m    650\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m    651\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    652\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m    653\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m    654\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m    655\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m    656\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m    657\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m    658\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m    659\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m    660\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m    661\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m    662\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m    663\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m    664\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m    665\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m    666\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m    667\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    668\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m    669\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m    670\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m    671\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    672\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m    673\u001b[0m             },\n\u001b[1;32m    674\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m    675\u001b[0m         ),\n\u001b[1;32m    676\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    677\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    678\u001b[0m         ),\n\u001b[1;32m    679\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m    680\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    681\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[1;32m    682\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/openai/_base_client.py:1271\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1258\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1259\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1266\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1267\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1268\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1269\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1270\u001b[0m     )\n\u001b[0;32m-> 1271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/openai/_base_client.py:942\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    935\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    940\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    941\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 942\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    943\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    944\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    945\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    946\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    947\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[1;32m    948\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/openai/_base_client.py:978\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    975\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl)\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 978\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend(\n\u001b[1;32m    979\u001b[0m         request,\n\u001b[1;32m    980\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_stream_response_body(request\u001b[38;5;241m=\u001b[39mrequest),\n\u001b[1;32m    981\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    982\u001b[0m     )\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    984\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    906\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[1;32m    910\u001b[0m )\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_auth(\n\u001b[1;32m    915\u001b[0m     request,\n\u001b[1;32m    916\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[1;32m    917\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m    918\u001b[0m     history\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m    919\u001b[0m )\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_redirects(\n\u001b[1;32m    943\u001b[0m         request,\n\u001b[1;32m    944\u001b[0m         follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m    945\u001b[0m         history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[1;32m    946\u001b[0m     )\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_single_request(request)\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/httpx/_client.py:1015\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1011\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1012\u001b[0m     )\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1015\u001b[0m     response \u001b[38;5;241m=\u001b[39m transport\u001b[38;5;241m.\u001b[39mhandle_request(request)\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1019\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/httpx/_transports/default.py:233\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    220\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    221\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    222\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    231\u001b[0m )\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    238\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    239\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    240\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    241\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    242\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, Iterable)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mhandle_request(\n\u001b[1;32m    197\u001b[0m         pool_request\u001b[38;5;241m.\u001b[39mrequest\n\u001b[1;32m    198\u001b[0m     )\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/httpcore/_sync/http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/httpcore/_sync/http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    107\u001b[0m     (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m         trailing_data,\n\u001b[0;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_response_headers(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m         http_version,\n\u001b[1;32m    116\u001b[0m         status,\n\u001b[1;32m    117\u001b[0m         reason_phrase,\n\u001b[1;32m    118\u001b[0m         headers,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    121\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/httpcore/_sync/http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    183\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_event(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/httpcore/_sync/http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\u001b[38;5;241m.\u001b[39mread(\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mREAD_NUM_BYTES, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    226\u001b[0m     )\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv(max_bytes)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/ssl.py:1233\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1230\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1231\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1232\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(buflen)\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/ssl.py:1106\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# llm_model = 'llama3.18b'\n",
    "project_root = '/Users/log/Github/textual_grounding/'\n",
    "prompt_type = 'cot'\n",
    "few_shot_txt = 'cot_examples.txt'\n",
    "sample_size = 200\n",
    "isLongest = False \n",
    "isRandom = False\n",
    "isShortest = False\n",
    "identifier = 'shortest'\n",
    "\n",
    "models = ['llama3.1405b']\n",
    "# models = ['llama3.170b']\n",
    "for model in models:\n",
    "    run_model(model, prompt_type, few_shot_txt, sample_size, project_root, \n",
    "              identifier=identifier, \n",
    "              isLongest=isLongest, \n",
    "              isRandom=isRandom,\n",
    "              isShortest=isShortest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 13:04:53,717 - INFO - Batch input file created at: /Users/log/Github/textual_grounding/logan/batch_files/VanillaCoT/MultiArith/claude-3-5-sonnet-20240620/zero_shot_vanilla_cot_None_MultiArith_claude-3-5-sonnet-20240620.jsonl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'custom_id': '397', 'params': {'model': 'claude-3-5-sonnet-20240620', 'max_tokens': 1024, 'messages': [{'role': 'user', 'content': \"Debby's class is going on a field trip to the zoo. If each van can hold 9 people and there are 40 students and 14 adults going, how many vans will they need?\\n\\nThink through your answer step by step. Put the concise form of your final answer in curly brackets e.g. {A}, {True} or {3.0}.\\n\"}]}}, {'custom_id': '433', 'params': {'model': 'claude-3-5-sonnet-20240620', 'max_tokens': 1024, 'messages': [{'role': 'user', 'content': 'John had 5 action figures, but needed 7 total for a complete collection. If each one costs $5, how much money would he need to finish his collection?\\n\\nThink through your answer step by step. Put the concise form of your final answer in curly brackets e.g. {A}, {True} or {3.0}.\\n'}]}}]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 13:04:54,013 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages/batches?beta=true \"HTTP/1.1 200 OK\"\n",
      "2024-11-22 13:04:54,018 - INFO - Created batch with ID: msgbatch_013bywEtURWwy2zHNsPPiPwR\n",
      "2024-11-22 13:04:54,146 - INFO - HTTP Request: GET https://api.anthropic.com/v1/messages/batches/msgbatch_013bywEtURWwy2zHNsPPiPwR?beta=true \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 55\u001b[0m\n\u001b[1;32m     45\u001b[0m     prompts\u001b[38;5;241m.\u001b[39mappend(prompt)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# tasks = batch_agents.prepare_batch_input(\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m#     llm_model=llm_model,\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m#     ids=ids,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m \n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m#Execute the batch processing with GPT-4 and save results\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m batch_agents\u001b[38;5;241m.\u001b[39mbatch_api_agent(\n\u001b[1;32m     56\u001b[0m     llm_model\u001b[38;5;241m=\u001b[39mllm_model,\n\u001b[1;32m     57\u001b[0m     ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[1;32m     58\u001b[0m     prompts\u001b[38;5;241m=\u001b[39mprompts,\n\u001b[1;32m     59\u001b[0m     batch_output_file\u001b[38;5;241m=\u001b[39mbatch_output_file,\n\u001b[1;32m     60\u001b[0m     batch_results_file\u001b[38;5;241m=\u001b[39mbatch_results_path\n\u001b[1;32m     61\u001b[0m )\n",
      "File \u001b[0;32m~/Github/textual_grounding/agents/batch_api_agents.py:264\u001b[0m, in \u001b[0;36mbatch_api_agent\u001b[0;34m(llm_model, ids, prompts, temperature, max_tokens, batch_output_file, batch_results_file, poll_interval, max_wait_time)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m message_batch\u001b[38;5;241m.\u001b[39mprocessing_status \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch processing failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage_batch\u001b[38;5;241m.\u001b[39merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 264\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(poll_interval)\n\u001b[1;32m    265\u001b[0m wait_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m poll_interval\n\u001b[1;32m    266\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch Status: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage_batch\u001b[38;5;241m.\u001b[39mprocessing_status\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Time elapsed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwait_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from agents.batch_api_agents import prepare_batch_input, batch_api_agent\n",
    "from openai import OpenAI\n",
    "import importlib\n",
    "import agents.batch_api_agents as batch_agents\n",
    "\n",
    "# Reload the module to ensure the latest changes are loaded\n",
    "importlib.reload(batch_agents)\n",
    "\n",
    "llm_model = 'claude-3-5-sonnet-20240620'\n",
    "project_root = '/Users/log/Github/textual_grounding/'\n",
    "prompt_type = 'zero_shot_vanilla_cot'\n",
    "few_shot_txt = None\n",
    "sample_size = 2\n",
    "json_datasets = ['logical_deduction_seven_objects','reasoning_about_colored_objects']\n",
    "jsonl_datasets = ['GSM_Plus', 'MultiArith', 'SVAMP', 'p_GSM8K', 'StrategyQA', 'commonsenseQA','SPARTQA']\n",
    "all_datasets = jsonl_datasets + json_datasets\n",
    "all_datasets = ['MultiArith']\n",
    "for dataset in all_datasets:\n",
    "    if dataset in json_datasets:\n",
    "        data_path = os.path.join(project_root, 'data', dataset, 'test.json')\n",
    "    else:\n",
    "        data_path = os.path.join(project_root, 'data', dataset, 'test.jsonl')\n",
    "        \n",
    "    save_dir = os.path.join(project_root, 'logan/results/final/VanillaCoT', dataset, f'{llm_model}')\n",
    "    batch_dir = os.path.join(project_root, 'logan/batch_files/VanillaCoT', dataset, f'{llm_model}')\n",
    "    os.makedirs(save_dir, exist_ok=True)  # Ensure the directory exists\n",
    "    os.makedirs(batch_dir, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "    batch_results_path = os.path.join(save_dir, f'{prompt_type}_{few_shot_txt}_{dataset}_{llm_model}.jsonl')   \n",
    "    batch_output_file = os.path.join(batch_dir, f'{prompt_type}_{few_shot_txt}_{dataset}_{llm_model}.jsonl')   \n",
    "\n",
    "    ids, questions = load_data_size_specific(data_path, sample_size=sample_size)\n",
    "    if few_shot_txt:\n",
    "        with open(fewshot_prompt_path, 'r') as file:\n",
    "            few_shot_prompt = file.read()\n",
    "    else:\n",
    "        few_shot_prompt = \"\"\n",
    "    prompts = []\n",
    "    for question in questions:\n",
    "        prompt = get_prompt(prompt_type, few_shot_prompt, question)\n",
    "        # print(prompt)\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    # tasks = batch_agents.prepare_batch_input(\n",
    "    #     llm_model=llm_model,\n",
    "    #     ids=ids,\n",
    "    #     prompts=prompts,\n",
    "    #     batch_output_file=batch_output_file\n",
    "    # )\n",
    "\n",
    "    #Execute the batch processing with GPT-4 and save results\n",
    "    batch_agents.batch_api_agent(\n",
    "        llm_model=llm_model,\n",
    "        ids=ids,\n",
    "        prompts=prompts,\n",
    "        batch_output_file=batch_output_file,\n",
    "        batch_results_file=batch_results_path\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
