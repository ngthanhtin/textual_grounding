{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import os\n",
    "import sys\n",
    "import anthropic\n",
    "import ollama\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from google.generativeai.types import RequestOptions\n",
    "from google.api_core import retry\n",
    "\n",
    "from typing import List, Tuple\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import datetime\n",
    "import openai\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "from together import Together\n",
    "\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, TimeoutError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Shot - Vanilla CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_vanilla_cot = \"\"\"\n",
    "Think through your answer step by step. Put the concise form of your final answer in curly brackets e.g. {A}, {True} or {3.0}.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_last_sentence(text):\n",
    "    # Split the text at the point where multiple-choice options start (e.g., \"A)\", \"B)\", etc.)\n",
    "    parts = re.split(r'\\n[A-Z]\\)', text, flags=re.MULTILINE)\n",
    "    \n",
    "    if parts:\n",
    "        # The first part contains everything before the options\n",
    "        pre_options = parts[0]\n",
    "        \n",
    "        # Use a regex to find all sentences ending with ., ?, or !\n",
    "        sentences = re.findall(r'[^.!?]*[.!?]', pre_options, re.DOTALL)\n",
    "        \n",
    "        if sentences:\n",
    "            # Return the last sentence after stripping any extra spaces\n",
    "            return sentences[-1].strip()\n",
    "    \n",
    "    # If splitting or finding sentences fails, return the original text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# def extract_last_sentence(text):\n",
    "#     # Split the text into sentences, excluding option markers like \"A)\", \"B)\", etc.\n",
    "#     sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|!)(?=\\s[A-Z]\\))', text)\n",
    "    \n",
    "#     # Return the last sentence after stripping any extra spaces.\n",
    "#     return sentences[-1].strip() if sentences else text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(prompt_type: str, few_shot_prompt: str, question: str) -> str:\n",
    "    # print(question)\n",
    "    last_sentence = extract_last_sentence(question)\n",
    "    \n",
    "    # instruction = f\"I want you to answer this question but your explanation should contain references referring back to the information in the question. To do that, first, re-generate the question with proper tags for key phrases, the key phrases that are most relevant to answering the question {last_sentence} and then generate your answers. The output format is as follow:\\n\\\"\n",
    "    \n",
    "        # CHANGE FOR MATH\n",
    "    instruction = f\"I want you to answer this question but your explanation should contain references referring back to the information in the question. To do that, first, re-generate the question with proper tags for key phrases, the key phrases that are most relevant to answering the question and then generate your answers. The output format is as follow:\\n\\\n",
    "                Reformatted Question: \\\n",
    "                    Answer:\"\n",
    "    hot_instruction = f\"I want you to answer this question but your explanation should contain references referring back to the information in the question. To do that, first, re-generate the question with proper comment tags for key phrases, the key phrases that are most relevant to answering the question and then generate your answers. The output format is as follow:\\n\\\n",
    "                Reformatted Question: \\\n",
    "                    Answer:\"\n",
    "                    \n",
    "    prompts = {\n",
    "        \"zero_shot_vanilla_cot\": f\"{question}\\n{zero_shot_vanilla_cot}\",\n",
    "        \"echo_fewshot\": f\"{few_shot_prompt}\\n\\nQuestion:{question}\\nRepeat the question and then think through your answer step by step. Put the concise form of your final answer in curly brackets e.g. {{A}}, {{True}} or {{3.0}}.\",\n",
    "        \"gcot\": f\"{few_shot_prompt}\\n{question}\\n{instruction}\",\n",
    "        \"cot\": f\"{few_shot_prompt}\\n{question}\\nPlease generate your explanation first, then generate the final answer in the bracket as follow:\\nAnswer: {{}}\",\n",
    "        \"hot\": f\"{few_shot_prompt}\\n{question}\\n{hot_instruction}\",\n",
    "    }\n",
    "    return prompts[prompt_type]\n",
    "\n",
    "def save_results(save_path: str, ids: List[str], questions: List[str], answers: List[str], append: bool = False):\n",
    "    df = pd.DataFrame({'id': ids, 'question': questions, 'answer': answers})\n",
    "    if append and os.path.exists(save_path):\n",
    "        df.to_csv(save_path, mode='a', index=False, header=False)\n",
    "    else:\n",
    "        df.to_csv(save_path, index=False)\n",
    "\n",
    "def read_jsonl_file(filepath: str) -> List[dict]:\n",
    "    data = []\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            json_obj = json.loads(line)\n",
    "            data.append(json_obj)\n",
    "    return data\n",
    "\n",
    "def load_already_answered_ids(save_path: str) -> set:\n",
    "    if os.path.exists(save_path):\n",
    "        df = pd.read_csv(save_path)\n",
    "        answered_ids = set(df['id'].tolist())\n",
    "        # print(f\"Loaded {len(answered_ids)} already answered IDs from: {save_path}\")\n",
    "        print(f\"Already answered IDs: {answered_ids}\")\n",
    "        return answered_ids\n",
    "    else:\n",
    "        print(f\"No existing save file found at: {save_path}. Starting fresh.\")\n",
    "        return set()\n",
    "\n",
    "def initialize_save_file(save_path: str):\n",
    "    if not os.path.exists(save_path):\n",
    "        # Create an empty DataFrame with headers and save\n",
    "        df = pd.DataFrame(columns=['id', 'question', 'answer'])\n",
    "        df.to_csv(save_path, index=False)\n",
    "        print(f\"Initialized new save file with headers at: {save_path}\")\n",
    "\n",
    "def load_data_size_specific(data_path: str, sample_size: int = 0, random_seed: int = 0):\n",
    "    random.seed(random_seed)\n",
    "\n",
    "    if data_path.endswith('.jsonl'):\n",
    "        data = read_jsonl_file(data_path)\n",
    "    elif data_path.endswith('.json'):\n",
    "        with open(data_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "    \n",
    "    question_length = 0\n",
    "    eligible_data = [x for x in data if len(x[\"question\"]) >= question_length]\n",
    "    \n",
    "    if sample_size > 0 and sample_size < len(eligible_data):\n",
    "        sampled_data = random.sample(eligible_data, sample_size)\n",
    "    else:\n",
    "        sampled_data = eligible_data\n",
    "    \n",
    "    ids = [x[\"id\"] for x in sampled_data]\n",
    "    # ids = [x[\"unique_id\"] for x in sampled_data]\n",
    "    questions = [x[\"question\"] for x in sampled_data]\n",
    "    \n",
    "    return ids, questions\n",
    "\n",
    "        \n",
    "def get_longest_questions_and_ids(data_path, sample_size=200):\n",
    "    # data = read_jsonl_file(data_path)\n",
    "    if data_path.endswith('.json'):\n",
    "        with open(data_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "    full_ids = [x[\"id\"] for x in data]\n",
    "    full_questions = [x[\"question\"] for x in data]\n",
    "\n",
    "    # Combine questions and IDs into a list of tuples\n",
    "    full_questions_ids = list(zip(full_questions, full_ids))\n",
    "    \n",
    "    # Sort the tuples by the length of the questions\n",
    "    sorted_full_questions_ids = sorted(full_questions_ids, key=lambda x: len(x[0]), reverse=True)\n",
    "    \n",
    "    # Select the shortest questions and their IDs\n",
    "    sampled_data = random.sample(data, sample_size)\n",
    "    longest_ = sorted_full_questions_ids[:min(sample_size, len(sorted_full_questions_ids))]\n",
    "\n",
    "    # Separate them back into two lists\n",
    "    longest_questions, longest_ids = zip(*longest_)\n",
    "\n",
    "    # Convert to lists if necessary\n",
    "    longest_questions = list(longest_questions)\n",
    "    longest_ids = list(longest_ids)\n",
    "    \n",
    "    return longest_ids, longest_questions\n",
    "\n",
    "def get_shortest_questions_and_ids(data_path, sample_size=200):\n",
    "    if data_path.endswith('.json'):\n",
    "        with open(data_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "    else:\n",
    "        data = read_jsonl_file(data_path)\n",
    "    full_ids = [x[\"id\"] for x in data]\n",
    "    full_questions = [x[\"question\"] for x in data]\n",
    "\n",
    "    # Combine questions and IDs into a list of tuples\n",
    "    full_questions_ids = list(zip(full_questions, full_ids))\n",
    "    \n",
    "    # Sort the tuples by the length of the questions in ascending order\n",
    "    sorted_full_questions_ids = sorted(full_questions_ids, key=lambda x: len(x[0]))\n",
    "    \n",
    "    # Select the shortest questions and their IDs\n",
    "    sampled_data = random.sample(data, sample_size)\n",
    "    shortest_ = sorted_full_questions_ids[:min(sample_size, len(sorted_full_questions_ids))]\n",
    "\n",
    "    # Separate them back into two lists\n",
    "    shortest_questions, shortest_ids = zip(*shortest_)\n",
    "\n",
    "    # Convert to lists if necessary\n",
    "    shortest_questions = list(shortest_questions)\n",
    "    shortest_ids = list(shortest_ids)\n",
    "    \n",
    "    return shortest_ids, shortest_questions\n",
    "\n",
    "    \n",
    "def get_random_questions_and_ids(data_path, sample_size=200):\n",
    "\n",
    "    data = read_jsonl_file(data_path)\n",
    "\n",
    "    longest_questions, longest_ids = get_longest_questions_and_ids(data_path, sample_size)\n",
    "    \n",
    "    result_ids = []\n",
    "    result_questions = []\n",
    "    # Select the shortest questions and their IDs\n",
    "    sampled_data = random.sample(data, sample_size*2)\n",
    "    totalQuestions = 0\n",
    "    for x in sampled_data:\n",
    "        if totalQuestions >= sample_size:\n",
    "            break\n",
    "        if x[\"question\"] not in longest_questions:\n",
    "            result_ids.append(x[\"id\"])\n",
    "            result_questions.append(x[\"question\"])\n",
    "            totalQuestions += 1\n",
    "    \n",
    "    return result_ids, result_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_4o(prompt: str) -> str:\n",
    "    client = OpenAI()\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-2024-08-06\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{prompt}\"\n",
    "            }\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "def query_llama(prompt):\n",
    "    client = openai.OpenAI(\n",
    "        api_key=os.environ.get(\"SAMBANOVA_API_KEY\"),\n",
    "        base_url=\"https://api.sambanova.ai/v1\",\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model='Meta-Llama-3.1-8B-Instruct',\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "        temperature=0.6, # Meta default\n",
    "        top_p = 0.9 # Meta default\n",
    "    )\n",
    "    time.sleep(2)  # Pause execution for 2 seconds\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def query_llama_70b(prompt):\n",
    "    client = openai.OpenAI(\n",
    "        api_key=os.environ.get(\"SAMBANOVA_API_KEY\"),\n",
    "        base_url=\"https://api.sambanova.ai/v1\",\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model='Meta-Llama-3.1-70B-Instruct',\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "        temperature=0.6, # Meta default\n",
    "        top_p = 0.9 # Meta default\n",
    "    )\n",
    "    time.sleep(2)  # Pause execution for 2 seconds\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def query_llama_405b(prompt):\n",
    "    client = openai.OpenAI(\n",
    "        api_key=os.environ.get(\"SAMBANOVA_API_KEY\"),\n",
    "        base_url=\"https://api.sambanova.ai/v1\",\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model='Meta-Llama-3.1-405B-Instruct',\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "        temperature=0.6, # Meta default\n",
    "        top_p = 0.9 # Meta default\n",
    "    )\n",
    "    time.sleep(7)  # Pause execution for 2 seconds\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "    \n",
    "def query_together(prompt):\n",
    "    client = Together()\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    print(prompt)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm(llm_model: str, ids: List[str], questions: List[str], few_shot_prompt: str, prompt_type: str, save_path: str, already_answered_ids: set) -> Tuple[List[str], List[str], List[str]]:\n",
    "    answers = []\n",
    "    ids_can_be_answered = []\n",
    "    questions_can_be_answered = []\n",
    "    \n",
    "    for id, q in tqdm(zip(ids, questions), total=len(ids)):\n",
    "        # print(q)\n",
    "        # print(f\"Processing ID: {id}\")\n",
    "        if id in already_answered_ids:\n",
    "            print(f\"Skipping: {id}\", end=' ')\n",
    "            continue\n",
    "        # if id == 1146: # weird ID that breaks llama\n",
    "        #     continue\n",
    "        \n",
    "        # print(few_shot_prompt)\n",
    "        prompt = get_prompt(prompt_type, few_shot_prompt, q)\n",
    "        # print(prompt)\n",
    "        try:\n",
    "            # print(prompt)\n",
    "            if llm_model == 'gemini':\n",
    "                answer = query_gemini(prompt, id)\n",
    "            elif llm_model == 'claude':\n",
    "                answer = query_claude(prompt)\n",
    "            elif llm_model == '4o':\n",
    "                # answer = query_4o_multiturn(prompt)\n",
    "                if prompt_type == 'multi_convo':\n",
    "                    fact_prompt = get_prompt(prompt_type=\"fact_prompt\", few_shot_prompt=\"\", question=q)\n",
    "                    \n",
    "                    answer_prompt = get_prompt(prompt_type=\"answer_prompt_data\", few_shot_prompt=\"\", question=q)\n",
    "                    answer = query_4o_multiconvo(fact_prompt=fact_prompt, answer_prompt=answer_prompt, extracted_question=q)\n",
    "                else:\n",
    "                    answer = query_4o(prompt)\n",
    "                \n",
    "            elif llm_model == 'llama3.18b':\n",
    "                answer = query_llama(prompt)\n",
    "            elif llm_model == 'llama3.170b':\n",
    "                answer = query_llama_70b(prompt)\n",
    "            elif llm_model == 'llama3.1405b':\n",
    "                answer = query_llama_405b(prompt)\n",
    "            elif llm_model == 'together':\n",
    "                answer = query_together(prompt)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported LLM model: {llm_model}\")\n",
    "            # print(f\"Answer for ID {id}: {answer}\")\n",
    "            \n",
    "            answers.append(answer)\n",
    "            questions_can_be_answered.append(q)\n",
    "            ids_can_be_answered.append(id)\n",
    "\n",
    "            # Save after each answer\n",
    "            save_results(save_path, [id], [q], [answer], append=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question {id}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return ids_can_be_answered, questions_can_be_answered, answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_datasets = ['logical_deduction_seven_objects','reasoning_about_colored_objects', 'squad']\n",
    "jsonl_datasets = ['GSM8K', 'date', 'GSM_Plus', 'MultiArith', 'ASDiv', 'SVAMP', 'AQUA', 'p_GSM8K', 'StrategyQA', 'commonsenseQA','SPARTQA']\n",
    "all_datasets = jsonl_datasets + json_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(llm_model, prompt_type, few_shot_txt, sample_size, project_root, identifier, isRandom = False, isLongest = False, isShortest = False):\n",
    "    # for dataset in all_datasets:\n",
    "    for dataset in ['MATH']:\n",
    "        print(f\"------Processing dataset: {dataset}-------\")\n",
    "        if few_shot_txt:\n",
    "            # fewshot_prompt_path = os.path.join(project_root, \"prompt\", dataset, few_shot_txt)\n",
    "            # fewshot_prompt_path = '/Users/log/Github/textual_grounding/prompt/GSM8K/fewshot_design_1_v4.txt'\n",
    "            fewshot_prompt_path = '/Users/log/Github/textual_grounding/prompt/MATH/comment_hot_examples.txt'\n",
    "        # print(fewshot_prompt_path)\n",
    "        # continue \n",
    "        if prompt_type == 'zero_shot_vanilla_cot':\n",
    "            save_dir = os.path.join(project_root, 'logan/results/final/VanillaCoT', dataset, f'{llm_model}')\n",
    "        elif prompt_type == 'gcot':\n",
    "            save_dir = os.path.join(project_root, 'logan/results/final/GCoT', dataset, f'{llm_model}')\n",
    "        elif prompt_type == 'cot':\n",
    "            save_dir = os.path.join(project_root, 'logan/results/final/fewshot_CoT', dataset, f'{llm_model}')\n",
    "        else:\n",
    "            save_dir = os.path.join(project_root, 'logan/results', dataset, f'{llm_model}')\n",
    "        os.makedirs(save_dir, exist_ok=True)  # Ensure the directory exists\n",
    "        save_path = os.path.join(save_dir, f'{prompt_type}_{identifier}_{few_shot_txt}_{dataset}_{llm_model}.csv')\n",
    "\n",
    "        if dataset in json_datasets:\n",
    "            data_path = os.path.join(project_root, 'data', dataset, 'test.json')\n",
    "        else:\n",
    "            # data_path = os.path.join(project_root, 'data', dataset, 'test.jsonl')\n",
    "            # data_path = '/Users/log/Github/textual_grounding/data/gsm_symbolic/main_formatted_test.jsonl'\n",
    "            data_path = '/Users/log/Github/textual_grounding/data/MATH/test.jsonl'\n",
    "\n",
    "\n",
    "        if isRandom:\n",
    "            ids, questions = get_random_questions_and_ids(data_path, sample_size=sample_size)\n",
    "        elif isLongest:\n",
    "            ids, questions = get_longest_questions_and_ids(data_path, sample_size=sample_size)\n",
    "        elif isShortest:\n",
    "            ids, questions = get_shortest_questions_and_ids(data_path, sample_size=sample_size)\n",
    "        else:\n",
    "            ids, questions = load_data_size_specific(data_path, sample_size=sample_size)\n",
    "        print(sorted(ids))\n",
    "\n",
    "        if few_shot_txt:\n",
    "            with open(fewshot_prompt_path, 'r') as file:\n",
    "                few_shot_prompt = file.read()\n",
    "        else:\n",
    "            few_shot_prompt = \"\"\n",
    "\n",
    "        initialize_save_file(save_path)\n",
    "        already_answered_ids = load_already_answered_ids(save_path)\n",
    "\n",
    "        ids_answered, questions_answered, answers = query_llm(\n",
    "            llm_model=llm_model,\n",
    "            ids=ids,\n",
    "            questions=questions,\n",
    "            few_shot_prompt=few_shot_prompt,\n",
    "            prompt_type=prompt_type,\n",
    "            save_path=save_path,\n",
    "            already_answered_ids=already_answered_ids\n",
    "        )\n",
    "\n",
    "        print(f\"Processing complete for {dataset}. {len(ids_answered)} new answers saved to {save_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Processing dataset: MATH-------\n",
      "['test/algebra/1031.json', 'test/algebra/1082.json', 'test/algebra/1098.json', 'test/algebra/1143.json', 'test/algebra/1184.json', 'test/algebra/1255.json', 'test/algebra/1265.json', 'test/algebra/1275.json', 'test/algebra/1332.json', 'test/algebra/1339.json', 'test/algebra/170.json', 'test/algebra/187.json', 'test/algebra/1934.json', 'test/algebra/2043.json', 'test/algebra/2064.json', 'test/algebra/224.json', 'test/algebra/2264.json', 'test/algebra/2391.json', 'test/algebra/251.json', 'test/algebra/2517.json', 'test/algebra/2570.json', 'test/algebra/2593.json', 'test/algebra/276.json', 'test/algebra/297.json', 'test/algebra/305.json', 'test/algebra/634.json', 'test/algebra/661.json', 'test/algebra/668.json', 'test/algebra/694.json', 'test/algebra/824.json', 'test/algebra/907.json', 'test/algebra/972.json', 'test/counting_and_probability/1114.json', 'test/counting_and_probability/134.json', 'test/counting_and_probability/190.json', 'test/counting_and_probability/199.json', 'test/counting_and_probability/23957.json', 'test/counting_and_probability/389.json', 'test/counting_and_probability/71.json', 'test/counting_and_probability/803.json', 'test/geometry/106.json', 'test/geometry/347.json', 'test/geometry/456.json', 'test/geometry/547.json', 'test/geometry/627.json', 'test/geometry/711.json', 'test/geometry/802.json', 'test/geometry/880.json', 'test/geometry/947.json', 'test/geometry/967.json', 'test/intermediate_algebra/1102.json', 'test/intermediate_algebra/117.json', 'test/intermediate_algebra/1217.json', 'test/intermediate_algebra/1279.json', 'test/intermediate_algebra/1365.json', 'test/intermediate_algebra/1405.json', 'test/intermediate_algebra/1462.json', 'test/intermediate_algebra/1510.json', 'test/intermediate_algebra/158.json', 'test/intermediate_algebra/1714.json', 'test/intermediate_algebra/1797.json', 'test/intermediate_algebra/1806.json', 'test/intermediate_algebra/2015.json', 'test/intermediate_algebra/2146.json', 'test/intermediate_algebra/362.json', 'test/intermediate_algebra/834.json', 'test/intermediate_algebra/960.json', 'test/number_theory/1002.json', 'test/number_theory/1055.json', 'test/number_theory/1287.json', 'test/number_theory/516.json', 'test/number_theory/533.json', 'test/number_theory/686.json', 'test/number_theory/691.json', 'test/number_theory/72.json', 'test/number_theory/838.json', 'test/number_theory/931.json', 'test/prealgebra/1003.json', 'test/prealgebra/105.json', 'test/prealgebra/1114.json', 'test/prealgebra/1203.json', 'test/prealgebra/1227.json', 'test/prealgebra/1238.json', 'test/prealgebra/1251.json', 'test/prealgebra/1302.json', 'test/prealgebra/1423.json', 'test/prealgebra/1686.json', 'test/prealgebra/1742.json', 'test/prealgebra/1807.json', 'test/prealgebra/2019.json', 'test/prealgebra/2057.json', 'test/prealgebra/996.json', 'test/precalculus/285.json', 'test/precalculus/43.json', 'test/precalculus/580.json', 'test/precalculus/807.json', 'test/precalculus/881.json', 'test/precalculus/920.json', 'test/precalculus/927.json', 'test/precalculus/96.json']\n",
      "Already answered IDs: {'test/algebra/907.json', 'test/number_theory/838.json', 'test/counting_and_probability/190.json', 'test/algebra/1031.json', 'test/intermediate_algebra/1462.json'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping: test/intermediate_algebra/1462.json Skipping: test/number_theory/838.json Skipping: test/algebra/1031.json Skipping: test/counting_and_probability/190.json Skipping: test/algebra/907.json "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [00:03<00:56,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the value of $%<fact1>\n",
      "9^3%</fact1>\n",
      " + %<fact2>\n",
      "3(9^2)%</fact2>\n",
      " + %<fact3>\n",
      "3(9)%</fact3>\n",
      " + %<fact4>\n",
      "1%</fact4>\n",
      "$?\n",
      "Answer: The given expression is the expansion of $(9+1)^3$.  In general, the cube of $(x+y)^3$ is \\\\[(x+y)^3=1x^3+3x^2y+3xy^2+1y^3.\\\\]   The %<fact1>\n",
      "first%</fact1>\n",
      " and %<fact4>\n",
      "last terms%</fact4>\n",
      " in the given expression are cubes and the %<fact2>\n",
      "second%</fact2>\n",
      " and %<fact3>\n",
      "third terms%</fact3>\n",
      " both have coefficient 3, giving us a clue that this is a cube of a binomial and can be written in the form \\\\[(x+y)^3\\\\]In this case, $x=9$ and $y=1$, so our answer is\\\\[(9+1)^3\\\\ = 10^3 = \\\\boxed{1000}\\\\]\n",
      "\n",
      "Question: Let \\\\[f(x) = \\\\left\\\\{\\n\\\\begin{array}{cl} %<fact1>\n",
      "ax+3%</fact1>\n",
      ", &\\\\text{ if }%<fact2>\n",
      "x>2%</fact2>\n",
      ", \\\\\\\\\\n%<fact3>\n",
      "x-5%</fact3>\n",
      " &\\\\text{ if } %<fact4>\n",
      "-2 \\\\le x \\\\le 2%<fact4>\n",
      ", \\\\\\\\\\n%<fact5>\n",
      "2x-b%</fact5>\n",
      " &\\\\text{ if } %<fact6>\n",
      "x <-2%<fact6>\n",
      ".\\n\\\\end{array}\\n\\\\right.\\\\]Find <fact7>$a+b$</fact7> if the piecewise function is <fact8>continuous</fact8> (which means that its graph can be drawn without lifting your pencil from the paper).\n",
      "Answer: For the piecewise function to be <fact8>continuous</fact8>, the cases must \\\"meet\\\" at $2$ and $-2$. For example, %<fact1>\n",
      "$ax+3$%</fact1>\n",
      " and %<fact3>\n",
      "$x-5$%</fact3>\n",
      " must be equal when $x=2$. This implies $a(2)+3=2-5$, which we solve to get $2a=-6 \\\\Rightarrow a=-3$. Similarly, %<fact3>\n",
      "$x-5$%</fact3>\n",
      " and %<fact5>\n",
      "$2x-b$%</fact5>\n",
      " must be equal when $x=-2$. Substituting, we get $-2-5=2(-2)-b$, which implies $b=3$. So $<fact7>a+b</fact7>=-3+3=\\\\boxed{0}$.\n",
      "\n",
      "Question: %<fact1>\n",
      "Square ABCD%</fact1>\n",
      " has its center at %<fact2>\n",
      "$(8,-8)$%</fact2>\n",
      " and has an %<fact3>\n",
      "area of 4 square units%</fact3>\n",
      ". The top side of the square is horizontal. The square is then dilated with the %<fact4>\n",
      "dilation center at (0,0)%</fact4>\n",
      " and a %<fact5>\n",
      "scale factor of 2%</fact5>\n",
      ". What are the %<fact6>\n",
      "coordinates of the vertex of the image%</fact6>\n",
      " of square ABCD that is <fact7>farthest from the origin</fact7>? Give your answer as an ordered pair.\n",
      "Answer: With the %<fact4>\n",
      "center of dilation at the origin%</fact4>\n",
      " and a %<fact5>\n",
      "scale factor of 2%</fact5>\n",
      ", all the coordinates of %<fact1>\n",
      "square $ABCD$%</fact1>\n",
      " are twice the coordinates of its preimage. The preimage has an %<fact3>\n",
      "area of 4 square units%</fact3>\n",
      ", so its side length is 2 units. Since the center of the preimage is at %<fact2>\n",
      "$(8, -8)$%</fact2>\n",
      ", the four %<fact6>\n",
      "vertices%</fact6>\n",
      " of the preimage are at $(7, -9), (7, -7), (9, -7)$ and $(9, -9)$. The point $(9, -9)$ is the farthest from the origin on the preimage, so the point <fact7>farthest from the origin</fact7> on the image of %<fact1>\n",
      "square $ABCD$%</fact1>\n",
      " is $\\\\boxed{(18, -18)}.$\n",
      "\n",
      "Question: Shown below are rows 1, 2, and 3 of Pascal's triangle.\\n\\n\\\\[\\n\\\\begin{array}{ccccccc}\\n& & 1 & & 1 & & \\\\\\\\\\n& 1 & & 2 & & 1 & \\\\\\\\\\n1 & & 3 & & 3 & & 1\\n\\\\end{array}\\n\\\\]Let %<fact1>\n",
      "$(a_i),$%</fact1>\n",
      " %<fact2>\n",
      "$(b_i),$%</fact2>\n",
      " %<fact3>\n",
      "$(c_i)$%</fact3>\n",
      " be the sequence, from left to right, of elements in the %<fact4>\n",
      "2005th%</fact4>\n",
      ", %<fact5>\n",
      "2006th%</fact5>\n",
      ", and 2007th rows, respectively, with the leftmost element occurring at %<fact6>\n",
      "$i = 0.$%</fact6>\n",
      "  Compute\\n\\\\[\\\\sum_{%<fact6>\n",
      "i = 0%</fact6>\n",
      "}^{%<fact5>\n",
      "2006%</fact5>\n",
      "} \\\\frac{%<fact2>\n",
      "b_i%</fact2>\n",
      "}{%<fact3>\n",
      "c_i%</fact3>\n",
      "} - \\\\sum_{%<fact6>\n",
      "i = 0%</fact3>\n",
      "}^{%<fact4>\n",
      "2005%</fact4>\n",
      "} \\\\frac{%<fact1>\n",
      "a_i%</fact1>\n",
      "}{%<fact2>\n",
      "b_i%</fact2>\n",
      "}.\\\\]\n",
      "Answer: More generally, suppose %<fact1>\n",
      "$(a_i),$%</fact1>\n",
      " %<fact2>\n",
      "$(b_i),$%</fact2>\n",
      "%<fact3>\n",
      "$(c_i)$%</fact3>\n",
      " represent the entries \n",
      "Evaluate $(1+2i)6-3i$.\n",
      "I want you to answer this question but your explanation should contain references referring back to the information in the question. To do that, first, re-generate the question with proper tags for key phrases, the key phrases that are most relevant to answering the question and then generate your answers. The output format is as follow:\n",
      "                Reformatted Question:                     Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [00:05<01:26,  1.09it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m models \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtogether\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[0;32m---> 13\u001b[0m     run_model(model, prompt_type, few_shot_txt, sample_size, project_root, \n\u001b[1;32m     14\u001b[0m               identifier\u001b[38;5;241m=\u001b[39midentifier, \n\u001b[1;32m     15\u001b[0m               isLongest\u001b[38;5;241m=\u001b[39misLongest, \n\u001b[1;32m     16\u001b[0m               isRandom\u001b[38;5;241m=\u001b[39misRandom,\n\u001b[1;32m     17\u001b[0m               isShortest\u001b[38;5;241m=\u001b[39misShortest)\n",
      "Cell \u001b[0;32mIn[24], line 49\u001b[0m, in \u001b[0;36mrun_model\u001b[0;34m(llm_model, prompt_type, few_shot_txt, sample_size, project_root, identifier, isRandom, isLongest, isShortest)\u001b[0m\n\u001b[1;32m     46\u001b[0m initialize_save_file(save_path)\n\u001b[1;32m     47\u001b[0m already_answered_ids \u001b[38;5;241m=\u001b[39m load_already_answered_ids(save_path)\n\u001b[0;32m---> 49\u001b[0m ids_answered, questions_answered, answers \u001b[38;5;241m=\u001b[39m query_llm(\n\u001b[1;32m     50\u001b[0m     llm_model\u001b[38;5;241m=\u001b[39mllm_model,\n\u001b[1;32m     51\u001b[0m     ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[1;32m     52\u001b[0m     questions\u001b[38;5;241m=\u001b[39mquestions,\n\u001b[1;32m     53\u001b[0m     few_shot_prompt\u001b[38;5;241m=\u001b[39mfew_shot_prompt,\n\u001b[1;32m     54\u001b[0m     prompt_type\u001b[38;5;241m=\u001b[39mprompt_type,\n\u001b[1;32m     55\u001b[0m     save_path\u001b[38;5;241m=\u001b[39msave_path,\n\u001b[1;32m     56\u001b[0m     already_answered_ids\u001b[38;5;241m=\u001b[39malready_answered_ids\n\u001b[1;32m     57\u001b[0m )\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing complete for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(ids_answered)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m new answers saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[22], line 41\u001b[0m, in \u001b[0;36mquery_llm\u001b[0;34m(llm_model, ids, questions, few_shot_prompt, prompt_type, save_path, already_answered_ids)\u001b[0m\n\u001b[1;32m     39\u001b[0m     answer \u001b[38;5;241m=\u001b[39m query_llama_405b(prompt)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m llm_model \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtogether\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 41\u001b[0m     answer \u001b[38;5;241m=\u001b[39m query_together(prompt)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported LLM model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mllm_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[29], line 81\u001b[0m, in \u001b[0;36mquery_together\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquery_together\u001b[39m(prompt):\n\u001b[1;32m     79\u001b[0m     client \u001b[38;5;241m=\u001b[39m Together()\n\u001b[0;32m---> 81\u001b[0m     response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     82\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     83\u001b[0m         messages\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}]\n\u001b[1;32m     84\u001b[0m     )\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mprint\u001b[39m(prompt)\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/together/resources/chat/completions.py:141\u001b[0m, in \u001b[0;36mChatCompletions.create\u001b[0;34m(self, messages, model, max_tokens, stop, temperature, top_p, top_k, repetition_penalty, presence_penalty, frequency_penalty, min_p, logit_bias, seed, stream, logprobs, echo, n, safety_model, response_format, tools, tool_choice, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m requestor \u001b[38;5;241m=\u001b[39m api_requestor\u001b[38;5;241m.\u001b[39mAPIRequestor(\n\u001b[1;32m    113\u001b[0m     client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client,\n\u001b[1;32m    114\u001b[0m )\n\u001b[1;32m    116\u001b[0m parameter_payload \u001b[38;5;241m=\u001b[39m ChatCompletionRequest(\n\u001b[1;32m    117\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    118\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    139\u001b[0m )\u001b[38;5;241m.\u001b[39mmodel_dump(exclude_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 141\u001b[0m response, _, _ \u001b[38;5;241m=\u001b[39m requestor\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    142\u001b[0m     options\u001b[38;5;241m=\u001b[39mTogetherRequest(\n\u001b[1;32m    143\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    144\u001b[0m         url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    145\u001b[0m         params\u001b[38;5;241m=\u001b[39mparameter_payload,\n\u001b[1;32m    146\u001b[0m     ),\n\u001b[1;32m    147\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    148\u001b[0m )\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, TogetherResponse)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/together/abstract/api_requestor.py:242\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, options, stream, remaining_retries, request_timeout)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    233\u001b[0m     options: TogetherRequest,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    241\u001b[0m ]:\n\u001b[0;32m--> 242\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    243\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    244\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretries,\n\u001b[1;32m    245\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    246\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    247\u001b[0m     )\n\u001b[1;32m    249\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response(result, stream)\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/together/abstract/api_requestor.py:491\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[0;34m(self, options, remaining_retries, stream, request_timeout, absolute)\u001b[0m\n\u001b[1;32m    489\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 491\u001b[0m     result \u001b[38;5;241m=\u001b[39m _thread_context\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    492\u001b[0m         options\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    493\u001b[0m         abs_url,\n\u001b[1;32m    494\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    495\u001b[0m         data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m    496\u001b[0m         files\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39mfiles,\n\u001b[1;32m    497\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    498\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mrequest_timeout \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[1;32m    499\u001b[0m         proxies\u001b[38;5;241m=\u001b[39m_thread_context\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mproxies,\n\u001b[1;32m    500\u001b[0m         allow_redirects\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39mallow_redirects,\n\u001b[1;32m    501\u001b[0m     )\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    503\u001b[0m     utils\u001b[38;5;241m.\u001b[39mlog_debug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered requests.exceptions.Timeout\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/adapters.py:589\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    586\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 589\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    590\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    591\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    592\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[1;32m    593\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    594\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    595\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    596\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    597\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    598\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[1;32m    599\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    600\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    601\u001b[0m     )\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    790\u001b[0m     conn,\n\u001b[1;32m    791\u001b[0m     method,\n\u001b[1;32m    792\u001b[0m     url,\n\u001b[1;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    794\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    795\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    796\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    797\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[1;32m    798\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[1;32m    799\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[1;32m    800\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[1;32m    802\u001b[0m )\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/connection.py:464\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 464\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    467\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/http/client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1428\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[1;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/http/client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/http/client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/socket.py:708\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    710\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "project_root = '/Users/log/Github/textual_grounding/'\n",
    "prompt_type = 'gcot'\n",
    "few_shot_txt = '_'\n",
    "sample_size = 100\n",
    "isLongest = False \n",
    "isRandom = False\n",
    "isShortest = False\n",
    "identifier = 'main_test'\n",
    "\n",
    "# models = ['llama3.18b', 'llama3.170b', 'llama3.1405b']\n",
    "models = ['together']\n",
    "for model in models:\n",
    "    run_model(model, prompt_type, few_shot_txt, sample_size, project_root, \n",
    "              identifier=identifier, \n",
    "              isLongest=isLongest, \n",
    "              isRandom=isRandom,\n",
    "              isShortest=isShortest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from agents.batch_api_agents import prepare_batch_input, batch_api_agent\n",
    "from openai import OpenAI\n",
    "import importlib\n",
    "import agents.batch_api_agents as batch_agents\n",
    "\n",
    "# Reload the module to ensure the latest changes are loaded\n",
    "importlib.reload(batch_agents)\n",
    "\n",
    "llm_model = 'claude-3-5-sonnet-20240620'\n",
    "project_root = '/Users/log/Github/textual_grounding/'\n",
    "prompt_type = 'zero_shot_vanilla_cot'\n",
    "few_shot_txt = None\n",
    "sample_size = 2\n",
    "json_datasets = ['logical_deduction_seven_objects','reasoning_about_colored_objects']\n",
    "jsonl_datasets = ['GSM_Plus', 'MultiArith', 'SVAMP', 'p_GSM8K', 'StrategyQA', 'commonsenseQA','SPARTQA']\n",
    "all_datasets = jsonl_datasets + json_datasets\n",
    "all_datasets = ['MultiArith']\n",
    "for dataset in all_datasets:\n",
    "    if dataset in json_datasets:\n",
    "        data_path = os.path.join(project_root, 'data', dataset, 'test.json')\n",
    "    else:\n",
    "        data_path = os.path.join(project_root, 'data', dataset, 'test.jsonl')\n",
    "        \n",
    "    save_dir = os.path.join(project_root, 'logan/results/final/VanillaCoT', dataset, f'{llm_model}')\n",
    "    batch_dir = os.path.join(project_root, 'logan/batch_files/VanillaCoT', dataset, f'{llm_model}')\n",
    "    os.makedirs(save_dir, exist_ok=True)  # Ensure the directory exists\n",
    "    os.makedirs(batch_dir, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "    batch_results_path = os.path.join(save_dir, f'{prompt_type}_{few_shot_txt}_{dataset}_{llm_model}.jsonl')   \n",
    "    batch_output_file = os.path.join(batch_dir, f'{prompt_type}_{few_shot_txt}_{dataset}_{llm_model}.jsonl')   \n",
    "\n",
    "    ids, questions = load_data_size_specific(data_path, sample_size=sample_size)\n",
    "    if few_shot_txt:\n",
    "        with open(fewshot_prompt_path, 'r') as file:\n",
    "            few_shot_prompt = file.read()\n",
    "    else:\n",
    "        few_shot_prompt = \"\"\n",
    "    prompts = []\n",
    "    for question in questions:\n",
    "        prompt = get_prompt(prompt_type, few_shot_prompt, question)\n",
    "        # print(prompt)\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    # tasks = batch_agents.prepare_batch_input(\n",
    "    #     llm_model=llm_model,\n",
    "    #     ids=ids,\n",
    "    #     prompts=prompts,\n",
    "    #     batch_output_file=batch_output_file\n",
    "    # )\n",
    "\n",
    "    #Execute the batch processing with GPT-4 and save results\n",
    "    batch_agents.batch_api_agent(\n",
    "        llm_model=llm_model,\n",
    "        ids=ids,\n",
    "        prompts=prompts,\n",
    "        batch_output_file=batch_output_file,\n",
    "        batch_results_file=batch_results_path\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
