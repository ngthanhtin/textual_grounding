{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import ollama\n",
    "import google.generativeai as genai\n",
    "import anthropic\n",
    "import ollama\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from google.generativeai.types import RequestOptions\n",
    "from google.api_core import retry\n",
    "from typing import List, Tuple\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "import visualize\n",
    "import pandas as pd\n",
    "from utils.utils import add_color_to_tags, extract_parts_0, extract_parts_1\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(save_path: str, ids: List[str], questions: List[str], answers: List[str], append: bool = False):\n",
    "    \"\"\"\n",
    "    Saves the results to a CSV file. If append is True and the file exists, it appends without headers.\n",
    "    Otherwise, it writes a new file with headers.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({'id': ids, 'question': questions, 'answer': answers})\n",
    "    if append and os.path.exists(save_path):\n",
    "        df.to_csv(save_path, mode='a', index=False, header=False)\n",
    "    else:\n",
    "        df.to_csv(save_path, index=False)\n",
    "\n",
    "def read_jsonl_file(filepath: str) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Reads a JSONL file and returns a list of JSON objects.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            json_obj = json.loads(line)\n",
    "            data.append(json_obj)\n",
    "    return data\n",
    "\n",
    "def get_prompt(prompt_type: str, few_shot_prompt: str, question: str) -> str:\n",
    "    \"\"\"\n",
    "    Constructs the prompt based on the prompt type.\n",
    "    \"\"\"\n",
    "    prompts = {\n",
    "        \"cot\": f\"{few_shot_prompt}\\n{question}\\nPlease generate your explanation first, then generate the answer in the bracket as follow:\\n\" +\"Answer: {}\",\n",
    "        \"fs\": f\"{few_shot_prompt}\\n{question}\",\n",
    "        \"fs_inst\": f\"{few_shot_prompt}\\n{question}\\nI want you to answer this question but your explanation should contain references referring back to the information in the question. To do that, first, re-generate the question with proper tags and then generate your answers. The output format is as follow:\\n\\\n",
    "            Reformatted Question: \\\n",
    "                Answer:\",\n",
    "        \"zs\": f\"{question}\\nI want you to answer this question but your explanation should contain references referring back to the information in the question. To do that, first, re-generate the question with proper tags (<a>, <b>, <c>, etc) for refered information and then generate your answers that also have the tag (<a>, <b>, <c>, etc) for the grounded information. Give your answer by analyzing step by step, and give only numbers in the final answer. The output format is as follow:\\n\\\n",
    "            Reformatted Question: \\\n",
    "                Answer:\\\n",
    "                    Final answer:\",\n",
    "        \"fs_xml\": f\"{few_shot_prompt}\\n\\nRecreate the following question in the style of the correctly formatted examples shown previously. Make sure that your response has all its information inclosed in the proper <tags>. Begin your response with the <key_facts> section. Make sure that every fact in <key_facts> is very concise and contains a very short reference to the <question>. Do not include a <question> section in your response\\n\\n<question>\\n{question}\\n</question>\",\n",
    "        \"mermaid_get_answer\": f\"{few_shot_prompt}\\n\\n Your job is to extract the key facts from a question relevant to answering the question. The facts should be represented in a hierarchal format through a mermaid diagram. Do not create duplicate facts across multiple branches that represent the same information. Create a mermaid diagram that represents the key facts in the following question. Then, use the nodes from this graph to cite specific facts in your answer reasoning. Put your final answer in curly brackets e.g. Final_Answer: {{30}} \\n\\nquestion: {question}\", \n",
    "    }\n",
    "    return prompts.get(prompt_type, \"\")\n",
    "\n",
    "# cycle through all the keys \n",
    "def get_gemini_key(problem_id):\n",
    "    GOOGLE_KEYS = [\n",
    "        # 'AIzaSyBQ7zvIZoET3199GNhuz86vKagn_JCEOmk', # original - gen lang client\n",
    "        'AIzaSyCEI-5U4z7-3q-uwlvkOrdT2e78aNmjnbg', # chat app\n",
    "        # 'AIzaSyCvycd0yZZ4GSj47qDLk4JoPemvzUSfvio', # project 1\n",
    "        'AIzaSyD5xNbDkaJMEMBpWEXYNq5SheF6omdKpzg', # project 2\n",
    "        'AIzaSyAjcrp_otRjGsj0YvB1cUc2BMng6KSEZwU', # project 3\n",
    "    ]\n",
    "    index = problem_id%len(GOOGLE_KEYS)\n",
    "    print(f\"getting key from {index}\")\n",
    "    key = GOOGLE_KEYS[index]\n",
    "    return key\n",
    "\n",
    "def query_gemini(prompt: str, problem_id) -> str:\n",
    "    \"\"\"\n",
    "    Queries the Gemini LLM with the given prompt and returns the response text.\n",
    "    \"\"\"\n",
    "    genai.configure(api_key=get_gemini_key(problem_id))\n",
    "    model = genai.GenerativeModel('gemini-1.5-pro-latest')\n",
    "    response = model.generate_content(prompt, request_options=RequestOptions(retry=retry.Retry(initial=20, multiplier=3, maximum=121, timeout=60)))\n",
    "    text = response.candidates[0].content.parts[0].text\n",
    "    return text\n",
    "\n",
    "def query_claude(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Queries the Claude LLM with the given prompt and returns the response text.\n",
    "    \"\"\"\n",
    "    client = anthropic.Anthropic(api_key=API_KEYS['claude'])\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20240620\",\n",
    "        max_tokens=1024,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.content[0].text\n",
    "\n",
    "def query_llm(llm_model: str, ids: List[str], questions: List[str], few_shot_prompt: str, prompt_type: str, save_path: str, already_answered_ids: set) -> Tuple[List[str], List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Queries the specified LLM for each question, skipping already answered ones.\n",
    "    Saves each response immediately after it's obtained.\n",
    "    Returns lists of answered IDs, questions, and answers.\n",
    "    \"\"\"\n",
    "    answers = []\n",
    "    ids_can_be_answered = []\n",
    "    questions_can_be_answered = []\n",
    "    \n",
    "    for id, q in tqdm(zip(ids, questions), total=len(ids)):\n",
    "        # print(f\"Processing ID: {id}\")\n",
    "        if id in already_answered_ids:\n",
    "            print(f\"Skipping already answered ID: {id}\")\n",
    "            continue\n",
    "        \n",
    "        prompt = get_prompt(prompt_type, few_shot_prompt, q)\n",
    "        try:\n",
    "            if llm_model == 'gemini':\n",
    "                answer = query_gemini(prompt, id)\n",
    "            elif llm_model == 'claude':\n",
    "                answer = query_claude(prompt)\n",
    "            elif llm_model == 'llama3.1':\n",
    "                answer = ollama.generate(model='llama3.1', prompt=prompt)['response']\n",
    "                print(f\"Processed ID: {id}\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported LLM model: {llm_model}\")\n",
    "            # print(f\"Answer for ID {id}: {answer}\")\n",
    "            \n",
    "            # Append to lists\n",
    "            answers.append(answer)\n",
    "            questions_can_be_answered.append(q)\n",
    "            ids_can_be_answered.append(id)\n",
    "\n",
    "            # Save after each answer\n",
    "            save_results(save_path, [id], [q], [answer], append=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question {id}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return ids_can_be_answered, questions_can_be_answered, answers\n",
    "\n",
    "def load_data(data_path: str, sample_size: int = None) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Loads data from a JSONL file, optionally sampling a subset.\n",
    "    \"\"\"\n",
    "    data = read_jsonl_file(data_path)\n",
    "    print(f\"Loaded {len(data)} records from: {data_path}\")\n",
    "    if sample_size:\n",
    "        data = random.sample(data, sample_size)\n",
    "        print(f\"Sampled {sample_size} records.\")\n",
    "    questions = [x[\"question\"] for x in data]\n",
    "    ids = [x[\"id\"] for x in data]\n",
    "    return ids, questions\n",
    "\n",
    "def load_data_deterministic(data_path: str, sample_size: int = None) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Loads data from a JSONL file in a deterministic manner by sorting.\n",
    "    \"\"\"\n",
    "    data = read_jsonl_file(data_path)\n",
    "    print(f\"Loaded {len(data)} records from: {data_path}\")\n",
    "    if sample_size:\n",
    "        # Sort the data based on a consistent criterion (e.g., 'id' or 'question')\n",
    "        sorted_data = sorted(data, key=lambda x: x['id'])\n",
    "        # Take the first 'sample_size' items\n",
    "        data = sorted_data[:sample_size]\n",
    "        print(f\"Selected first {sample_size} records after sorting.\")\n",
    "    questions = [x[\"question\"] for x in data]\n",
    "    ids = [x[\"id\"] for x in data]\n",
    "    return ids, questions\n",
    "\n",
    "def load_few_shot_prompt(prompt_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Loads the few-shot prompt from a text file.\n",
    "    \"\"\"\n",
    "    with open(prompt_path, 'r') as file:\n",
    "        prompt = file.read()\n",
    "    # print(f\"Loaded few-shot prompt from: {prompt_path}\")\n",
    "    return prompt\n",
    "\n",
    "def load_already_answered_ids(save_path: str) -> set:\n",
    "    \"\"\"\n",
    "    Loads the set of IDs that have already been answered from the CSV file.\n",
    "    Returns an empty set if the file does not exist.\n",
    "    \"\"\"\n",
    "    if os.path.exists(save_path):\n",
    "        df = pd.read_csv(save_path)\n",
    "        answered_ids = set(df['id'].astype(int).tolist())\n",
    "        # print(f\"Loaded {len(answered_ids)} already answered IDs from: {save_path}\")\n",
    "        print(f\"Already answered IDs: {answered_ids}\")\n",
    "        return answered_ids\n",
    "    else:\n",
    "        print(f\"No existing save file found at: {save_path}. Starting fresh.\")\n",
    "        return set()\n",
    "\n",
    "def initialize_save_file(save_path: str):\n",
    "    \"\"\"\n",
    "    Initializes the CSV file with headers if it doesn't exist.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(save_path):\n",
    "        # Create an empty DataFrame with headers and save\n",
    "        df = pd.DataFrame(columns=['id', 'question', 'answer'])\n",
    "        df.to_csv(save_path, index=False)\n",
    "        print(f\"Initialized new save file with headers at: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1319 records from: /Users/log/Github/textual_grounding/data/GSM8K/test.jsonl\n",
      "Selected first 200 records after sorting.\n",
      "Already answered IDs: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping already answered ID: 0\n",
      "Skipping already answered ID: 1\n",
      "Skipping already answered ID: 2\n",
      "Skipping already answered ID: 3\n",
      "Skipping already answered ID: 4\n",
      "Skipping already answered ID: 5\n",
      "Skipping already answered ID: 6\n",
      "Skipping already answered ID: 7\n",
      "Skipping already answered ID: 8\n",
      "Skipping already answered ID: 9\n",
      "Skipping already answered ID: 10\n",
      "Skipping already answered ID: 11\n",
      "Skipping already answered ID: 12\n",
      "Skipping already answered ID: 13\n",
      "Skipping already answered ID: 14\n",
      "Skipping already answered ID: 15\n",
      "Skipping already answered ID: 16\n",
      "Skipping already answered ID: 17\n",
      "Skipping already answered ID: 18\n",
      "Skipping already answered ID: 19\n",
      "Skipping already answered ID: 20\n",
      "Skipping already answered ID: 21\n",
      "Skipping already answered ID: 22\n",
      "Skipping already answered ID: 23\n",
      "Skipping already answered ID: 24\n",
      "Skipping already answered ID: 25\n",
      "Skipping already answered ID: 26\n",
      "Skipping already answered ID: 27\n",
      "Skipping already answered ID: 28\n",
      "Skipping already answered ID: 29\n",
      "Skipping already answered ID: 30\n",
      "Skipping already answered ID: 31\n",
      "Skipping already answered ID: 32\n",
      "Skipping already answered ID: 33\n",
      "Skipping already answered ID: 34\n",
      "Skipping already answered ID: 35\n",
      "Skipping already answered ID: 36\n",
      "Skipping already answered ID: 37\n",
      "Skipping already answered ID: 38\n",
      "Skipping already answered ID: 39\n",
      "Skipping already answered ID: 40\n",
      "Skipping already answered ID: 41\n",
      "Skipping already answered ID: 42\n",
      "Skipping already answered ID: 43\n",
      "Skipping already answered ID: 44\n",
      "Skipping already answered ID: 45\n",
      "Skipping already answered ID: 46\n",
      "Skipping already answered ID: 47\n",
      "Skipping already answered ID: 48\n",
      "Skipping already answered ID: 49\n",
      "Skipping already answered ID: 50\n",
      "Skipping already answered ID: 51\n",
      "Skipping already answered ID: 52\n",
      "Skipping already answered ID: 53\n",
      "Skipping already answered ID: 54\n",
      "Skipping already answered ID: 55\n",
      "Skipping already answered ID: 56\n",
      "Skipping already answered ID: 57\n",
      "Skipping already answered ID: 58\n",
      "Skipping already answered ID: 59\n",
      "Skipping already answered ID: 60\n",
      "Skipping already answered ID: 61\n",
      "Skipping already answered ID: 62\n",
      "Skipping already answered ID: 63\n",
      "Skipping already answered ID: 64\n",
      "Skipping already answered ID: 65\n",
      "Skipping already answered ID: 66\n",
      "Skipping already answered ID: 67\n",
      "Skipping already answered ID: 68\n",
      "Skipping already answered ID: 69\n",
      "Skipping already answered ID: 70\n",
      "Skipping already answered ID: 71\n",
      "Skipping already answered ID: 72\n",
      "Skipping already answered ID: 73\n",
      "Skipping already answered ID: 74\n",
      "Skipping already answered ID: 75\n",
      "Skipping already answered ID: 76\n",
      "Skipping already answered ID: 77\n",
      "Skipping already answered ID: 78\n",
      "Skipping already answered ID: 79\n",
      "Skipping already answered ID: 80\n",
      "Skipping already answered ID: 81\n",
      "Skipping already answered ID: 82\n",
      "Skipping already answered ID: 83\n",
      "Skipping already answered ID: 84\n",
      "Skipping already answered ID: 85\n",
      "Skipping already answered ID: 86\n",
      "Skipping already answered ID: 87\n",
      "Skipping already answered ID: 88\n",
      "Skipping already answered ID: 89\n",
      "Skipping already answered ID: 90\n",
      "Skipping already answered ID: 91\n",
      "Skipping already answered ID: 92\n",
      "Skipping already answered ID: 93\n",
      "Skipping already answered ID: 94\n",
      "Skipping already answered ID: 95\n",
      "Skipping already answered ID: 96\n",
      "Skipping already answered ID: 97\n",
      "Skipping already answered ID: 98\n",
      "Skipping already answered ID: 99\n",
      "Skipping already answered ID: 100\n",
      "Skipping already answered ID: 101\n",
      "Skipping already answered ID: 102\n",
      "Skipping already answered ID: 103\n",
      "Skipping already answered ID: 104\n",
      "Skipping already answered ID: 105\n",
      "Skipping already answered ID: 106\n",
      "Skipping already answered ID: 107\n",
      "Skipping already answered ID: 108\n",
      "Skipping already answered ID: 109\n",
      "Skipping already answered ID: 110\n",
      "Skipping already answered ID: 111\n",
      "Skipping already answered ID: 112\n",
      "Skipping already answered ID: 113\n",
      "Skipping already answered ID: 114\n",
      "Skipping already answered ID: 115\n",
      "Skipping already answered ID: 116\n",
      "Skipping already answered ID: 117\n",
      "Skipping already answered ID: 118\n",
      "Skipping already answered ID: 119\n",
      "Skipping already answered ID: 120\n",
      "Skipping already answered ID: 121\n",
      "Skipping already answered ID: 122\n",
      "Skipping already answered ID: 123\n",
      "Skipping already answered ID: 124\n",
      "Skipping already answered ID: 125\n",
      "Skipping already answered ID: 126\n",
      "Skipping already answered ID: 127\n",
      "Skipping already answered ID: 128\n",
      "Skipping already answered ID: 129\n",
      "Skipping already answered ID: 130\n",
      "Skipping already answered ID: 131\n",
      "Skipping already answered ID: 132\n",
      "Skipping already answered ID: 133\n",
      "Skipping already answered ID: 134\n",
      "Skipping already answered ID: 135\n",
      "Skipping already answered ID: 136\n",
      "Skipping already answered ID: 137\n",
      "Skipping already answered ID: 138\n",
      "Skipping already answered ID: 139\n",
      "Skipping already answered ID: 140\n",
      "Skipping already answered ID: 141\n",
      "Skipping already answered ID: 142\n",
      "Skipping already answered ID: 143\n",
      "Skipping already answered ID: 144\n",
      "Skipping already answered ID: 145\n",
      "Skipping already answered ID: 146\n",
      "Skipping already answered ID: 147\n",
      "Skipping already answered ID: 148\n",
      "Skipping already answered ID: 149\n",
      "getting key from 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1727594966.201728 29728448 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n",
      "100%|██████████| 200/200 [00:07<00:00, 27.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping already answered ID: 151\n",
      "Skipping already answered ID: 152\n",
      "Skipping already answered ID: 153\n",
      "Skipping already answered ID: 154\n",
      "Skipping already answered ID: 155\n",
      "Skipping already answered ID: 156\n",
      "Skipping already answered ID: 157\n",
      "Skipping already answered ID: 158\n",
      "Skipping already answered ID: 159\n",
      "Skipping already answered ID: 160\n",
      "Skipping already answered ID: 161\n",
      "Skipping already answered ID: 162\n",
      "Skipping already answered ID: 163\n",
      "Skipping already answered ID: 164\n",
      "Skipping already answered ID: 165\n",
      "Skipping already answered ID: 166\n",
      "Skipping already answered ID: 167\n",
      "Skipping already answered ID: 168\n",
      "Skipping already answered ID: 169\n",
      "Skipping already answered ID: 170\n",
      "Skipping already answered ID: 171\n",
      "Skipping already answered ID: 172\n",
      "Skipping already answered ID: 173\n",
      "Skipping already answered ID: 174\n",
      "Skipping already answered ID: 175\n",
      "Skipping already answered ID: 176\n",
      "Skipping already answered ID: 177\n",
      "Skipping already answered ID: 178\n",
      "Skipping already answered ID: 179\n",
      "Skipping already answered ID: 180\n",
      "Skipping already answered ID: 181\n",
      "Skipping already answered ID: 182\n",
      "Skipping already answered ID: 183\n",
      "Skipping already answered ID: 184\n",
      "Skipping already answered ID: 185\n",
      "Skipping already answered ID: 186\n",
      "Skipping already answered ID: 187\n",
      "Skipping already answered ID: 188\n",
      "Skipping already answered ID: 189\n",
      "Skipping already answered ID: 190\n",
      "Skipping already answered ID: 191\n",
      "Skipping already answered ID: 192\n",
      "Skipping already answered ID: 193\n",
      "Skipping already answered ID: 194\n",
      "Skipping already answered ID: 195\n",
      "Skipping already answered ID: 196\n",
      "Skipping already answered ID: 197\n",
      "Skipping already answered ID: 198\n",
      "Skipping already answered ID: 199\n",
      "Processing complete. 1 new answers saved to /Users/log/Github/textual_grounding/logan/results/GSM8K/gemini/mermaid/mermaid_get_answer_gemini_0929_010513.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# time = datetime.datetime.now().strftime(\"%m%d_%H%M%S\")\n",
    "time = '0929_010513'\n",
    "project_root = '/Users/log/Github/textual_grounding/'\n",
    "dataset = 'GSM8K'\n",
    "\n",
    "# llm_model = 'llama3.1'\n",
    "llm_model = 'gemini'\n",
    "prompt_type = 'mermaid_get_answer'\n",
    "# prompt_type = 'cot'\n",
    "few_shot_txt = 'fewshot_mermaid_full.txt'\n",
    "# few_shot_txt = '3shot_cot.txt'\n",
    "\n",
    "# Paths\n",
    "data_path = os.path.join(project_root, 'data', dataset, 'test.jsonl')\n",
    "# data_path = '/Users/log/Github/textual_grounding/logan/results/GSM8K/llama/mermaid/mermaid_get_graph_llama3.1_20240924_001821.csv'\n",
    "\n",
    "fewshot_prompt_path = os.path.join(project_root, \"prompt\", dataset, few_shot_txt)\n",
    "save_dir = os.path.join(project_root, 'logan/results', dataset, f'{llm_model}/mermaid')\n",
    "os.makedirs(save_dir, exist_ok=True)  # Ensure the directory exists\n",
    "save_path = os.path.join(save_dir, f'{prompt_type}_{llm_model}_{time}.csv')\n",
    "\n",
    "ids, questions = load_data_deterministic(data_path, sample_size=200)  # Set sample_size as needed\n",
    "few_shot_prompt = load_few_shot_prompt(fewshot_prompt_path)\n",
    "\n",
    "initialize_save_file(save_path)\n",
    "already_answered_ids = load_already_answered_ids(save_path)\n",
    "\n",
    "\n",
    "ids_answered, questions_answered, answers = query_llm(\n",
    "    llm_model=llm_model,\n",
    "    ids=ids,\n",
    "    questions=questions,\n",
    "    few_shot_prompt=few_shot_prompt,\n",
    "    prompt_type=prompt_type,\n",
    "    save_path=save_path,\n",
    "    already_answered_ids=already_answered_ids\n",
    ")\n",
    "\n",
    "print(f\"Processing complete. {len(ids_answered)} new answers saved to {save_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XML - visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML content has been successfully written to test_grounding_answer_prompt_fs_xml_llama3.1.html\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "def extract_parts_1(answer_text):\n",
    "    \"\"\"\n",
    "    Processes the answer text to extract key facts (with numbers), answer reasoning, and the final answer.\n",
    "\n",
    "    Args:\n",
    "        answer_text (str): The full answer text containing <key_facts>, <answer_reasoning>, and <final_answer>.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (key_facts_list, answer_reasoning, final_answer)\n",
    "               where key_facts_list is a list of tuples (fact_number, fact_content)\n",
    "    \"\"\"\n",
    "    # Extract key_facts\n",
    "    key_facts_match = re.search(r'<key_facts>(.*?)</key_facts>', answer_text, re.DOTALL)\n",
    "    key_facts_content = key_facts_match.group(1).strip() if key_facts_match else \"\"\n",
    "\n",
    "    # Extract individual facts with their numbers\n",
    "    facts = re.findall(r'<fact_(\\d+)>(.*?)</fact_\\d+>', key_facts_content, re.DOTALL)\n",
    "    key_facts_list = [(number.strip(), content.strip()) for number, content in facts]\n",
    "\n",
    "    # Extract answer_reasoning\n",
    "    reasoning_match = re.search(r'<answer_reasoning>(.*?)</answer_reasoning>', answer_text, re.DOTALL)\n",
    "    answer_reasoning = reasoning_match.group(1).strip() if reasoning_match else \"\"\n",
    "\n",
    "    # Extract final_answer\n",
    "    final_match = re.search(r'<final_answer>(.*?)</final_answer>', answer_text, re.DOTALL)\n",
    "    final_answer = final_match.group(1).strip() if final_match else \"\"\n",
    "\n",
    "    return key_facts_list, answer_reasoning, final_answer\n",
    "\n",
    "\n",
    "def add_color_to_tags(text):\n",
    "    \"\"\"\n",
    "    Adds background color to specific tags within the text based on a predefined color mapping.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text containing tags like <fact_1>, <fact_2>, etc.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with added inline CSS for background colors.\n",
    "    \"\"\"\n",
    "    tag_color_mapping = {\n",
    "        'fact_1': 'yellow',  \n",
    "        'fact_2': 'lightblue',\n",
    "        'fact_3': 'lightgreen',\n",
    "        'fact_4': 'lightcoral',\n",
    "        'fact_5': 'lightcyan', \n",
    "        'fact_6': 'orange',\n",
    "    }\n",
    "    # Iterate over the tag-color mappings\n",
    "    for tag, color in tag_color_mapping.items():\n",
    "        # Regex to find the tag and replace it with the same tag having a style attribute\n",
    "        text = re.sub(\n",
    "            f'<{tag}>(.*?)</{tag}>',\n",
    "            f'<{tag} style=\"background-color: {color};\">\\\\1</{tag}>',\n",
    "            text,\n",
    "            flags=re.DOTALL\n",
    "        )\n",
    "    return text\n",
    "\n",
    "\n",
    "def parse_csv_file(file_path):\n",
    "    \"\"\"\n",
    "    Parses the input CSV file and extracts questions and their corresponding answers.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the input CSV file.\n",
    "\n",
    "    Returns:\n",
    "        list of tuples: Each tuple contains (question, answer_text).\n",
    "    \"\"\"\n",
    "    qa_pairs = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            question = row.get('question', 'No question found.').strip()\n",
    "            answer_text = row.get('answer', 'No answer found.').strip()\n",
    "            qa_pairs.append((question, answer_text))\n",
    "    return qa_pairs\n",
    "\n",
    "\n",
    "def create_highlight_html(qa_pairs):\n",
    "    \"\"\"\n",
    "    Creates HTML content with highlighted questions, key facts, answer reasoning, and answers.\n",
    "\n",
    "    Args:\n",
    "        qa_pairs (list of tuples): Each tuple contains (question, answer_text).\n",
    "\n",
    "    Returns:\n",
    "        str: The complete HTML content as a string.\n",
    "    \"\"\"\n",
    "    html_content = \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <title>Question and Answer Highlights</title>\n",
    "        <style>\n",
    "            body {\n",
    "                font-family: Arial, sans-serif;\n",
    "                margin: 20px;\n",
    "                background-color: #f0f0f0;\n",
    "            }\n",
    "            .container {\n",
    "                background-color: #ffffff;\n",
    "                padding: 20px;\n",
    "                margin-bottom: 20px;\n",
    "                border-radius: 8px;\n",
    "                box-shadow: 0 2px 5px rgba(0,0,0,0.1);\n",
    "            }\n",
    "            .question {\n",
    "                font-size: 1.2em;\n",
    "                margin-bottom: 10px;\n",
    "            }\n",
    "            .key-facts {\n",
    "                margin-bottom: 10px;\n",
    "            }\n",
    "            .key-facts ul {\n",
    "                list-style-type: number;\n",
    "                padding-left: 20px;\n",
    "            }\n",
    "            .key-facts ul li{\n",
    "                margin-bottom: 4px;\n",
    "            }\n",
    "            .answer-reasoning, .final-answer {\n",
    "                margin-bottom: 10px;\n",
    "            }\n",
    "            .highlight {\n",
    "                background-color: #FFFF00; /* Yellow background for visibility */\n",
    "                font-weight: bold; /* Bold text for emphasis */\n",
    "            }\n",
    "            /* Styles for specific facts */\n",
    "            fact_1 {\n",
    "                background-color: yellow;\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            fact_2 {\n",
    "                background-color: lightblue;\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            fact_3 {\n",
    "                background-color: lightgreen;\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            fact_4 {\n",
    "                background-color: lightcoral;\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            fact_5 {\n",
    "                background-color: lightcyan;\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            fact_6 {\n",
    "                background-color: orange;\n",
    "                font-weight: bold;\n",
    "            }\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "    <h1>Question and Answer Highlights</h1>\n",
    "    \"\"\"\n",
    "    for i, (question, answer_text) in enumerate(qa_pairs, 1):\n",
    "        try:\n",
    "            key_facts, answer_reasoning, final_answer = extract_parts_1(answer_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot extract parts for question {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Convert key_facts list to HTML bullet points with \"Key fact X:\" prefix\n",
    "        if key_facts:\n",
    "            key_facts_html = \"<ul>\\n\"\n",
    "            for fact_number, fact_content in key_facts:\n",
    "                # Apply color to tags in fact_content\n",
    "                highlighted_fact = add_color_to_tags(fact_content)\n",
    "                # Prepend \"Key fact X:\"\n",
    "                key_facts_html += f\"    <li><fact_{fact_number}>{highlighted_fact}</fact_{fact_number}></li>\\n\"\n",
    "            key_facts_html += \"</ul>\"\n",
    "        else:\n",
    "            key_facts_html = \"<p>No key facts available.</p>\"\n",
    "\n",
    "        # Apply color to tags in answer_reasoning and final_answer\n",
    "        highlighted_reasoning = add_color_to_tags(answer_reasoning)\n",
    "        highlighted_final_answer = add_color_to_tags(final_answer)\n",
    "\n",
    "        # Build the HTML structure\n",
    "        html_content += f\"<div class='container'>\"\n",
    "        html_content += f\"<div class='question'><strong>Question:</strong> {question}</div>\"\n",
    "        html_content += f\"<div class='key-facts'><strong>Key Facts:</strong> {key_facts_html}</div>\"\n",
    "        html_content += f\"<div class='answer-reasoning'><strong>Answer Reasoning:</strong> {highlighted_reasoning}</div>\"\n",
    "        html_content += f\"<div class='final-answer'><strong>Answer:</strong> {highlighted_final_answer}</div>\"\n",
    "        html_content += \"</div>\\n\"\n",
    "\n",
    "    # Close the HTML tags\n",
    "    html_content += \"\"\"\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    return html_content\n",
    "\n",
    "\n",
    "def main():\n",
    "    input_file = '/Users/log/Github/textual_grounding/logan/results/GSM8K/llama/test_grounding_answer_prompt_fs_xml_llama3.1.csv'  # Replace with your input CSV file path\n",
    "    output_file = 'test_grounding_answer_prompt_fs_xml_llama3.1.html'  # Replace with your desired output HTML file path\n",
    "\n",
    "    # Parse the input CSV file to extract questions and answers\n",
    "    qa_pairs = parse_csv_file(input_file)\n",
    "\n",
    "    # Check if any QA pairs were found\n",
    "    if not qa_pairs:\n",
    "        print(\"No question-answer pairs were found in the input file.\")\n",
    "        return\n",
    "\n",
    "    # Generate the HTML content\n",
    "    html_content = create_highlight_html(qa_pairs)\n",
    "\n",
    "    # Write the HTML content to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(html_content)\n",
    "\n",
    "    print(f\"HTML content has been successfully written to {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mermaid - Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total QA Pairs Parsed: 200\n",
      "No ground truth answer found for ID 489\n",
      "No ground truth answer found for ID 1113\n",
      "Total Ground Truth Entries: 1317\n",
      "HTML content has been successfully written to mm_gemini.html\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import json  # For handling JSONL\n",
    "import os\n",
    "\n",
    "def extract_parts_new_format(answer_text):\n",
    "    \"\"\"\n",
    "    Processes the answer text to extract Mermaid diagrams, answer reasoning, and the final answer.\n",
    "\n",
    "    Args:\n",
    "        answer_text (str): The full answer text containing Mermaid diagrams, answer reasoning, and final answer.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (mermaid_diagrams, answer_reasoning, final_answer)\n",
    "               where mermaid_diagrams is a list of Mermaid diagram strings\n",
    "    \"\"\"\n",
    "    # Extract Mermaid diagrams\n",
    "    # Modified Pattern:\n",
    "    # - Capture 'graph TD' along with the diagram content\n",
    "    mermaid_pattern = re.compile(\n",
    "        r'Fact_Diagram:\\s*(graph\\s+TD\\s*[\\s\\S]*?)(?=Answer_Reasoning:|$)',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    mermaid_diagrams = mermaid_pattern.findall(answer_text)\n",
    "    mermaid_diagrams = [diagram.strip() for diagram in mermaid_diagrams]\n",
    "\n",
    "    # Remove Mermaid diagrams from the answer_text to process the remaining text\n",
    "    answer_text_clean = mermaid_pattern.sub('', answer_text).strip()\n",
    "\n",
    "    # Extract answer_reasoning\n",
    "    reasoning_pattern = re.compile(\n",
    "        r'Answer_Reasoning:\\s*([\\s\\S]*?)(?=Final_Answer:|$)',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    reasoning_match = reasoning_pattern.search(answer_text_clean)\n",
    "    if reasoning_match:\n",
    "        answer_reasoning = reasoning_match.group(1).strip()\n",
    "    else:\n",
    "        # Fallback if 'Answer_Reasoning:' section is missing\n",
    "        answer_reasoning = \"\"\n",
    "\n",
    "    # Extract final_answer\n",
    "    final_answer_pattern = re.compile(\n",
    "        r'Final_Answer:\\s*\\{([^}]+)\\}',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    final_match = final_answer_pattern.search(answer_text_clean)\n",
    "    final_answer = final_match.group(1).replace(',', '').replace('$', '').strip() if final_match else \"\"\n",
    "\n",
    "    return mermaid_diagrams, answer_reasoning, final_answer\n",
    "\n",
    "\n",
    "def add_color_to_tags_new(text):\n",
    "    \"\"\"\n",
    "    Adds background color to specific tags within the text based on dynamically assigned colors.\n",
    "    Each span will have a class corresponding to the tag's name.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text containing tags like <B>, <C1>, etc.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with added inline CSS for background colors and class names.\n",
    "    \"\"\"\n",
    "    # Find all unique tags in the text using regex\n",
    "    tags = set(re.findall(r'<([A-Za-z]+\\d*)>', text))\n",
    "\n",
    "    # Predefined color palette\n",
    "    color_palette = [\n",
    "        'lightyellow', 'lightblue', 'lightgreen', 'lightcoral',\n",
    "        'lightcyan', 'lightpink', 'lightsalmon', 'lightgray',\n",
    "        'lightgoldenrodyellow', 'lightseagreen', 'lightskyblue',\n",
    "        'lightsteelblue'\n",
    "    ]\n",
    "\n",
    "    # Dictionary to hold tag-color mapping\n",
    "    tag_color_mapping = {}\n",
    "\n",
    "    # Assign colors to tags, cycling through the color palette if necessary\n",
    "    for i, tag in enumerate(sorted(tags)):\n",
    "        color = color_palette[i % len(color_palette)]\n",
    "        tag_color_mapping[tag] = color\n",
    "\n",
    "    # Function to replace tags with styled spans including class names\n",
    "    def replace_tag(match):\n",
    "        tag = match.group(1)\n",
    "        content = match.group(2)\n",
    "        color = tag_color_mapping.get(tag, 'lightgray')  # Default color if not found\n",
    "        return f'<span class=\"{tag}\" style=\"background-color: {color}; font-weight: bold;\">{content}</span>'\n",
    "\n",
    "    # Regex to find tags and replace them with styled spans\n",
    "    tag_regex = re.compile(r'<([A-Za-z]+\\d*)>\\s*([\\s\\S]*?)\\s*</\\1>')\n",
    "\n",
    "    # Replace all tags with styled spans\n",
    "    text = tag_regex.sub(replace_tag, text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def parse_csv_file(file_path):\n",
    "    \"\"\"\n",
    "    Parses the input CSV file and extracts questions, answers, and their corresponding IDs.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the input CSV file.\n",
    "\n",
    "    Returns:\n",
    "        list of tuples: Each tuple contains (id, question, answer_text).\n",
    "    \"\"\"\n",
    "    qa_pairs = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            question = row.get('question', 'No question found.').strip()\n",
    "            answer_text = row.get('answer', 'No answer found.').strip()\n",
    "            id_ = row.get('id')\n",
    "            if id_ is not None:\n",
    "                try:\n",
    "                    id_int = int(id_)\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping a row due to invalid 'id' (not an integer): {id_}\")\n",
    "                    continue\n",
    "                qa_pairs.append((id_int, question, answer_text))\n",
    "            else:\n",
    "                # Handle cases without 'id' by skipping\n",
    "                print(f\"Skipping a row due to missing 'id': {row}\")\n",
    "    return qa_pairs\n",
    "\n",
    "\n",
    "def read_ground_truth(jsonl_path):\n",
    "    \"\"\"\n",
    "    Reads the ground truth answers from a JSONL file and maps them by ID.\n",
    "\n",
    "    Args:\n",
    "        jsonl_path (str): Path to the ground truth JSONL file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping each ID to its ground truth answer.\n",
    "    \"\"\"\n",
    "    ground_truth = {}\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            id_ = data.get('id')\n",
    "            answer = data.get('answer')\n",
    "            if id_ is not None and answer is not None:\n",
    "                # Extract the last number after '####' if present\n",
    "                match = re.search(r'####\\s*([\\d.,]+)', answer)\n",
    "                if match:\n",
    "                    # Remove commas and dollar signs for numerical comparison\n",
    "                    clean_answer = match.group(1).replace(',', '').replace('$', '').strip()\n",
    "                    ground_truth[id_] = clean_answer\n",
    "                else:\n",
    "                    # If '####' not found, try to extract any number within {}\n",
    "                    match_braces = re.search(r'\\{([\\d,.\\-]+)\\}', answer)\n",
    "                    if match_braces:\n",
    "                        clean_answer = match_braces.group(1).replace(',', '').replace('$', '').strip()\n",
    "                        ground_truth[id_] = clean_answer\n",
    "                    else:\n",
    "                        print(f\"No ground truth answer found for ID {id_}\")\n",
    "            else:\n",
    "                print(f\"Invalid ground truth entry: {data}\")\n",
    "    return ground_truth\n",
    "\n",
    "\n",
    "def create_highlight_html_new(qa_pairs, ground_truth):\n",
    "    \"\"\"\n",
    "    Creates HTML content with Mermaid diagrams (both pure text and rendered), highlighted answer reasoning,\n",
    "    final answers colored based on correctness, and ground truth answers.\n",
    "\n",
    "    Args:\n",
    "        qa_pairs (list of tuples): Each tuple contains (id, question, answer_text).\n",
    "        ground_truth (dict): A dictionary mapping each ID to its ground truth answer.\n",
    "\n",
    "    Returns:\n",
    "        str: The complete HTML content as a string.\n",
    "    \"\"\"\n",
    "    html_content = \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <title>Question and Answer Highlights</title>\n",
    "        <style>\n",
    "            body {\n",
    "                font-family: Arial, sans-serif;\n",
    "                margin: 20px;\n",
    "                background-color: #f0f0f0;\n",
    "            }\n",
    "            .container {\n",
    "                background-color: #ffffff;\n",
    "                padding: 20px;\n",
    "                margin-bottom: 20px;\n",
    "                border-radius: 8px;\n",
    "                box-shadow: 0 2px 5px rgba(0,0,0,0.1);\n",
    "            }\n",
    "            .question {\n",
    "                font-size: 1.2em;\n",
    "                margin-bottom: 10px;\n",
    "            }\n",
    "            .mermaid {\n",
    "                margin-bottom: 10px;\n",
    "            }\n",
    "            .mermaidPure {\n",
    "                background-color: #f9f9f9;\n",
    "                padding: 10px;\n",
    "                border: 1px solid #ddd;\n",
    "                border-radius: 4px;\n",
    "                white-space: pre-wrap; /* Preserves whitespace and newlines */\n",
    "                font-family: Consolas, \"Courier New\", monospace;\n",
    "                margin-bottom: 10px;\n",
    "            }\n",
    "            .answer-reasoning, .final-answer, .ground-truth-answer {\n",
    "                margin-bottom: 10px;\n",
    "            }\n",
    "            .final-answer {\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            .ground-truth-answer {\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            /* Styles for the highlighted spans */\n",
    "            .highlighted {\n",
    "                padding: 2px 4px;\n",
    "                border-radius: 3px;\n",
    "                display: inline-block;\n",
    "            }\n",
    "            /* Styles for the summary section */\n",
    "            .summary {\n",
    "                background-color: #e0ffe0;\n",
    "                padding: 15px;\n",
    "                border: 2px solid #00cc00;\n",
    "                border-radius: 8px;\n",
    "                font-size: 1.2em;\n",
    "                margin-top: 30px;\n",
    "            }\n",
    "        </style>\n",
    "        <!-- Include Mermaid.js -->\n",
    "        <script type=\"module\">\n",
    "            import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';\n",
    "            mermaid.initialize({ startOnLoad: true });\n",
    "        </script>\n",
    "    </head>\n",
    "    <body>\n",
    "    <h1>Question and Answer Highlights</h1>\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize counters for correct and total answers\n",
    "    correct_answers = 0\n",
    "    total_answers = 0\n",
    "\n",
    "    for i, (id_, question, answer_text) in enumerate(qa_pairs, 1):\n",
    "        try:\n",
    "            mermaid_diagrams, answer_reasoning, final_answer = extract_parts_new_format(answer_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot extract parts for question ID {id_}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Convert mermaid diagrams to HTML divs with pure text and rendered version\n",
    "        mermaid_html = \"\"\n",
    "        for diagram in mermaid_diagrams:\n",
    "            # Escape HTML special characters in the pure text version\n",
    "            escaped_diagram = diagram.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')\n",
    "            mermaid_html += f\"<div class='mermaidPure'><pre>{escaped_diagram}</pre></div>\\n\"\n",
    "            mermaid_html += f\"<div class='mermaid'>\\n{diagram}\\n</div>\\n\"\n",
    "\n",
    "        # Apply color to tags in answer_reasoning\n",
    "        highlighted_reasoning = add_color_to_tags_new(answer_reasoning)\n",
    "\n",
    "        # Retrieve ground truth answer\n",
    "        gt_answer = ground_truth.get(id_)\n",
    "        if gt_answer is None:\n",
    "            gt_answer_display = \"<span style='color: gray;'>Ground truth not available.</span>\"\n",
    "            is_correct = False\n",
    "        else:\n",
    "            # Normalize both final_answer and gt_answer for comparison\n",
    "            try:\n",
    "                final_answer_num = float(final_answer.replace(',', '').replace('$', ''))\n",
    "                gt_answer_num = float(gt_answer.replace(',', '').replace('$', ''))\n",
    "                is_correct = final_answer_num == gt_answer_num\n",
    "                # Format numbers with commas and two decimal places if needed\n",
    "                if final_answer_num.is_integer():\n",
    "                    final_answer_display = f\"{int(final_answer_num):,}\"\n",
    "                else:\n",
    "                    final_answer_display = f\"{final_answer_num:,.2f}\"\n",
    "                if gt_answer_num.is_integer():\n",
    "                    gt_answer_display = f\"{int(gt_answer_num):,}\"\n",
    "                else:\n",
    "                    gt_answer_display = f\"{gt_answer_num:,.2f}\"\n",
    "            except ValueError:\n",
    "                # In case conversion fails, fallback to string comparison\n",
    "                is_correct = final_answer == gt_answer\n",
    "                final_answer_display = final_answer\n",
    "                gt_answer_display = gt_answer\n",
    "\n",
    "        # Style the final answer based on correctness\n",
    "        if is_correct:\n",
    "            highlighted_final_answer = f\"<span style='font-size:1.1em; color: green;'>{final_answer_display}</span>\"\n",
    "            correct_answers += 1\n",
    "        else:\n",
    "            highlighted_final_answer = f\"<span style='font-size:1.1em; color: red;'>{final_answer_display}</span>\"\n",
    "        total_answers += 1\n",
    "\n",
    "        # Display ground truth answer\n",
    "        if gt_answer is not None:\n",
    "            ground_truth_html = f\"<div class='ground-truth-answer'><strong>Ground Truth Answer:</strong> {gt_answer_display}</div>\"\n",
    "        else:\n",
    "            ground_truth_html = f\"<div class='ground-truth-answer'><strong>Ground Truth Answer:</strong> Not available.</div>\"\n",
    "\n",
    "        # Build the HTML structure\n",
    "        html_content += f\"<div class='container'>\"\n",
    "        html_content += f\"<div class='question'><strong>Question:</strong> {question}</div>\"\n",
    "        if mermaid_html:\n",
    "            html_content += f\"{mermaid_html}\"\n",
    "        else:\n",
    "            html_content += f\"<p>No diagram available.</p>\"\n",
    "        html_content += f\"<div class='answer-reasoning'><strong>Answer Reasoning:</strong> {highlighted_reasoning}</div>\"\n",
    "        html_content += f\"<div class='final-answer'><strong>Final Answer:</strong> {highlighted_final_answer}</div>\"\n",
    "        html_content += f\"{ground_truth_html}\"\n",
    "        html_content += \"</div>\\n\"\n",
    "\n",
    "    # After processing all QA pairs, add the summary section\n",
    "    summary_percentage = (correct_answers / total_answers * 100) if total_answers > 0 else 0\n",
    "    summary_html = f\"\"\"\n",
    "    <div class='summary'>\n",
    "        <strong>Summary:</strong> Correct Answers: {correct_answers} / {total_answers} ({summary_percentage:.2f}%)\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    # It's more logical to place the summary at the end\n",
    "    # html_content += summary_html\n",
    "    output_html = summary_html + html_content\n",
    "    # Close the HTML tags\n",
    "    output_html += \"\"\"\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    return output_html\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Replace these paths with your actual file paths\n",
    "    input_csv = '/Users/log/Github/textual_grounding/logan/results/GSM8K/gemini/mermaid/mermaid_get_answer_gemini_0929_010513.csv'  # Replace with your input CSV file path\n",
    "    ground_truth_file = '/Users/log/Github/textual_grounding/data/GSM8K/test.jsonl'  # Path to the ground truth JSONL file\n",
    "    output_file = 'mm_gemini.html'  # Replace with your desired output HTML file path\n",
    "\n",
    "    # Check if input files exist\n",
    "    if not os.path.isfile(input_csv):\n",
    "        print(f\"Input CSV file not found: {input_csv}\")\n",
    "        return\n",
    "    if not os.path.isfile(ground_truth_file):\n",
    "        print(f\"Ground truth JSONL file not found: {ground_truth_file}\")\n",
    "        return\n",
    "\n",
    "    # Parse the input CSV file to extract IDs, questions, and answers\n",
    "    qa_pairs = parse_csv_file(input_csv)\n",
    "    print(f\"Total QA Pairs Parsed: {len(qa_pairs)}\")  # Debug: Print the number of QA pairs parsed\n",
    "\n",
    "    # Read the ground truth answers\n",
    "    ground_truth = read_ground_truth(ground_truth_file)\n",
    "    print(f\"Total Ground Truth Entries: {len(ground_truth)}\")  # Debug: Print the number of ground truth entries\n",
    "\n",
    "    # Check if any QA pairs were found\n",
    "    if not qa_pairs:\n",
    "        print(\"No question-answer pairs were found in the input file.\")\n",
    "        return\n",
    "\n",
    "    # Generate the HTML content\n",
    "    html_content = create_highlight_html_new(qa_pairs, ground_truth)\n",
    "\n",
    "    # Write the HTML content to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(html_content)\n",
    "\n",
    "    print(f\"HTML content has been successfully written to {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoT - Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total QA Pairs Parsed: 200\n",
      "No ground truth answer found for ID 489\n",
      "No ground truth answer found for ID 1113\n",
      "Total Ground Truth Entries: 1317\n",
      "HTML content has been successfully written to cot_llama3.1.html\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import json  # For handling JSONL\n",
    "import os\n",
    "\n",
    "def extract_parts_regular_cot(answer_text):\n",
    "    \"\"\"\n",
    "    Processes the answer text to extract answer reasoning and the final answer.\n",
    "\n",
    "    Args:\n",
    "        answer_text (str): The full answer text containing answer reasoning and final answer.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (answer_reasoning, final_answer)\n",
    "               where answer_reasoning is the full model response,\n",
    "               and final_answer is the extracted answer.\n",
    "    \"\"\"\n",
    "\n",
    "    # Fallback: Extract Final Answer from '{...}' in the reasoning\n",
    "    curly_match = re.search(r'\\{([\\d.]+)\\}', answer_text)\n",
    "    final_answer = curly_match.group(1).strip() if curly_match else \"\"\n",
    "\n",
    "    return answer_text.strip(), final_answer\n",
    "\n",
    "\n",
    "def add_color_to_tags_new(text):\n",
    "    \"\"\"\n",
    "    Adds background color to specific tags within the text based on dynamically assigned colors.\n",
    "    Each span will have a class corresponding to the tag's name.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text containing tags like <B>, <C1>, etc.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with added inline CSS for background colors and class names.\n",
    "    \"\"\"\n",
    "    # Find all unique tags in the text using regex\n",
    "    # This regex matches tags like <B>, <C1>, <Node123>, etc.\n",
    "    tags = set(re.findall(r'<([A-Za-z]+\\d*)>', text))\n",
    "\n",
    "    # Predefined color palette\n",
    "    color_palette = [\n",
    "        'lightyellow', 'lightblue', 'lightgreen', 'lightcoral',\n",
    "        'lightcyan', 'lightpink', 'lightsalmon', 'lightgray',\n",
    "        'lightgoldenrodyellow', 'lightseagreen', 'lightskyblue',\n",
    "        'lightsteelblue', 'lightsteelblue', 'lightsteelblue'\n",
    "    ]\n",
    "\n",
    "    # Dictionary to hold tag-color mapping\n",
    "    tag_color_mapping = {}\n",
    "\n",
    "    # Assign colors to tags, cycling through the color palette if necessary\n",
    "    for i, tag in enumerate(sorted(tags)):\n",
    "        color = color_palette[i % len(color_palette)]\n",
    "        tag_color_mapping[tag] = color\n",
    "\n",
    "    # Function to replace tags with styled spans including class names\n",
    "    def replace_tag(match):\n",
    "        tag = match.group(1)\n",
    "        content = match.group(2)\n",
    "        color = tag_color_mapping.get(tag, 'lightgray')  # Default color if not found\n",
    "        return f'<span class=\"{tag}\" style=\"background-color: {color}; font-weight: bold;\">{content}</span>'\n",
    "\n",
    "    # Regex to find tags and replace them with styled spans\n",
    "    # This regex matches <Tag>Content</Tag>\n",
    "    text = re.sub(r'<([A-Za-z]+\\d*)>(.*?)</\\1>', replace_tag, text, flags=re.DOTALL)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def parse_csv_file(file_path):\n",
    "    \"\"\"\n",
    "    Parses the input CSV file and extracts questions, answers, and their corresponding IDs.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the input CSV file.\n",
    "\n",
    "    Returns:\n",
    "        list of tuples: Each tuple contains (id, question, answer_text).\n",
    "    \"\"\"\n",
    "    qa_pairs = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            question = row.get('question', 'No question found.').strip()\n",
    "            answer_text = row.get('answer', 'No answer found.').strip()\n",
    "            id_ = row.get('id')\n",
    "            if id_ is not None:\n",
    "                try:\n",
    "                    id_int = int(id_)\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping a row due to invalid 'id' (not an integer): {id_}\")\n",
    "                    continue\n",
    "                qa_pairs.append((id_int, question, answer_text))\n",
    "            else:\n",
    "                # Handle cases without 'id' by skipping\n",
    "                print(f\"Skipping a row due to missing 'id': {row}\")\n",
    "    return qa_pairs\n",
    "\n",
    "\n",
    "def read_ground_truth(jsonl_path):\n",
    "    \"\"\"\n",
    "    Reads the ground truth answers from a JSONL file and maps them by ID.\n",
    "\n",
    "    Args:\n",
    "        jsonl_path (str): Path to the ground truth JSONL file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping each ID to its ground truth answer.\n",
    "    \"\"\"\n",
    "    ground_truth = {}\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            id_ = data.get('id')\n",
    "            answer = data.get('answer')\n",
    "            if id_ is not None and answer is not None:\n",
    "                # Extract the last number or text after '####'\n",
    "                match = re.search(r'####\\s*([\\d.]+)', answer)\n",
    "                if match:\n",
    "                    ground_truth[id_] = match.group(1).strip()\n",
    "                else:\n",
    "                    print(f\"No ground truth answer found for ID {id_}\")\n",
    "            else:\n",
    "                print(f\"Invalid ground truth entry: {data}\")\n",
    "    return ground_truth\n",
    "\n",
    "\n",
    "def create_highlight_html_new(qa_pairs, ground_truth):\n",
    "    \"\"\"\n",
    "    Creates HTML content with highlighted answer reasoning,\n",
    "    final answers colored based on correctness, and ground truth answers.\n",
    "\n",
    "    Args:\n",
    "        qa_pairs (list of tuples): Each tuple contains (id, question, answer_text).\n",
    "        ground_truth (dict): A dictionary mapping each ID to its ground truth answer.\n",
    "\n",
    "    Returns:\n",
    "        str: The complete HTML content as a string.\n",
    "    \"\"\"\n",
    "    html_content = \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <title>Question and Answer Highlights</title>\n",
    "        <style>\n",
    "            body {\n",
    "                font-family: Arial, sans-serif;\n",
    "                margin: 20px;\n",
    "                background-color: #f0f0f0;\n",
    "            }\n",
    "            .container {\n",
    "                background-color: #ffffff;\n",
    "                padding: 20px;\n",
    "                margin-bottom: 20px;\n",
    "                border-radius: 8px;\n",
    "                box-shadow: 0 2px 5px rgba(0,0,0,0.1);\n",
    "            }\n",
    "            .question {\n",
    "                font-size: 1.2em;\n",
    "                margin-bottom: 10px;\n",
    "            }\n",
    "            .answer-reasoning, .final-answer, .ground-truth-answer {\n",
    "                margin-bottom: 10px;\n",
    "            }\n",
    "            .final-answer {\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            .ground-truth-answer {\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            /* Styles for the highlighted spans */\n",
    "            .highlighted {\n",
    "                padding: 2px 4px;\n",
    "                border-radius: 3px;\n",
    "                display: inline-block;\n",
    "            }\n",
    "            /* Styles for the summary section */\n",
    "            .summary {\n",
    "                background-color: #e0ffe0;\n",
    "                padding: 15px;\n",
    "                border: 2px solid #00cc00;\n",
    "                border-radius: 8px;\n",
    "                font-size: 1.2em;\n",
    "                margin-bottom: 30px;\n",
    "            }\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "    <h1>Question and Answer Highlights</h1>\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize counters for correct and total answers\n",
    "    correct_answers = 0\n",
    "    total_answers = 0\n",
    "\n",
    "    # Placeholder for summary to be added at the top\n",
    "    summary_html = \"\"  # Will be updated after processing all QA pairs\n",
    "\n",
    "    # Temporary storage for all QA containers\n",
    "    qa_html_sections = \"\"\n",
    "\n",
    "    for i, (id_, question, answer_text) in enumerate(qa_pairs, 1):\n",
    "        try:\n",
    "            answer_reasoning, final_answer = extract_parts_regular_cot(answer_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot extract parts for question ID {id_}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Apply color to tags in answer_reasoning\n",
    "        highlighted_reasoning = add_color_to_tags_new(answer_reasoning)\n",
    "\n",
    "        # Retrieve ground truth answer\n",
    "        gt_answer = ground_truth.get(id_)\n",
    "        if gt_answer is None:\n",
    "            gt_answer_display = \"<span style='color: gray;'>Ground truth not available.</span>\"\n",
    "            is_correct = False\n",
    "        else:\n",
    "            gt_answer_display = gt_answer\n",
    "            # Compare final_answer with ground truth\n",
    "            is_correct = final_answer == gt_answer\n",
    "\n",
    "        # Style the final answer based on correctness\n",
    "        if is_correct:\n",
    "            highlighted_final_answer = f\"<span style='font-size:1.1em; color: green;'>{final_answer}</span>\"\n",
    "            correct_answers += 1\n",
    "        else:\n",
    "            highlighted_final_answer = f\"<span style='font-size:1.1em; color: red;'>{final_answer}</span>\"\n",
    "        total_answers += 1\n",
    "\n",
    "        # Display ground truth answer\n",
    "        if gt_answer is not None:\n",
    "            ground_truth_html = f\"<div class='ground-truth-answer'><strong>Ground Truth Answer:</strong> {gt_answer_display}</div>\"\n",
    "        else:\n",
    "            ground_truth_html = f\"<div class='ground-truth-answer'><strong>Ground Truth Answer:</strong> Not available.</div>\"\n",
    "\n",
    "        # Build the HTML structure for this QA pair\n",
    "        qa_html_sections += f\"<div class='container'>\"\n",
    "        qa_html_sections += f\"<div class='question'><strong>Question:</strong> {question}</div>\"\n",
    "        qa_html_sections += f\"<div class='answer-reasoning'><strong>Answer Reasoning:</strong> {highlighted_reasoning}</div>\"\n",
    "        qa_html_sections += f\"<div class='final-answer'><strong>Final Answer:</strong> {highlighted_final_answer}</div>\"\n",
    "        qa_html_sections += f\"{ground_truth_html}\"\n",
    "        qa_html_sections += \"</div>\\n\"\n",
    "\n",
    "    # After processing all QA pairs, create the summary\n",
    "    accuracy_percentage = (correct_answers / total_answers * 100) if total_answers > 0 else 0\n",
    "    summary_html = f\"\"\"\n",
    "    <div class='summary'>\n",
    "        <strong>Summary:</strong> Correct Answers: {correct_answers} / {total_answers} ({accuracy_percentage:.2f}%)\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "    # Append the summary at the top, right after the header\n",
    "    html_content += summary_html\n",
    "    html_content += qa_html_sections\n",
    "\n",
    "    # Close the HTML tags\n",
    "    html_content += \"\"\"\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    return html_content\n",
    "\n",
    "\n",
    "def main():\n",
    "    input_csv = '/Users/log/Github/textual_grounding/logan/results/GSM8K/llama/mermaid/cot_llama3.1_20240927_003000.csv'  # Replace with your input CSV file path\n",
    "    ground_truth_file = '/Users/log/Github/textual_grounding/data/GSM8K/test.jsonl'  # Path to the ground truth JSONL file\n",
    "    output_file = 'cot_llama3.1.html'  # Replace with your desired output HTML file path\n",
    "\n",
    "    # Check if input files exist\n",
    "    if not os.path.isfile(input_csv):\n",
    "        print(f\"Input CSV file not found: {input_csv}\")\n",
    "        return\n",
    "    if not os.path.isfile(ground_truth_file):\n",
    "        print(f\"Ground truth JSONL file not found: {ground_truth_file}\")\n",
    "        return\n",
    "\n",
    "    # Parse the input CSV file to extract IDs, questions, and answers\n",
    "    qa_pairs = parse_csv_file(input_csv)\n",
    "    print(f\"Total QA Pairs Parsed: {len(qa_pairs)}\")  # Debug: Print the number of QA pairs parsed\n",
    "\n",
    "    # Read the ground truth answers\n",
    "    ground_truth = read_ground_truth(ground_truth_file)\n",
    "    print(f\"Total Ground Truth Entries: {len(ground_truth)}\")  # Debug: Print the number of ground truth entries\n",
    "\n",
    "    # Check if any QA pairs were found\n",
    "    if not qa_pairs:\n",
    "        print(\"No question-answer pairs were found in the input file.\")\n",
    "        return\n",
    "\n",
    "    # Generate the HTML content\n",
    "    html_content = create_highlight_html_new(qa_pairs, ground_truth)\n",
    "\n",
    "    # Write the HTML content to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(html_content)\n",
    "\n",
    "    print(f\"HTML content has been successfully written to {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tin - visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML content has been successfully written to question_answer_highlights_prompt_fs_llama3.1.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "\n",
    "# Define the prompt_type and llm_model as needed\n",
    "prompt_type = \"fs\"  # Example value, set accordingly\n",
    "llm_model = \"llama3.1\"  # Example value, set accordingly\n",
    "\n",
    "save_html_path = f\"question_answer_highlights_prompt_{prompt_type}_{llm_model}.html\"\n",
    "# df_path = f'logan/results/{dataset}/llama/test_grounding_answer_prompt_{prompt_type}_{llm_model}.csv'\n",
    "df_path = '/Users/log/Github/textual_grounding/logan/results/GSM8K/llama/test_grounding_answer_prompt_fs_inst_llama3.1.csv'\n",
    "\n",
    "if prompt_type in [\"fs\", \"fs_inst\"]:\n",
    "    prefix = 'The answer is'\n",
    "elif prompt_type == \"zs\":\n",
    "    prefix = 'Final answer:'\n",
    "\n",
    "df = pd.read_csv(df_path)\n",
    "questions = df['question'].tolist()\n",
    "answers = df['answer'].tolist()\n",
    "\n",
    "def add_color_to_tags(text):\n",
    "    \"\"\"\n",
    "    Adds background color to specific tags within the text based on a predefined color mapping.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text containing tags like <a>, <b>, etc.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with added inline CSS for background colors.\n",
    "    \"\"\"\n",
    "    tag_color_mapping = {\n",
    "        'a': 'yellow',       # You can customize colors as needed\n",
    "        'b': 'lightblue',\n",
    "        'c': 'lightgreen',\n",
    "        'd': 'lightcoral',\n",
    "        'e': 'lightcyan',\n",
    "        'f': 'orange',       # Extend as needed\n",
    "        # Add more tags if necessary\n",
    "    }\n",
    "    # Iterate over the tag-color mappings\n",
    "    for tag, color in tag_color_mapping.items():\n",
    "        # Regex to find the tag and replace it with the same tag having a style attribute\n",
    "        text = re.sub(\n",
    "            f'<{tag}>(.*?)</{tag}>',\n",
    "            f'<span style=\"background-color: {color};\">{r\"\\1\"}</span>',\n",
    "            text,\n",
    "            flags=re.DOTALL\n",
    "        )\n",
    "    return text\n",
    "\n",
    "def parse_csv_file(file_path):\n",
    "    \"\"\"\n",
    "    Parses the input CSV file and extracts questions and their corresponding answers.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the input CSV file.\n",
    "\n",
    "    Returns:\n",
    "        list of tuples: Each tuple contains (question, answer_text).\n",
    "    \"\"\"\n",
    "    qa_pairs = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            question = row.get('question', 'No question found.').strip()\n",
    "            answer_text = row.get('answer', 'No answer found.').strip()\n",
    "            qa_pairs.append((question, answer_text))\n",
    "    return qa_pairs\n",
    "\n",
    "def create_highlight_html(qa_pairs):\n",
    "    \"\"\"\n",
    "    Creates HTML content with highlighted questions and answers based on tags.\n",
    "\n",
    "    Args:\n",
    "        qa_pairs (list of tuples): Each tuple contains (question, answer_text).\n",
    "\n",
    "    Returns:\n",
    "        str: The complete HTML content as a string.\n",
    "    \"\"\"\n",
    "    html_content = \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <title>Question and Answer Highlights</title>\n",
    "        <style>\n",
    "            body {\n",
    "                font-family: Arial, sans-serif;\n",
    "                margin: 20px;\n",
    "                background-color: #f0f0f0;\n",
    "            }\n",
    "            .container {\n",
    "                background-color: #ffffff;\n",
    "                padding: 20px;\n",
    "                margin-bottom: 20px;\n",
    "                border-radius: 8px;\n",
    "                box-shadow: 0 2px 5px rgba(0,0,0,0.1);\n",
    "            }\n",
    "            .question {\n",
    "                font-size: 1.2em;\n",
    "                margin-bottom: 10px;\n",
    "            }\n",
    "            .answer {\n",
    "                margin-bottom: 10px;\n",
    "            }\n",
    "            .highlight {\n",
    "                background-color: #FFFF00; /* Default highlight color */\n",
    "                font-weight: bold; /* Bold text for emphasis */\n",
    "            }\n",
    "            /* Additional styles for specific tags can be added here if needed */\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "    <h1>Question and Answer Highlights</h1>\n",
    "    \"\"\"\n",
    "    for i, (question, answer_text) in enumerate(qa_pairs, 1):\n",
    "        try:\n",
    "            # Apply color to tags in question and answer\n",
    "            highlighted_question = add_color_to_tags(question)\n",
    "            highlighted_answer = add_color_to_tags(answer_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot process question-answer pair {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Build the HTML structure\n",
    "        html_content += f\"<div class='container'>\"\n",
    "        html_content += f\"<div class='question'><strong>Question:</strong> {highlighted_question}</div>\"\n",
    "        html_content += f\"<div class='answer'><strong>Answer:</strong> {highlighted_answer}</div>\"\n",
    "        html_content += \"</div>\\n\"\n",
    "\n",
    "    # Close the HTML tags\n",
    "    html_content += \"\"\"\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    return html_content\n",
    "\n",
    "def main():\n",
    "    input_file = df_path  # Use the defined df_path\n",
    "    output_file = save_html_path  # Use the defined save_html_path\n",
    "\n",
    "    # Parse the input CSV file to extract questions and answers\n",
    "    qa_pairs = parse_csv_file(input_file)\n",
    "\n",
    "    # Check if any QA pairs were found\n",
    "    if not qa_pairs:\n",
    "        print(\"No question-answer pairs were found in the input file.\")\n",
    "        return\n",
    "\n",
    "    # Generate the HTML content\n",
    "    html_content = create_highlight_html(qa_pairs)\n",
    "\n",
    "    # Write the HTML content to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(html_content)\n",
    "\n",
    "    print(f\"HTML content has been successfully written to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total QA Pairs Parsed: 200\n",
      "No ground truth answer found for ID 489\n",
      "No ground truth answer found for ID 1113\n",
      "Total Ground Truth Entries: 1317\n",
      "\n",
      "===== Analysis Statistics =====\n",
      "\n",
      "Total Responses Analyzed: 200\n",
      "Responses with Final Answer in Curly Brackets: 136 (68.00%)\n",
      "Responses without Final Answer in Curly Brackets: 64 (32.00%)\n",
      "Responses with Ground Truth Available: 200 (100.00%)\n",
      "Correct Answers: 94\n",
      "Incorrect Answers: 106\n",
      "Accuracy: 47.00%\n",
      "Responses without Ground Truth: 0\n",
      "\n",
      "----- Tag Statistics -----\n",
      "Total Tags Found: 501\n",
      "Average Number of Tags per Response: 2.50\n",
      "Average Length of Tag Content: 8.68 characters\n",
      "--------------------------\n",
      "\n",
      "===== End of Statistics =====\n",
      "\n",
      "Statistics analysis completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import json  # For handling JSONL\n",
    "import os\n",
    "\n",
    "def extract_parts_regular_cot(answer_text):\n",
    "    \"\"\"\n",
    "    Processes the answer text to extract answer reasoning and the final answer.\n",
    "\n",
    "    Args:\n",
    "        answer_text (str): The full answer text containing answer reasoning and final answer.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (answer_reasoning, final_answer, has_curly)\n",
    "               where answer_reasoning is the full model response,\n",
    "               final_answer is the extracted answer,\n",
    "               and has_curly is a boolean indicating if the final answer was in curly brackets.\n",
    "    \"\"\"\n",
    "    # Attempt to extract Final Answer from 'Final Answer:'\n",
    "    final_match = re.search(r'Final Answer:\\s*(\\S+)', answer_text, re.IGNORECASE)\n",
    "    if final_match and final_match.group(1).strip():\n",
    "        final_answer = final_match.group(1).strip()\n",
    "        has_curly = False\n",
    "    else:\n",
    "        # Fallback: Extract Final Answer from '{...}' in the reasoning\n",
    "        curly_match = re.search(r'\\{([\\d.]+)\\}', answer_text)\n",
    "        final_answer = curly_match.group(1).strip() if curly_match else \"\"\n",
    "        has_curly = bool(curly_match)\n",
    "\n",
    "    return answer_text.strip(), final_answer, has_curly\n",
    "\n",
    "def parse_csv_file(file_path):\n",
    "    \"\"\"\n",
    "    Parses the input CSV file and extracts questions, answers, and their corresponding IDs.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the input CSV file.\n",
    "\n",
    "    Returns:\n",
    "        list of tuples: Each tuple contains (id, question, answer_text).\n",
    "    \"\"\"\n",
    "    qa_pairs = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            question = row.get('question', 'No question found.').strip()\n",
    "            answer_text = row.get('answer', 'No answer found.').strip()\n",
    "            id_ = row.get('id')\n",
    "            if id_ is not None:\n",
    "                try:\n",
    "                    id_int = int(id_)\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping a row due to invalid 'id' (not an integer): {id_}\")\n",
    "                    continue\n",
    "                qa_pairs.append((id_int, question, answer_text))\n",
    "            else:\n",
    "                # Handle cases without 'id' by skipping\n",
    "                print(f\"Skipping a row due to missing 'id': {row}\")\n",
    "    return qa_pairs\n",
    "\n",
    "def read_ground_truth(jsonl_path):\n",
    "    \"\"\"\n",
    "    Reads the ground truth answers from a JSONL file and maps them by ID.\n",
    "\n",
    "    Args:\n",
    "        jsonl_path (str): Path to the ground truth JSONL file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping each ID to its ground truth answer.\n",
    "    \"\"\"\n",
    "    ground_truth = {}\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            id_ = data.get('id')\n",
    "            answer = data.get('answer')\n",
    "            if id_ is not None and answer is not None:\n",
    "                # Extract the last number or text after '####'\n",
    "                match = re.search(r'####\\s*([\\d.]+)', answer)\n",
    "                if match:\n",
    "                    ground_truth[id_] = match.group(1).strip()\n",
    "                else:\n",
    "                    print(f\"No ground truth answer found for ID {id_}\")\n",
    "            else:\n",
    "                print(f\"Invalid ground truth entry: {data}\")\n",
    "    return ground_truth\n",
    "\n",
    "def create_statistics(qa_pairs, ground_truth):\n",
    "    \"\"\"\n",
    "    Creates and prints statistics based on the QA pairs and ground truth.\n",
    "\n",
    "    Args:\n",
    "        qa_pairs (list of tuples): Each tuple contains (id, question, answer_text).\n",
    "        ground_truth (dict): A dictionary mapping each ID to its ground truth answer.\n",
    "    \"\"\"\n",
    "    total_responses = len(qa_pairs)\n",
    "    responses_with_curly = 0\n",
    "    responses_without_curly = 0\n",
    "    correct_answers = 0\n",
    "    incorrect_answers = 0\n",
    "    no_ground_truth = 0\n",
    "\n",
    "    # Variables for tag statistics\n",
    "    total_tags = 0\n",
    "    total_tag_length = 0\n",
    "    tag_counts = []  # List to store number of tags per response\n",
    "    tag_lengths = []  # List to store lengths of tag content across all responses\n",
    "\n",
    "    for id_, question, answer_text in qa_pairs:\n",
    "        try:\n",
    "            answer_reasoning, final_answer, has_curly = extract_parts_regular_cot(answer_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot extract parts for question ID {id_}: {e}\")\n",
    "            continue\n",
    "\n",
    "        if has_curly:\n",
    "            responses_with_curly += 1\n",
    "        else:\n",
    "            responses_without_curly += 1\n",
    "\n",
    "        # Extract tags and their content\n",
    "        tags_in_response = re.findall(r'<([A-Za-z]+\\d*)>(.*?)</\\1>', answer_text)\n",
    "        number_of_tags = len(tags_in_response)\n",
    "        tag_counts.append(number_of_tags)\n",
    "        total_tags += number_of_tags\n",
    "\n",
    "        for tag, content in tags_in_response:\n",
    "            content_length = len(content)\n",
    "            tag_lengths.append(content_length)\n",
    "            total_tag_length += content_length\n",
    "\n",
    "        # Retrieve ground truth answer\n",
    "        gt_answer = ground_truth.get(id_)\n",
    "        if gt_answer is None:\n",
    "            no_ground_truth += 1\n",
    "            continue\n",
    "\n",
    "        # Compare final_answer with ground truth\n",
    "        if final_answer == gt_answer:\n",
    "            correct_answers += 1\n",
    "        else:\n",
    "            incorrect_answers += 1\n",
    "\n",
    "    # Calculate additional metrics\n",
    "    accuracy_percentage = (correct_answers / (correct_answers + incorrect_answers) * 100) if (correct_answers + incorrect_answers) > 0 else 0\n",
    "    curly_percentage = (responses_with_curly / total_responses * 100) if total_responses > 0 else 0\n",
    "    no_curly_percentage = (responses_without_curly / total_responses * 100) if total_responses > 0 else 0\n",
    "    ground_truth_available = total_responses - no_ground_truth\n",
    "    ground_truth_available_percentage = (ground_truth_available / total_responses * 100) if total_responses > 0 else 0\n",
    "\n",
    "    # Calculate tag statistics\n",
    "    average_tags_per_response = (total_tags / total_responses) if total_responses > 0 else 0\n",
    "    average_tag_length = (total_tag_length / total_tags) if total_tags > 0 else 0\n",
    "\n",
    "    # Print the statistics\n",
    "    print(\"\\n===== Analysis Statistics =====\\n\")\n",
    "    print(f\"Total Responses Analyzed: {total_responses}\")\n",
    "    print(f\"Responses with Final Answer in Curly Brackets: {responses_with_curly} ({curly_percentage:.2f}%)\")\n",
    "    print(f\"Responses without Final Answer in Curly Brackets: {responses_without_curly} ({no_curly_percentage:.2f}%)\")\n",
    "    print(f\"Responses with Ground Truth Available: {ground_truth_available} ({ground_truth_available_percentage:.2f}%)\")\n",
    "    print(f\"Correct Answers: {correct_answers}\")\n",
    "    print(f\"Incorrect Answers: {incorrect_answers}\")\n",
    "    print(f\"Accuracy: {accuracy_percentage:.2f}%\")\n",
    "    print(f\"Responses without Ground Truth: {no_ground_truth}\")\n",
    "\n",
    "    # Tag Statistics\n",
    "    print(\"\\n----- Tag Statistics -----\")\n",
    "    print(f\"Total Tags Found: {total_tags}\")\n",
    "    print(f\"Average Number of Tags per Response: {average_tags_per_response:.2f}\")\n",
    "    print(f\"Average Length of Tag Content: {average_tag_length:.2f} characters\")\n",
    "    print(\"--------------------------\\n\")\n",
    "    print(\"===== End of Statistics =====\\n\")\n",
    "\n",
    "def main():\n",
    "    input_csv = '/Users/log/Github/textual_grounding/logan/results/GSM8K/llama/mermaid/mermaid_get_answer_llama3.1_20240926_215344.csv'  # Replace with your input CSV file path\n",
    "    ground_truth_file = '/Users/log/Github/textual_grounding/data/GSM8K/test.jsonl'  # Path to the ground truth JSONL file\n",
    "\n",
    "    # Check if input files exist\n",
    "    if not os.path.isfile(input_csv):\n",
    "        print(f\"Input CSV file not found: {input_csv}\")\n",
    "        return\n",
    "    if not os.path.isfile(ground_truth_file):\n",
    "        print(f\"Ground truth JSONL file not found: {ground_truth_file}\")\n",
    "        return\n",
    "\n",
    "    # Parse the input CSV file to extract IDs, questions, and answers\n",
    "    qa_pairs = parse_csv_file(input_csv)\n",
    "    print(f\"Total QA Pairs Parsed: {len(qa_pairs)}\")  # Debug: Print the number of QA pairs parsed\n",
    "\n",
    "    # Read the ground truth answers\n",
    "    ground_truth = read_ground_truth(ground_truth_file)\n",
    "    print(f\"Total Ground Truth Entries: {len(ground_truth)}\")  # Debug: Print the number of ground truth entries\n",
    "\n",
    "    # Check if any QA pairs were found\n",
    "    if not qa_pairs:\n",
    "        print(\"No question-answer pairs were found in the input file.\")\n",
    "        return\n",
    "\n",
    "    # Generate and print the statistics\n",
    "    create_statistics(qa_pairs, ground_truth)\n",
    "\n",
    "    print(\"Statistics analysis completed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
