{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import ollama\n",
    "import google.generativeai as genai\n",
    "import anthropic\n",
    "import ollama\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from google.generativeai.types import RequestOptions\n",
    "from google.api_core import retry\n",
    "from typing import List, Tuple\n",
    "import json\n",
    "import datetime\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "import visualize\n",
    "import pandas as pd\n",
    "from utils.utils import add_color_to_tags, extract_parts_0, extract_parts_1\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(save_path: str, ids: List[str], questions: List[str], answers: List[str], append: bool = False):\n",
    "    \"\"\"\n",
    "    Saves the results to a CSV file. If append is True and the file exists, it appends without headers.\n",
    "    Otherwise, it writes a new file with headers.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({'id': ids, 'question': questions, 'answer': answers})\n",
    "    if append and os.path.exists(save_path):\n",
    "        df.to_csv(save_path, mode='a', index=False, header=False)\n",
    "    else:\n",
    "        df.to_csv(save_path, index=False)\n",
    "\n",
    "def read_jsonl_file(filepath: str) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Reads a JSONL file and returns a list of JSON objects.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            json_obj = json.loads(line)\n",
    "            data.append(json_obj)\n",
    "    return data\n",
    "\n",
    "def get_prompt(prompt_type: str, few_shot_prompt: str, question: str) -> str:\n",
    "    \"\"\"\n",
    "    Constructs the prompt based on the prompt type.\n",
    "    \"\"\"\n",
    "    prompts = {\n",
    "        \"cot\": f\"{few_shot_prompt}\\n{question}\\nPlease generate your explanation first, then generate the answer in the bracket as follow:\\n\" +\"Answer: {}\",\n",
    "        \"fs\": f\"{few_shot_prompt}\\n{question}\",\n",
    "        \"fs_inst\": f\"{few_shot_prompt}\\n{question}\\nI want you to answer this question but your explanation should contain references referring back to the information in the question. To do that, first, re-generate the question with proper tags and then generate your answers. The output format is as follow:\\n\\\n",
    "            Reformatted Question: \\\n",
    "                Answer:\",\n",
    "        \"zs\": f\"{question}\\nI want you to answer this question but your explanation should contain references referring back to the information in the question. To do that, first, re-generate the question with proper tags (<a>, <b>, <c>, etc) for refered information and then generate your answers that also have the tag (<a>, <b>, <c>, etc) for the grounded information. Give your answer by analyzing step by step, and give only numbers in the final answer. The output format is as follow:\\n\\\n",
    "            Reformatted Question: \\\n",
    "                Answer:\\\n",
    "                    Final answer:\",\n",
    "        \"fs_xml\": f\"{few_shot_prompt}\\n\\nRecreate the following question in the style of the correctly formatted examples shown previously. Make sure that your response has all its information inclosed in the proper <tags>. Begin your response with the <key_facts> section. Make sure that every fact in <key_facts> is very concise and contains a very short reference to the <question>. Do not include a <question> section in your response\\n\\n<question>\\n{question}\\n</question>\",\n",
    "        \"fs_log_inst\": f\"{few_shot_prompt}\\n\\n{question}\\nTo answer this question, your explanation should contain references referring back to the information in the question. To do that, first, re-generate the question with proper tags and then generate your answers based off the tags. Put your final answer in curly brackets e.g. Final_Answer: {{30}}\",\n",
    "        \"mermaid_get_answer\": f\"{few_shot_prompt}\\n\\n Your job is to extract the key facts from a question relevant to answering the question. The facts should be represented in a hierarchal format through a mermaid diagram. Do not create duplicate facts across multiple branches that represent the same information. Create a mermaid diagram that represents the key facts in the following question. Then, use the nodes from this graph to cite specific facts in your answer reasoning. Put your final answer in curly brackets e.g. Final_Answer: {{30}} \\n\\nquestion: {question}\", \n",
    "    }\n",
    "    return prompts.get(prompt_type, \"\")\n",
    "\n",
    "# cycle through all the keys \n",
    "# def get_gemini_key(problem_id):\n",
    "#     GOOGLE_KEYS = [\n",
    "#         'AIzaSyBQ7zvIZoET3199GNhuz86vKagn_JCEOmk', # original - gen lang client\n",
    "#         'AIzaSyCEI-5U4z7-3q-uwlvkOrdT2e78aNmjnbg', # chat app\n",
    "#         'AIzaSyCvycd0yZZ4GSj47qDLk4JoPemvzUSfvio', # project 1\n",
    "#         'AIzaSyD5xNbDkaJMEMBpWEXYNq5SheF6omdKpzg', # project 2\n",
    "#         'AIzaSyAjcrp_otRjGsj0YvB1cUc2BMng6KSEZwU', # project 3\n",
    "#         'AIzaSyB43xEllzAqGJjz-ExIGadXpUQllQ6PiI4', # project 4\n",
    "#         'AIzaSyDDTCn4lKul4vMj9GmEGJBxZFHb6QZSoA8', # project 5\n",
    "#         'AIzaSyB1sNUXN9CNpRWwqQnwVBBzMF37kYCNOIY', # project 6\n",
    "#         'AIzaSyBqRruZh4d4jq8q6FtUci71nOkqcVlpNLM', # project 7\n",
    "#         'AIzaSyATMO-YWZX4qtMru-NKcodolGr_4kKme5U', # project 8\n",
    "#         'AIzaSyBbKx5spKBPS2tVaUje2Vc1e2v7T6ouUGc', # project 9\n",
    "#         'AIzaSyCPb6W1e7uNI6UoSDTkJRmvkNbl1Tzgpmg', # project 10\n",
    "#         'AIzaSyDqj50lzn-YYIZ92NID4MKgReTeSEJgZuk', # project 11 --\n",
    "#         'AIzaSyBXO1lqmulX82oJjgGh4EPWWcGunxlFjFg', # project 12\n",
    "#         'AIzaSyBt95gM49zINc5l0cZKy285wvtc-kTUTt0', # project 13\n",
    "#         'AIzaSyBf4ty3TH3UC0-TvE-UwhMcrYePZS8_lNs', # project 14\n",
    "#         'AIzaSyDOLgV0DQN7jQwvUbpYyr7jjz8TPYLzdDc', # project 15\n",
    "#     ]\n",
    "#     index = problem_id%len(GOOGLE_KEYS)\n",
    "#     print(f\"getting key from {index}\")\n",
    "#     key = GOOGLE_KEYS[index]\n",
    "#     return key\n",
    "\n",
    "# def query_gemini(prompt: str, problem_id) -> str:\n",
    "#     \"\"\"\n",
    "#     Queries the Gemini LLM with the given prompt and returns the response text.\n",
    "#     \"\"\"\n",
    "#     genai.configure(api_key=get_gemini_key(problem_id))\n",
    "#     model = genai.GenerativeModel('gemini-1.5-pro-latest')\n",
    "#     response = model.generate_content(prompt, request_options=RequestOptions(retry=retry.Retry(initial=20, multiplier=3, maximum=121, timeout=60)))\n",
    "#     text = response.candidates[0].content.parts[0].text\n",
    "#     return text\n",
    "\n",
    "def query_claude(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Queries the Claude LLM with the given prompt and returns the response text.\n",
    "    \"\"\"\n",
    "    client = anthropic.Anthropic(api_key=API_KEYS['claude'])\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20240620\",\n",
    "        max_tokens=1024,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.content[0].text\n",
    "\n",
    "def query_4o_mini(prompt: str) -> str:\n",
    "    client = OpenAI()\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini-2024-07-18\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{prompt}\"\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "def query_llm(llm_model: str, ids: List[str], questions: List[str], few_shot_prompt: str, prompt_type: str, save_path: str, already_answered_ids: set) -> Tuple[List[str], List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Queries the specified LLM for each question, skipping already answered ones.\n",
    "    Saves each response immediately after it's obtained.\n",
    "    Returns lists of answered IDs, questions, and answers.\n",
    "    \"\"\"\n",
    "    answers = []\n",
    "    ids_can_be_answered = []\n",
    "    questions_can_be_answered = []\n",
    "    \n",
    "    for id, q in tqdm(zip(ids, questions), total=len(ids)):\n",
    "        # print(f\"Processing ID: {id}\")\n",
    "        if id in already_answered_ids:\n",
    "            print(f\"Skipping already answered ID: {id}\")\n",
    "            continue\n",
    "        \n",
    "        prompt = get_prompt(prompt_type, few_shot_prompt, q)\n",
    "        try:\n",
    "            if llm_model == 'gemini':\n",
    "                answer = query_gemini(prompt, id)\n",
    "            elif llm_model == 'claude':\n",
    "                answer = query_claude(prompt)\n",
    "            elif llm_model == 'llama3.1':\n",
    "                answer = ollama.generate(model='llama3.1', prompt=prompt)['response']\n",
    "                print(f\"Processed ID: {id}\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported LLM model: {llm_model}\")\n",
    "            # print(f\"Answer for ID {id}: {answer}\")\n",
    "            \n",
    "            # Append to lists\n",
    "            answers.append(answer)\n",
    "            questions_can_be_answered.append(q)\n",
    "            ids_can_be_answered.append(id)\n",
    "\n",
    "            # Save after each answer\n",
    "            save_results(save_path, [id], [q], [answer], append=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question {id}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return ids_can_be_answered, questions_can_be_answered, answers\n",
    "\n",
    "def load_data(data_path: str, sample_size: int = None) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Loads data from a JSONL file, optionally sampling a subset.\n",
    "    \"\"\"\n",
    "    data = read_jsonl_file(data_path)\n",
    "    print(f\"Loaded {len(data)} records from: {data_path}\")\n",
    "    if sample_size:\n",
    "        data = random.sample(data, sample_size)\n",
    "        print(f\"Sampled {sample_size} records.\")\n",
    "    questions = [x[\"question\"] for x in data]\n",
    "    ids = [x[\"id\"] for x in data]\n",
    "    return ids, questions\n",
    "\n",
    "def load_data_deterministic(data_path: str, sample_size: int = None) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Loads data from a JSONL file in a deterministic manner by sorting.\n",
    "    \"\"\"\n",
    "    data = read_jsonl_file(data_path)\n",
    "    print(f\"Loaded {len(data)} records from: {data_path}\")\n",
    "    if sample_size:\n",
    "        # Sort the data based on a consistent criterion (e.g., 'id' or 'question')\n",
    "        sorted_data = sorted(data, key=lambda x: x['id'])\n",
    "        # Take the first 'sample_size' items\n",
    "        data = sorted_data[:sample_size]\n",
    "        print(f\"Selected first {sample_size} records after sorting.\")\n",
    "    questions = [x[\"question\"] for x in data]\n",
    "    ids = [x[\"id\"] for x in data]\n",
    "    return ids, questions\n",
    "\n",
    "def load_data_size_specific(data_path: str, sample_size: int = 0):\n",
    "    # data = read_jsonl_file(data_path)\n",
    "    with open(data_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    random_data = data\n",
    "    print(random_data)\n",
    "    \n",
    "    # for idx, item in enumerate(random_data):\n",
    "    #     if 'id' not in item:\n",
    "    #         # Option 1: Use enumeration for simple integer IDs\n",
    "    #         item['id'] = idx + 1  # Starting IDs from 1\n",
    "    \n",
    "    question_length = 0 # 336  # 526 # 800\n",
    "    \n",
    "    questions = [x[\"prompt\"] for x in random_data if len(x[\"prompt\"]) >= question_length]\n",
    "    ids = [x[\"id\"] for x in random_data if len(x[\"prompt\"]) >= question_length]\n",
    "    return ids[:sample_size], questions[:sample_size]\n",
    "\n",
    "def load_few_shot_prompt(prompt_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Loads the few-shot prompt from a text file.\n",
    "    \"\"\"\n",
    "    with open(prompt_path, 'r') as file:\n",
    "        prompt = file.read()\n",
    "    # print(f\"Loaded few-shot prompt from: {prompt_path}\")\n",
    "    return prompt\n",
    "\n",
    "def load_already_answered_ids(save_path: str) -> set:\n",
    "    \"\"\"\n",
    "    Loads the set of IDs that have already been answered from the CSV file.\n",
    "    Returns an empty set if the file does not exist.\n",
    "    \"\"\"\n",
    "    if os.path.exists(save_path):\n",
    "        df = pd.read_csv(save_path)\n",
    "        answered_ids = set(df['id'].astype(int).tolist())\n",
    "        # print(f\"Loaded {len(answered_ids)} already answered IDs from: {save_path}\")\n",
    "        print(f\"Already answered IDs: {answered_ids}\")\n",
    "        return answered_ids\n",
    "    else:\n",
    "        print(f\"No existing save file found at: {save_path}. Starting fresh.\")\n",
    "        return set()\n",
    "\n",
    "def initialize_save_file(save_path: str):\n",
    "    \"\"\"\n",
    "    Initializes the CSV file with headers if it doesn't exist.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(save_path):\n",
    "        # Create an empty DataFrame with headers and save\n",
    "        df = pd.DataFrame(columns=['id', 'question', 'answer'])\n",
    "        df.to_csv(save_path, index=False)\n",
    "        print(f\"Initialized new save file with headers at: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b605482a93d40e698e065b9672c2231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.68M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdaab0f32be94ecf8fc70e9eb7e364cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/290k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7112049326d44dde995d6c08e4ae2caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/289k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f62aeb3d6d04ab1813dc6a260a8faea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7376 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f60e87d0a43d42b2822ad61eab36982a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/651 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bc9fc7a98be4b0686e9f983c4287f1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/651 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved as JSON files.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"lucasmccabe/logiqa\")\n",
    "\n",
    "# Save each split to a separate JSON file\n",
    "for split in dataset.keys():\n",
    "    # Convert the dataset to a list of dictionaries\n",
    "    data = dataset[split].to_dict()\n",
    "    \n",
    "    # Convert the list of dictionaries to JSON\n",
    "    with open(f'logiqa_{split}.json', 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "\n",
    "print(\"Dataset saved as JSON files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time = datetime.datetime.now().strftime(\"%m%d_%H%M%S\")\n",
    "time = '1003_002310'\n",
    "project_root = '/Users/log/Github/textual_grounding/'\n",
    "dataset = 'AIW'\n",
    "\n",
    "# llm_model = 'llama3.1'\n",
    "llm_model = 'gemini'\n",
    "prompt_type = 'fs_log_inst'\n",
    "# prompt_type = 'cot'\n",
    "few_shot_txt = 'fewshot_cot.txt'\n",
    "# few_shot_txt = '3shot_cot.txt'\n",
    "\n",
    "# Paths\n",
    "# data_path = os.path.join(project_root, 'data', dataset, 'test.jsonl')\n",
    "data_path = os.path.join(project_root, 'data', dataset, 'test.json')\n",
    "# data_path = '/Users/log/Github/textual_grounding/logan/results/GSM8K/llama/mermaid/mermaid_get_graph_llama3.1_20240924_001821.csv'\n",
    "\n",
    "fewshot_prompt_path = os.path.join(project_root, \"prompt\", dataset, few_shot_txt)\n",
    "# fewshot_prompt_path = '/Users/log/Github/textual_grounding/prompt/GSM8K/fewshot_mermaid_full.txt'\n",
    "save_dir = os.path.join(project_root, 'logan/results', dataset, f'{llm_model}/mermaid')\n",
    "os.makedirs(save_dir, exist_ok=True)  # Ensure the directory exists\n",
    "save_path = os.path.join(save_dir, f'{prompt_type}_{llm_model}_{time}.csv')\n",
    "\n",
    "# ids, questions = load_data_deterministic(data_path, sample_size=200)\n",
    "ids, questions = load_data_size_specific(data_path, sample_size=200)\n",
    "few_shot_prompt = load_few_shot_prompt(fewshot_prompt_path)\n",
    "\n",
    "\n",
    "\n",
    "initialize_save_file(save_path)\n",
    "already_answered_ids = load_already_answered_ids(save_path)\n",
    "\n",
    "\n",
    "ids_answered, questions_answered, answers = query_llm(\n",
    "    llm_model=llm_model,\n",
    "    ids=ids,\n",
    "    questions=questions,\n",
    "    few_shot_prompt=few_shot_prompt,\n",
    "    prompt_type=prompt_type,\n",
    "    save_path=save_path,\n",
    "    already_answered_ids=already_answered_ids\n",
    ")\n",
    "\n",
    "print(f\"Processing complete. {len(ids_answered)} new answers saved to {save_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogiQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/log/.cache/huggingface/modules/datasets_modules/datasets/logiqa/5843107fea0dc7d86317447a73f6b7a8a8b7c0cd665779638fe13811889aa8e1/logiqa.py:56: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  if re.match('^[A-Z][\\w\\s]+[?.!]$', text) is None:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44426b9f42ba43c386b817083e40a59c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c008a0d8a0b45448e6b4d9d846fe58b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/165k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638d983d7dca45e8b6c49c63fee71a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/164k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "594e51da61a04ad0a96c4f27bfef68b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7376 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae673f7085f468ea2434df2c2f825c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/651 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc6cfc3c9444cfe800af0e58e2b27b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/651 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "Context: In the planning of a new district in a township, it was decided to build a special community in the southeast, northwest, centered on the citizen park. These four communities are designated as cultural area, leisure area, commercial area and administrative service area. It is known that the administrative service area is southwest of the cultural area, and the cultural area is southeast of the leisure area.\n",
      "Query: Based on the above statement, which of the following can be derived?\n",
      "Options: ['Civic Park is north of the administrative service area.', 'The leisure area is southwest of the cultural area.', 'The cultural district is in the northeast of the business district.', 'The business district is southeast of the leisure area.']\n",
      "Correct Option Index: 0\n",
      "--------------------------------------------------\n",
      "Example 2:\n",
      "Context: The company sent three young staff members to the South for business trip. The three of them happened to be sitting in a row. At least one of the two people sitting on the right side of 24 years old was 20 years old.20 years old. At least one of the two people sitting on the left of the accountant is a salesperson, and one of the two people sitting on the right of the salesperson is also a salesperson.\n",
      "Query: So what are the three young people on business?\n",
      "Options: ['0-year-old accountant, 20-year-old salesperson, 24-year-old salesperson.', '0-year-old accountant, 24-year-old salesperson, 24-year-old salesperson.', '4-year-old accountant, 20-year-old salesperson, 20-year-old salesperson.', '0-year-old accountant, 20-year-old accountant, 24-year-old salesperson.']\n",
      "Correct Option Index: 0\n",
      "--------------------------------------------------\n",
      "Example 3:\n",
      "Context: In a traditional Chinese medicine preparation, there must be at least one kind of ginseng or codonopsis, and the following conditions must also be met? 1) If there is codonopsis, there must be atractylodes.2) Atractylodes macrocephala and ginseng can only have at most one. You must have Shouwu.4) If you have Shouwu, you must have Atractylodes.\n",
      "Query: According to the above statement, which of the following can be drawn about this Chinese medicine preparation?\n",
      "Options: ['o dangshen.', 'o Shouwu.', ' 白 术.', ' 白 术.']\n",
      "Correct Option Index: 1\n",
      "--------------------------------------------------\n",
      "Example 4:\n",
      "Context: In recent years, graduate entrance examinations have continued to heat up. Correspondingly, a variety of postgraduate counseling classes have emerged, especially English and political counseling classes are almost a must for the postgraduates. Xiaozhuang, who has just started working, also intends to take the postgraduate entrance exam, so Xiaozhuang must take English tutoring classes.\n",
      "Query: Which of the following can best strengthen the above argument.\n",
      "Options: ['If you take an English tutoring class, you can pass the graduate entrance exam.', 'Only those who intend to take the graduate entrance exam will participate in the English tutoring class.', 'Even if you take an English tutoring class, you may not be able to pass the graduate entrance exam.', 'If you do not participate in the English tutoring class, you cannot pass the graduate entrance exam.']\n",
      "Correct Option Index: 3\n",
      "--------------------------------------------------\n",
      "Example 5:\n",
      "Context: A unit conducted the year-end assessment and after democratic voting, five people were identified as candidates for the first prize. In the selection of five in four, the following factors need to be considered? 1) At least one person of Bingding is selected.2) If E is selected, then A and B are also selected.3) A maximum of 2 people are selected.\n",
      "Query: According to the above statement, it can be concluded that who is not in the fourth?\n",
      "Options: ['A.', 'B.', 'C.', 'Ding.']\n",
      "Correct Option Index: 3\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Add the directory containing logiqa.py to the Python path\n",
    "logiqa_path = \"/Users/log/Github/textual_grounding/data/logiqa\"\n",
    "sys.path.append(logiqa_path)\n",
    "\n",
    "# Import the LogiQA class from the logiqa module if needed\n",
    "from logiqa import LogiQA\n",
    "\n",
    "# Load the dataset using Hugging Face load_dataset method\n",
    "dataset = load_dataset('/Users/log/Github/textual_grounding/data/logiqa/logiqa.py', split='test')\n",
    "\n",
    "# Print out the first 5 examples from the test set\n",
    "for idx in range(5):\n",
    "    example = dataset[idx]\n",
    "    print(f\"Example {idx + 1}:\")\n",
    "    print(f\"Context: {example['context']}\")\n",
    "    print(f\"Query: {example['query']}\")\n",
    "    print(f\"Options: {example['options']}\")\n",
    "    print(f\"Correct Option Index: {example['correct_option']}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 300 examples to logiqa_300_examples.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset (adjust the path as needed)\n",
    "dataset = load_dataset('/Users/log/Github/textual_grounding/data/logiqa/logiqa.py', split='test')\n",
    "\n",
    "# Prepare to write the first 300 examples to a JSONL file\n",
    "output_file = 'logiqa_300_examples.jsonl'\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for idx, example in enumerate(dataset):\n",
    "        if idx >= 300:\n",
    "            break\n",
    "        \n",
    "        # Create the \"question\" field by concatenating context, query, and options\n",
    "        context = example['context']\n",
    "        query = example['query']\n",
    "        options = example['options']\n",
    "        options_str = \" \".join([f\"({chr(65 + i)}) {opt}\" for i, opt in enumerate(options)])\n",
    "        question = f\"{context} {query}\\n{options_str}\"\n",
    "        \n",
    "        # Create the dictionary for the current example\n",
    "        example_dict = {\n",
    "            \"id\": idx,\n",
    "            \"question\": question,\n",
    "            \"answer\": chr(65 + example['correct_option'])  # Convert index to letter (A, B, C, D)\n",
    "        }\n",
    "        \n",
    "        # Write the example as a JSON object to the JSONL file\n",
    "        f.write(json.dumps(example_dict) + '\\n')\n",
    "\n",
    "print(f\"Saved 300 examples to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XML - visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML content has been successfully written to test_grounding_answer_prompt_fs_xml_llama3.1.html\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "def extract_parts_1(answer_text):\n",
    "    \"\"\"\n",
    "    Processes the answer text to extract key facts (with numbers), answer reasoning, and the final answer.\n",
    "\n",
    "    Args:\n",
    "        answer_text (str): The full answer text containing <key_facts>, <answer_reasoning>, and <final_answer>.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (key_facts_list, answer_reasoning, final_answer)\n",
    "               where key_facts_list is a list of tuples (fact_number, fact_content)\n",
    "    \"\"\"\n",
    "    # Extract key_facts\n",
    "    key_facts_match = re.search(r'<key_facts>(.*?)</key_facts>', answer_text, re.DOTALL)\n",
    "    key_facts_content = key_facts_match.group(1).strip() if key_facts_match else \"\"\n",
    "\n",
    "    # Extract individual facts with their numbers\n",
    "    facts = re.findall(r'<fact_(\\d+)>(.*?)</fact_\\d+>', key_facts_content, re.DOTALL)\n",
    "    key_facts_list = [(number.strip(), content.strip()) for number, content in facts]\n",
    "\n",
    "    # Extract answer_reasoning\n",
    "    reasoning_match = re.search(r'<answer_reasoning>(.*?)</answer_reasoning>', answer_text, re.DOTALL)\n",
    "    answer_reasoning = reasoning_match.group(1).strip() if reasoning_match else \"\"\n",
    "\n",
    "    # Extract final_answer\n",
    "    final_match = re.search(r'<final_answer>(.*?)</final_answer>', answer_text, re.DOTALL)\n",
    "    final_answer = final_match.group(1).strip() if final_match else \"\"\n",
    "\n",
    "    return key_facts_list, answer_reasoning, final_answer\n",
    "\n",
    "\n",
    "def add_color_to_tags(text):\n",
    "    \"\"\"\n",
    "    Adds background color to specific tags within the text based on a predefined color mapping.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text containing tags like <fact_1>, <fact_2>, etc.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with added inline CSS for background colors.\n",
    "    \"\"\"\n",
    "    tag_color_mapping = {\n",
    "        'fact_1': 'yellow',  \n",
    "        'fact_2': 'lightblue',\n",
    "        'fact_3': 'lightgreen',\n",
    "        'fact_4': 'lightcoral',\n",
    "        'fact_5': 'lightcyan', \n",
    "        'fact_6': 'orange',\n",
    "    }\n",
    "    # Iterate over the tag-color mappings\n",
    "    for tag, color in tag_color_mapping.items():\n",
    "        # Regex to find the tag and replace it with the same tag having a style attribute\n",
    "        text = re.sub(\n",
    "            f'<{tag}>(.*?)</{tag}>',\n",
    "            f'<{tag} style=\"background-color: {color};\">\\\\1</{tag}>',\n",
    "            text,\n",
    "            flags=re.DOTALL\n",
    "        )\n",
    "    return text\n",
    "\n",
    "\n",
    "def parse_csv_file(file_path):\n",
    "    \"\"\"\n",
    "    Parses the input CSV file and extracts questions and their corresponding answers.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the input CSV file.\n",
    "\n",
    "    Returns:\n",
    "        list of tuples: Each tuple contains (question, answer_text).\n",
    "    \"\"\"\n",
    "    qa_pairs = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            question = row.get('question', 'No question found.').strip()\n",
    "            answer_text = row.get('answer', 'No answer found.').strip()\n",
    "            qa_pairs.append((question, answer_text))\n",
    "    return qa_pairs\n",
    "\n",
    "\n",
    "def create_highlight_html(qa_pairs):\n",
    "    \"\"\"\n",
    "    Creates HTML content with highlighted questions, key facts, answer reasoning, and answers.\n",
    "\n",
    "    Args:\n",
    "        qa_pairs (list of tuples): Each tuple contains (question, answer_text).\n",
    "\n",
    "    Returns:\n",
    "        str: The complete HTML content as a string.\n",
    "    \"\"\"\n",
    "    html_content = \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <title>Question and Answer Highlights</title>\n",
    "        <style>\n",
    "            body {\n",
    "                font-family: Arial, sans-serif;\n",
    "                margin: 20px;\n",
    "                background-color: #f0f0f0;\n",
    "            }\n",
    "            .container {\n",
    "                background-color: #ffffff;\n",
    "                padding: 20px;\n",
    "                margin-bottom: 20px;\n",
    "                border-radius: 8px;\n",
    "                box-shadow: 0 2px 5px rgba(0,0,0,0.1);\n",
    "            }\n",
    "            .question {\n",
    "                font-size: 1.2em;\n",
    "                margin-bottom: 10px;\n",
    "            }\n",
    "            .key-facts {\n",
    "                margin-bottom: 10px;\n",
    "            }\n",
    "            .key-facts ul {\n",
    "                list-style-type: number;\n",
    "                padding-left: 20px;\n",
    "            }\n",
    "            .key-facts ul li{\n",
    "                margin-bottom: 4px;\n",
    "            }\n",
    "            .answer-reasoning, .final-answer {\n",
    "                margin-bottom: 10px;\n",
    "            }\n",
    "            .highlight {\n",
    "                background-color: #FFFF00; /* Yellow background for visibility */\n",
    "                font-weight: bold; /* Bold text for emphasis */\n",
    "            }\n",
    "            /* Styles for specific facts */\n",
    "            fact_1 {\n",
    "                background-color: yellow;\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            fact_2 {\n",
    "                background-color: lightblue;\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            fact_3 {\n",
    "                background-color: lightgreen;\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            fact_4 {\n",
    "                background-color: lightcoral;\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            fact_5 {\n",
    "                background-color: lightcyan;\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            fact_6 {\n",
    "                background-color: orange;\n",
    "                font-weight: bold;\n",
    "            }\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "    <h1>Question and Answer Highlights</h1>\n",
    "    \"\"\"\n",
    "    for i, (question, answer_text) in enumerate(qa_pairs, 1):\n",
    "        try:\n",
    "            key_facts, answer_reasoning, final_answer = extract_parts_1(answer_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot extract parts for question {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Convert key_facts list to HTML bullet points with \"Key fact X:\" prefix\n",
    "        if key_facts:\n",
    "            key_facts_html = \"<ul>\\n\"\n",
    "            for fact_number, fact_content in key_facts:\n",
    "                # Apply color to tags in fact_content\n",
    "                highlighted_fact = add_color_to_tags(fact_content)\n",
    "                # Prepend \"Key fact X:\"\n",
    "                key_facts_html += f\"    <li><fact_{fact_number}>{highlighted_fact}</fact_{fact_number}></li>\\n\"\n",
    "            key_facts_html += \"</ul>\"\n",
    "        else:\n",
    "            key_facts_html = \"<p>No key facts available.</p>\"\n",
    "\n",
    "        # Apply color to tags in answer_reasoning and final_answer\n",
    "        highlighted_reasoning = add_color_to_tags(answer_reasoning)\n",
    "        highlighted_final_answer = add_color_to_tags(final_answer)\n",
    "\n",
    "        # Build the HTML structure\n",
    "        html_content += f\"<div class='container'>\"\n",
    "        html_content += f\"<div class='question'><strong>Question:</strong> {question}</div>\"\n",
    "        html_content += f\"<div class='key-facts'><strong>Key Facts:</strong> {key_facts_html}</div>\"\n",
    "        html_content += f\"<div class='answer-reasoning'><strong>Answer Reasoning:</strong> {highlighted_reasoning}</div>\"\n",
    "        html_content += f\"<div class='final-answer'><strong>Answer:</strong> {highlighted_final_answer}</div>\"\n",
    "        html_content += \"</div>\\n\"\n",
    "\n",
    "    # Close the HTML tags\n",
    "    html_content += \"\"\"\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    return html_content\n",
    "\n",
    "\n",
    "def main():\n",
    "    input_file = '/Users/log/Github/textual_grounding/logan/results/GSM8K/llama/test_grounding_answer_prompt_fs_xml_llama3.1.csv'  # Replace with your input CSV file path\n",
    "    output_file = 'test_grounding_answer_prompt_fs_xml_llama3.1.html'  # Replace with your desired output HTML file path\n",
    "\n",
    "    # Parse the input CSV file to extract questions and answers\n",
    "    qa_pairs = parse_csv_file(input_file)\n",
    "\n",
    "    # Check if any QA pairs were found\n",
    "    if not qa_pairs:\n",
    "        print(\"No question-answer pairs were found in the input file.\")\n",
    "        return\n",
    "\n",
    "    # Generate the HTML content\n",
    "    html_content = create_highlight_html(qa_pairs)\n",
    "\n",
    "    # Write the HTML content to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(html_content)\n",
    "\n",
    "    print(f\"HTML content has been successfully written to {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mermaid - Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total QA Pairs Parsed: 58\n",
      "Total Ground Truth Entries: 256\n",
      "id: 1: 2\n",
      "id: 2: 5\n",
      "id: 3: 2\n",
      "id: 16: 5\n",
      "id: 17: 2\n",
      "id: 18: 5\n",
      "id: 19: 3\n",
      "id: 20: 5\n",
      "id: 33: 3\n",
      "id: 34: 5\n",
      "id: 35: 2\n",
      "id: 36: 5\n",
      "id: 37: 2\n",
      "id: 50: 3\n",
      "id: 51: 7\n",
      "id: 52: 4\n",
      "id: 53: 7\n",
      "id: 54: 3\n",
      "id: 67: 3\n",
      "id: 68: 5\n",
      "id: 69: 2\n",
      "id: 70: 2\n",
      "id: 71: 2\n",
      "id: 84: 2\n",
      "id: 85: 2\n",
      "id: 86: 5\n",
      "id: 87: 7\n",
      "id: 88: 3\n",
      "id: 118: null\n",
      "id: 119: null\n",
      "id: 120: 3\n",
      "id: 121: 3\n",
      "id: 122: 3\n",
      "id: 135: 2\n",
      "id: 136: 2\n",
      "id: 137: 2\n",
      "id: 138: 5\n",
      "id: 139: 5\n",
      "id: 152: 8\n",
      "id: 153: 8\n",
      "id: 154: 3\n",
      "id: 155: 4\n",
      "id: 156: 4\n",
      "id: 169: 5\n",
      "id: 170: 5\n",
      "id: 171: 5\n",
      "id: 172: 5\n",
      "id: 173: 5\n",
      "id: 186: 5\n",
      "id: 187: 5\n",
      "id: 188: 2\n",
      "id: 189: 5\n",
      "id: 190: 2\n",
      "id: 203: 2\n",
      "id: 204: 2\n",
      "id: 205: 7\n",
      "id: 206: 3\n",
      "id: 207: 7\n",
      "HTML content has been successfully written to mm_aiw_gemini.html\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import json  # For handling JSONL\n",
    "import os\n",
    "\n",
    "def extract_parts_new_format(answer_text):\n",
    "    \"\"\"\n",
    "    Processes the answer text to extract Mermaid diagrams, answer reasoning, and the final answer.\n",
    "\n",
    "    Args:\n",
    "        answer_text (str): The full answer text containing Mermaid diagrams, answer reasoning, and final answer.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (mermaid_diagrams, answer_reasoning, final_answer)\n",
    "               where mermaid_diagrams is a list of Mermaid diagram strings\n",
    "    \"\"\"\n",
    "    # Extract Mermaid diagrams\n",
    "    # Modified Pattern:\n",
    "    # - Capture 'graph TD' along with the diagram content\n",
    "    mermaid_pattern = re.compile(\n",
    "        r'Fact_Diagram:\\s*(graph\\s+TD\\s*[\\s\\S]*?)(?=Answer_Reasoning:|$)',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    mermaid_diagrams = mermaid_pattern.findall(answer_text)\n",
    "    mermaid_diagrams = [diagram.strip() for diagram in mermaid_diagrams]\n",
    "\n",
    "    # Remove Mermaid diagrams from the answer_text to process the remaining text\n",
    "    answer_text_clean = mermaid_pattern.sub('', answer_text).strip()\n",
    "\n",
    "    # Extract answer_reasoning\n",
    "    reasoning_pattern = re.compile(\n",
    "        r'Answer_Reasoning:\\s*([\\s\\S]*?)(?=Final_Answer:|$)',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    reasoning_match = reasoning_pattern.search(answer_text_clean)\n",
    "    if reasoning_match:\n",
    "        answer_reasoning = reasoning_match.group(1).strip()\n",
    "    else:\n",
    "        # Fallback if 'Answer_Reasoning:' section is missing\n",
    "        answer_reasoning = \"\"\n",
    "\n",
    "    # Extract final_answer\n",
    "    final_answer_pattern = re.compile(\n",
    "        r'Final_Answer:\\s*\\{([^}]+)\\}',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    final_match = final_answer_pattern.search(answer_text_clean)\n",
    "    final_answer = final_match.group(1).replace(',', '').replace('$', '').strip() if final_match else \"\"\n",
    "\n",
    "    return mermaid_diagrams, answer_reasoning, final_answer\n",
    "\n",
    "\n",
    "def add_color_to_tags_new(text):\n",
    "    \"\"\"\n",
    "    Adds background color to specific tags within the text based on dynamically assigned colors.\n",
    "    Each span will have a class corresponding to the tag's name.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text containing tags like <B>, <C1>, etc.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with added inline CSS for background colors and class names.\n",
    "    \"\"\"\n",
    "    # Find all unique tags in the text using regex\n",
    "    tags = set(re.findall(r'<([A-Za-z]+\\d*)>', text))\n",
    "\n",
    "    # Predefined color palette\n",
    "    color_palette = [\n",
    "        'lightyellow', 'lightblue', 'lightgreen', 'lightcoral',\n",
    "        'lightcyan', 'lightpink', 'lightsalmon', 'lightgray',\n",
    "        'lightgoldenrodyellow', 'lightseagreen', 'lightskyblue',\n",
    "        'lightsteelblue'\n",
    "    ]\n",
    "\n",
    "    # Dictionary to hold tag-color mapping\n",
    "    tag_color_mapping = {}\n",
    "\n",
    "    # Assign colors to tags, cycling through the color palette if necessary\n",
    "    for i, tag in enumerate(sorted(tags)):\n",
    "        color = color_palette[i % len(color_palette)]\n",
    "        tag_color_mapping[tag] = color\n",
    "\n",
    "    # Function to replace tags with styled spans including class names\n",
    "    def replace_tag(match):\n",
    "        tag = match.group(1)\n",
    "        content = match.group(2)\n",
    "        color = tag_color_mapping.get(tag, 'lightgray')  # Default color if not found\n",
    "        return f'<span class=\"{tag}\" style=\"background-color: {color}; font-weight: bold;\">{content}</span>'\n",
    "\n",
    "    # Regex to find tags and replace them with styled spans\n",
    "    tag_regex = re.compile(r'<([A-Za-z]+\\d*)>\\s*([\\s\\S]*?)\\s*</\\1>')\n",
    "\n",
    "    # Replace all tags with styled spans\n",
    "    text = tag_regex.sub(replace_tag, text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def parse_csv_file(file_path):\n",
    "    \"\"\"\n",
    "    Parses the input CSV file and extracts questions, answers, and their corresponding IDs.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the input CSV file.\n",
    "\n",
    "    Returns:\n",
    "        list of tuples: Each tuple contains (id, question, answer_text).\n",
    "    \"\"\"\n",
    "    qa_pairs = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            question = row.get('question', 'No question found.').strip()\n",
    "            answer_text = row.get('answer', 'No answer found.').strip()\n",
    "            id_ = row.get('id')\n",
    "            if id_ is not None:\n",
    "                try:\n",
    "                    id_int = int(id_)\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping a row due to invalid 'id' (not an integer): {id_}\")\n",
    "                    continue\n",
    "                qa_pairs.append((id_int, question, answer_text))\n",
    "            else:\n",
    "                # Handle cases without 'id' by skipping\n",
    "                print(f\"Skipping a row due to missing 'id': {row}\")\n",
    "    return qa_pairs\n",
    "\n",
    "\n",
    "# def read_ground_truth(jsonl_path):\n",
    "#     \"\"\"\n",
    "#     Reads the ground truth answers from a JSONL file and maps them by ID.\n",
    "\n",
    "#     Args:\n",
    "#         jsonl_path (str): Path to the ground truth JSONL file.\n",
    "\n",
    "#     Returns:\n",
    "#         dict: A dictionary mapping each ID to its ground truth answer.\n",
    "#     \"\"\"\n",
    "#     ground_truth = {}\n",
    "#     with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#             data = json.loads(line)\n",
    "#             id_ = data.get('index')\n",
    "#             answer = data.get('answer')\n",
    "#             if id_ is not None and answer is not None:\n",
    "\n",
    "#                 ground_truth[id_] = answer\n",
    "#             else:\n",
    "#                 print(f\"Invalid ground truth entry: {data}\")\n",
    "#     return ground_truth\n",
    "\n",
    "def read_ground_truth(json_path):\n",
    "    \"\"\"\n",
    "    Reads the ground truth answers from a JSON file and maps them by a generated ID.\n",
    "\n",
    "    Args:\n",
    "        json_path (str): Path to the ground truth JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping each generated ID to its ground truth answer.\n",
    "    \"\"\"\n",
    "    ground_truth = {}\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            for idx, item in enumerate(data, start=1):\n",
    "                # Generate a unique ID for each question, e.g., Q1, Q2, ...\n",
    "                id_ = item.get('id')\n",
    "                \n",
    "                # Extract the final answer\n",
    "                final_ans = item.get('answer')\n",
    "                \n",
    "                if final_ans is not None:\n",
    "\n",
    "                    clean_ans = final_ans.replace(',', '').replace('$', '').strip()          \n",
    "                    ground_truth[id_] = clean_ans\n",
    "                else:\n",
    "                    print(f\"No final answer found for question ID {id_}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {json_path}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "    \n",
    "    return ground_truth\n",
    "\n",
    "def create_highlight_html_new(qa_pairs, ground_truth):\n",
    "    \"\"\"\n",
    "    Creates HTML content with Mermaid diagrams (both pure text and rendered), highlighted answer reasoning,\n",
    "    final answers colored based on correctness, and ground truth answers.\n",
    "\n",
    "    Args:\n",
    "        qa_pairs (list of tuples): Each tuple contains (id, question, answer_text).\n",
    "        ground_truth (dict): A dictionary mapping each ID to its ground truth answer.\n",
    "\n",
    "    Returns:\n",
    "        str: The complete HTML content as a string.\n",
    "    \"\"\"\n",
    "    html_content = \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <title>Question and Answer Highlights</title>\n",
    "        <style>\n",
    "            body {\n",
    "                font-family: Arial, sans-serif;\n",
    "                margin: 20px;\n",
    "                background-color: #f0f0f0;\n",
    "            }\n",
    "            .container {\n",
    "                background-color: #ffffff;\n",
    "                padding: 20px;\n",
    "                margin-bottom: 20px;\n",
    "                border-radius: 8px;\n",
    "                box-shadow: 0 2px 5px rgba(0,0,0,0.1);\n",
    "            }\n",
    "            .question {\n",
    "                font-size: 1.2em;\n",
    "                margin-bottom: 10px;\n",
    "            }\n",
    "            .mermaid {\n",
    "                margin-bottom: 10px;\n",
    "            }\n",
    "            .mermaidPure {\n",
    "                background-color: #f9f9f9;\n",
    "                padding: 10px;\n",
    "                border: 1px solid #ddd;\n",
    "                border-radius: 4px;\n",
    "                white-space: pre-wrap; /* Preserves whitespace and newlines */\n",
    "                font-family: Consolas, \"Courier New\", monospace;\n",
    "                margin-bottom: 10px;\n",
    "            }\n",
    "            .answer-reasoning, .final-answer, .ground-truth-answer {\n",
    "                margin-bottom: 10px;\n",
    "            }\n",
    "            .final-answer {\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            .ground-truth-answer {\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            /* Styles for the highlighted spans */\n",
    "            .highlighted {\n",
    "                padding: 2px 4px;\n",
    "                border-radius: 3px;\n",
    "                display: inline-block;\n",
    "            }\n",
    "            /* Styles for the summary section */\n",
    "            .summary {\n",
    "                background-color: #e0ffe0;\n",
    "                padding: 15px;\n",
    "                border: 2px solid #00cc00;\n",
    "                border-radius: 8px;\n",
    "                font-size: 1.2em;\n",
    "                margin-top: 30px;\n",
    "            }\n",
    "        </style>\n",
    "        <!-- Include Mermaid.js -->\n",
    "        <script type=\"module\">\n",
    "            import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';\n",
    "            mermaid.initialize({ startOnLoad: true });\n",
    "        </script>\n",
    "    </head>\n",
    "    <body>\n",
    "    <h1>Question and Answer Highlights</h1>\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize counters for correct and total answers\n",
    "    correct_answers = 0\n",
    "    total_answers = 0\n",
    "\n",
    "    # print(f\"id: {id}: {ground_truth}\")\n",
    "    for i, (id_, question, answer_text) in enumerate(qa_pairs, 1):\n",
    "        try:\n",
    "            mermaid_diagrams, answer_reasoning, final_answer = extract_parts_new_format(answer_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot extract parts for question ID {id_}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Convert mermaid diagrams to HTML divs with pure text and rendered version\n",
    "        mermaid_html = \"\"\n",
    "        for diagram in mermaid_diagrams:\n",
    "            # Escape HTML special characters in the pure text version\n",
    "            escaped_diagram = diagram.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')\n",
    "            mermaid_html += f\"<div class='mermaidPure'><pre>{escaped_diagram}</pre></div>\\n\"\n",
    "            mermaid_html += f\"<div class='mermaid'>\\n{diagram}\\n</div>\\n\"\n",
    "\n",
    "        # Apply color to tags in answer_reasoning\n",
    "        highlighted_reasoning = add_color_to_tags_new(answer_reasoning)\n",
    "        # Retrieve ground truth answer\n",
    "        # print(id_)\n",
    "        gt_answer = str(ground_truth.get(id_))\n",
    "        # print(gt_answer)\n",
    "        print(f\"id: {id_}: {gt_answer}\")\n",
    "        if gt_answer is None:\n",
    "            gt_answer_display = \"<span style='color: gray;'>Ground truth not available.</span>\"\n",
    "            is_correct = False\n",
    "        else:\n",
    "            # Normalize both final_answer and gt_answer for comparison\n",
    "            try:\n",
    "                # print(gt_answer)\n",
    "                final_answer_num = float(final_answer.replace(',', '').replace('$', ''))\n",
    "                if type(gt_answer) == list: # multi answer :(\n",
    "                    gt_answer_num = int(gt_answer[0].replace(',', '').replace('$', ''))\n",
    "                else:\n",
    "                    gt_answer_num = float(gt_answer.replace(',', '').replace('$', ''))\n",
    "                is_correct = final_answer_num == gt_answer_num\n",
    "                # Format numbers with commas and two decimal places if needed\n",
    "                if final_answer_num.is_integer():\n",
    "                    final_answer_display = f\"{int(final_answer_num):,}\"\n",
    "                else:\n",
    "                    final_answer_display = f\"{final_answer_num:,.2f}\"\n",
    "                if gt_answer_num.is_integer():\n",
    "                    gt_answer_display = f\"{int(gt_answer_num):,}\"\n",
    "                else:\n",
    "                    gt_answer_display = f\"{gt_answer_num:,.2f}\"\n",
    "            except ValueError:\n",
    "                # In case conversion fails, fallback to string comparison\n",
    "                is_correct = final_answer == gt_answer\n",
    "                final_answer_display = final_answer\n",
    "                gt_answer_display = gt_answer\n",
    "\n",
    "        # Style the final answer based on correctness\n",
    "        if is_correct:\n",
    "            highlighted_final_answer = f\"<span style='font-size:1.1em; color: green;'>{final_answer_display}</span>\"\n",
    "            correct_answers += 1\n",
    "        else:\n",
    "            highlighted_final_answer = f\"<span style='font-size:1.1em; color: red;'>{final_answer_display}</span>\"\n",
    "        total_answers += 1\n",
    "\n",
    "        # Display ground truth answer\n",
    "        if gt_answer is not None:\n",
    "            ground_truth_html = f\"<div class='ground-truth-answer'><strong>Ground Truth Answer:</strong> {gt_answer_display}</div>\"\n",
    "        else:\n",
    "            ground_truth_html = f\"<div class='ground-truth-answer'><strong>Ground Truth Answer:</strong> Not available.</div>\"\n",
    "\n",
    "        # Build the HTML structure\n",
    "        html_content += f\"<div class='container'>\"\n",
    "        html_content += f\"<div class='question'><strong>Question:</strong> {question}</div>\"\n",
    "        if mermaid_html:\n",
    "            html_content += f\"{mermaid_html}\"\n",
    "        else:\n",
    "            html_content += f\"<p>No diagram available.</p>\"\n",
    "        html_content += f\"<div class='answer-reasoning'><strong>Answer Reasoning:</strong> {highlighted_reasoning}</div>\"\n",
    "        html_content += f\"<div class='final-answer'><strong>Final Answer:</strong> {highlighted_final_answer}</div>\"\n",
    "        html_content += f\"{ground_truth_html}\"\n",
    "        html_content += \"</div>\\n\"\n",
    "\n",
    "    # After processing all QA pairs, add the summary section\n",
    "    summary_percentage = (correct_answers / total_answers * 100) if total_answers > 0 else 0\n",
    "    summary_html = f\"\"\"\n",
    "    <div class='summary'>\n",
    "        <strong>Summary:</strong> Correct Answers: {correct_answers} / {total_answers} ({summary_percentage:.2f}%)\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    # It's more logical to place the summary at the end\n",
    "    # html_content += summary_html\n",
    "    output_html = summary_html + html_content\n",
    "    # Close the HTML tags\n",
    "    output_html += \"\"\"\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    return output_html\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Replace these paths with your actual file paths\n",
    "    input_csv = '/Users/log/Github/textual_grounding/logan/results/AIW/gemini/mermaid/fs_log_inst_gemini_1003_002310.csv'  # Replace with your input CSV file path\n",
    "    ground_truth_file = '/Users/log/Github/textual_grounding/data/AIW/test.json'  # Path to the ground truth JSONL file\n",
    "    output_file = 'mm_aiw_gemini.html'  # Replace with your desired output HTML file path\n",
    "\n",
    "    # Check if input files exist\n",
    "    if not os.path.isfile(input_csv):\n",
    "        print(f\"Input CSV file not found: {input_csv}\")\n",
    "        return\n",
    "    if not os.path.isfile(ground_truth_file):\n",
    "        print(f\"Ground truth JSONL file not found: {ground_truth_file}\")\n",
    "        return\n",
    "\n",
    "    # Parse the input CSV file to extract IDs, questions, and answers\n",
    "    qa_pairs = parse_csv_file(input_csv)\n",
    "    print(f\"Total QA Pairs Parsed: {len(qa_pairs)}\")  # Debug: Print the number of QA pairs parsed\n",
    "\n",
    "    # Read the ground truth answers\n",
    "    ground_truth = read_ground_truth(ground_truth_file)\n",
    "    print(f\"Total Ground Truth Entries: {len(ground_truth)}\")  # Debug: Print the number of ground truth entries\n",
    "\n",
    "    # Check if any QA pairs were found\n",
    "    if not qa_pairs:\n",
    "        print(\"No question-answer pairs were found in the input file.\")\n",
    "        return\n",
    "\n",
    "    # Generate the HTML content\n",
    "    html_content = create_highlight_html_new(qa_pairs, ground_truth)\n",
    "\n",
    "    # Write the HTML content to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(html_content)\n",
    "\n",
    "    print(f\"HTML content has been successfully written to {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mm multi arith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoT - Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total QA Pairs Parsed: 200\n",
      "No ground truth answer found for ID 489\n",
      "No ground truth answer found for ID 1113\n",
      "Total Ground Truth Entries: 1317\n",
      "HTML content has been successfully written to cot_llama3.1.html\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import json  # For handling JSONL\n",
    "import os\n",
    "\n",
    "def extract_parts_regular_cot(answer_text):\n",
    "    \"\"\"\n",
    "    Processes the answer text to extract answer reasoning and the final answer.\n",
    "\n",
    "    Args:\n",
    "        answer_text (str): The full answer text containing answer reasoning and final answer.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (answer_reasoning, final_answer)\n",
    "               where answer_reasoning is the full model response,\n",
    "               and final_answer is the extracted answer.\n",
    "    \"\"\"\n",
    "\n",
    "    # Fallback: Extract Final Answer from '{...}' in the reasoning\n",
    "        curly_match = re.search(r'\\{([\\d.]+)\\}', answer_text)\n",
    "        final_answer = curly_match.group(1).strip() if curly_match else \"\"\n",
    "\n",
    "    return answer_text.strip(), final_answer\n",
    "\n",
    "\n",
    "def add_color_to_tags_new(text):\n",
    "    \"\"\"\n",
    "    Adds background color to specific tags within the text based on dynamically assigned colors.\n",
    "    Each span will have a class corresponding to the tag's name.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text containing tags like <B>, <C1>, etc.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with added inline CSS for background colors and class names.\n",
    "    \"\"\"\n",
    "    # Find all unique tags in the text using regex\n",
    "    # This regex matches tags like <B>, <C1>, <Node123>, etc.\n",
    "    tags = set(re.findall(r'<([A-Za-z]+\\d*)>', text))\n",
    "\n",
    "    # Predefined color palette\n",
    "    color_palette = [\n",
    "        'lightyellow', 'lightblue', 'lightgreen', 'lightcoral',\n",
    "        'lightcyan', 'lightpink', 'lightsalmon', 'lightgray',\n",
    "        'lightgoldenrodyellow', 'lightseagreen', 'lightskyblue',\n",
    "        'lightsteelblue', 'lightsteelblue', 'lightsteelblue'\n",
    "    ]\n",
    "\n",
    "    # Dictionary to hold tag-color mapping\n",
    "    tag_color_mapping = {}\n",
    "\n",
    "    # Assign colors to tags, cycling through the color palette if necessary\n",
    "    for i, tag in enumerate(sorted(tags)):\n",
    "        color = color_palette[i % len(color_palette)]\n",
    "        tag_color_mapping[tag] = color\n",
    "\n",
    "    # Function to replace tags with styled spans including class names\n",
    "    def replace_tag(match):\n",
    "        tag = match.group(1)\n",
    "        content = match.group(2)\n",
    "        color = tag_color_mapping.get(tag, 'lightgray')  # Default color if not found\n",
    "        return f'<span class=\"{tag}\" style=\"background-color: {color}; font-weight: bold;\">{content}</span>'\n",
    "\n",
    "    # Regex to find tags and replace them with styled spans\n",
    "    # This regex matches <Tag>Content</Tag>\n",
    "    text = re.sub(r'<([A-Za-z]+\\d*)>(.*?)</\\1>', replace_tag, text, flags=re.DOTALL)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def parse_csv_file(file_path):\n",
    "    \"\"\"\n",
    "    Parses the input CSV file and extracts questions, answers, and their corresponding IDs.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the input CSV file.\n",
    "\n",
    "    Returns:\n",
    "        list of tuples: Each tuple contains (id, question, answer_text).\n",
    "    \"\"\"\n",
    "    qa_pairs = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            question = row.get('question', 'No question found.').strip()\n",
    "            answer_text = row.get('answer', 'No answer found.').strip()\n",
    "            id_ = row.get('id')\n",
    "            if id_ is not None:\n",
    "                try:\n",
    "                    id_int = int(id_)\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping a row due to invalid 'id' (not an integer): {id_}\")\n",
    "                    continue\n",
    "                qa_pairs.append((id_int, question, answer_text))\n",
    "            else:\n",
    "                # Handle cases without 'id' by skipping\n",
    "                print(f\"Skipping a row due to missing 'id': {row}\")\n",
    "    return qa_pairs\n",
    "\n",
    "\n",
    "def read_ground_truth(jsonl_path):\n",
    "    \"\"\"\n",
    "    Reads the ground truth answers from a JSONL file and maps them by ID.\n",
    "\n",
    "    Args:\n",
    "        jsonl_path (str): Path to the ground truth JSONL file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping each ID to its ground truth answer.\n",
    "    \"\"\"\n",
    "    ground_truth = {}\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            id_ = data.get('id')\n",
    "            answer = data.get('answer')\n",
    "            if id_ is not None and answer is not None:\n",
    "                # Extract the last number or text after '####'\n",
    "                match = re.search(r'####\\s*([\\d.]+)', answer)\n",
    "                if match:\n",
    "                    ground_truth[id_] = match.group(1).strip()\n",
    "                else:\n",
    "                    print(f\"No ground truth answer found for ID {id_}\")\n",
    "            else:\n",
    "                print(f\"Invalid ground truth entry: {data}\")\n",
    "    return ground_truth\n",
    "\n",
    "\n",
    "def create_highlight_html_new(qa_pairs, ground_truth):\n",
    "    \"\"\"\n",
    "    Creates HTML content with highlighted answer reasoning,\n",
    "    final answers colored based on correctness, and ground truth answers.\n",
    "\n",
    "    Args:\n",
    "        qa_pairs (list of tuples): Each tuple contains (id, question, answer_text).\n",
    "        ground_truth (dict): A dictionary mapping each ID to its ground truth answer.\n",
    "\n",
    "    Returns:\n",
    "        str: The complete HTML content as a string.\n",
    "    \"\"\"\n",
    "    html_content = \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <title>Question and Answer Highlights</title>\n",
    "        <style>\n",
    "            body {\n",
    "                font-family: Arial, sans-serif;\n",
    "                margin: 20px;\n",
    "                background-color: #f0f0f0;\n",
    "            }\n",
    "            .container {\n",
    "                background-color: #ffffff;\n",
    "                padding: 20px;\n",
    "                margin-bottom: 20px;\n",
    "                border-radius: 8px;\n",
    "                box-shadow: 0 2px 5px rgba(0,0,0,0.1);\n",
    "            }\n",
    "            .question {\n",
    "                font-size: 1.2em;\n",
    "                margin-bottom: 10px;\n",
    "            }\n",
    "            .answer-reasoning, .final-answer, .ground-truth-answer {\n",
    "                margin-bottom: 10px;\n",
    "            }\n",
    "            .final-answer {\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            .ground-truth-answer {\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            /* Styles for the highlighted spans */\n",
    "            .highlighted {\n",
    "                padding: 2px 4px;\n",
    "                border-radius: 3px;\n",
    "                display: inline-block;\n",
    "            }\n",
    "            /* Styles for the summary section */\n",
    "            .summary {\n",
    "                background-color: #e0ffe0;\n",
    "                padding: 15px;\n",
    "                border: 2px solid #00cc00;\n",
    "                border-radius: 8px;\n",
    "                font-size: 1.2em;\n",
    "                margin-bottom: 30px;\n",
    "            }\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "    <h1>Question and Answer Highlights</h1>\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize counters for correct and total answers\n",
    "    correct_answers = 0\n",
    "    total_answers = 0\n",
    "\n",
    "    # Placeholder for summary to be added at the top\n",
    "    summary_html = \"\"  # Will be updated after processing all QA pairs\n",
    "\n",
    "    # Temporary storage for all QA containers\n",
    "    qa_html_sections = \"\"\n",
    "\n",
    "    for i, (id_, question, answer_text) in enumerate(qa_pairs, 1):\n",
    "        try:\n",
    "            answer_reasoning, final_answer = extract_parts_regular_cot(answer_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot extract parts for question ID {id_}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Apply color to tags in answer_reasoning\n",
    "        highlighted_reasoning = add_color_to_tags_new(answer_reasoning)\n",
    "\n",
    "        # Retrieve ground truth answer\n",
    "        gt_answer = ground_truth.get(id_)\n",
    "        if gt_answer is None:\n",
    "            gt_answer_display = \"<span style='color: gray;'>Ground truth not available.</span>\"\n",
    "            is_correct = False\n",
    "        else:\n",
    "            gt_answer_display = gt_answer\n",
    "            # Compare final_answer with ground truth\n",
    "            is_correct = final_answer == gt_answer\n",
    "\n",
    "        # Style the final answer based on correctness\n",
    "        if is_correct:\n",
    "            highlighted_final_answer = f\"<span style='font-size:1.1em; color: green;'>{final_answer}</span>\"\n",
    "            correct_answers += 1\n",
    "        else:\n",
    "            highlighted_final_answer = f\"<span style='font-size:1.1em; color: red;'>{final_answer}</span>\"\n",
    "        total_answers += 1\n",
    "\n",
    "        # Display ground truth answer\n",
    "        if gt_answer is not None:\n",
    "            ground_truth_html = f\"<div class='ground-truth-answer'><strong>Ground Truth Answer:</strong> {gt_answer_display}</div>\"\n",
    "        else:\n",
    "            ground_truth_html = f\"<div class='ground-truth-answer'><strong>Ground Truth Answer:</strong> Not available.</div>\"\n",
    "\n",
    "        # Build the HTML structure for this QA pair\n",
    "        qa_html_sections += f\"<div class='container'>\"\n",
    "        qa_html_sections += f\"<div class='question'><strong>Question:</strong> {question}</div>\"\n",
    "        qa_html_sections += f\"<div class='answer-reasoning'><strong>Answer Reasoning:</strong> {highlighted_reasoning}</div>\"\n",
    "        qa_html_sections += f\"<div class='final-answer'><strong>Final Answer:</strong> {highlighted_final_answer}</div>\"\n",
    "        qa_html_sections += f\"{ground_truth_html}\"\n",
    "        qa_html_sections += \"</div>\\n\"\n",
    "\n",
    "    # After processing all QA pairs, create the summary\n",
    "    accuracy_percentage = (correct_answers / total_answers * 100) if total_answers > 0 else 0\n",
    "    summary_html = f\"\"\"\n",
    "    <div class='summary'>\n",
    "        <strong>Summary:</strong> Correct Answers: {correct_answers} / {total_answers} ({accuracy_percentage:.2f}%)\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "    # Append the summary at the top, right after the header\n",
    "    html_content += summary_html\n",
    "    html_content += qa_html_sections\n",
    "\n",
    "    # Close the HTML tags\n",
    "    html_content += \"\"\"\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    return html_content\n",
    "\n",
    "\n",
    "def main():\n",
    "    input_csv = '/Users/log/Github/textual_grounding/logan/results/GSM8K/llama/mermaid/cot_llama3.1_20240927_003000.csv'  # Replace with your input CSV file path\n",
    "    ground_truth_file = '/Users/log/Github/textual_grounding/data/GSM8K/test.jsonl'  # Path to the ground truth JSONL file\n",
    "    output_file = 'cot_llama3.1.html'  # Replace with your desired output HTML file path\n",
    "\n",
    "    # Check if input files exist\n",
    "    if not os.path.isfile(input_csv):\n",
    "        print(f\"Input CSV file not found: {input_csv}\")\n",
    "        return\n",
    "    if not os.path.isfile(ground_truth_file):\n",
    "        print(f\"Ground truth JSONL file not found: {ground_truth_file}\")\n",
    "        return\n",
    "\n",
    "    # Parse the input CSV file to extract IDs, questions, and answers\n",
    "    qa_pairs = parse_csv_file(input_csv)\n",
    "    print(f\"Total QA Pairs Parsed: {len(qa_pairs)}\")  # Debug: Print the number of QA pairs parsed\n",
    "\n",
    "    # Read the ground truth answers\n",
    "    ground_truth = read_ground_truth(ground_truth_file)\n",
    "    print(f\"Total Ground Truth Entries: {len(ground_truth)}\")  # Debug: Print the number of ground truth entries\n",
    "\n",
    "    # Check if any QA pairs were found\n",
    "    if not qa_pairs:\n",
    "        print(\"No question-answer pairs were found in the input file.\")\n",
    "        return\n",
    "\n",
    "    # Generate the HTML content\n",
    "    html_content = create_highlight_html_new(qa_pairs, ground_truth)\n",
    "\n",
    "    # Write the HTML content to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(html_content)\n",
    "\n",
    "    print(f\"HTML content has been successfully written to {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tin - visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML content has been successfully written to question_answer_highlights_prompt_fs_llama3.1.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "\n",
    "# Define the prompt_type and llm_model as needed\n",
    "prompt_type = \"fs\"  # Example value, set accordingly\n",
    "llm_model = \"llama3.1\"  # Example value, set accordingly\n",
    "\n",
    "save_html_path = f\"question_answer_highlights_prompt_{prompt_type}_{llm_model}.html\"\n",
    "# df_path = f'logan/results/{dataset}/llama/test_grounding_answer_prompt_{prompt_type}_{llm_model}.csv'\n",
    "df_path = '/Users/log/Github/textual_grounding/logan/results/GSM8K/llama/test_grounding_answer_prompt_fs_inst_llama3.1.csv'\n",
    "\n",
    "if prompt_type in [\"fs\", \"fs_inst\"]:\n",
    "    prefix = 'The answer is'\n",
    "elif prompt_type == \"zs\":\n",
    "    prefix = 'Final answer:'\n",
    "\n",
    "df = pd.read_csv(df_path)\n",
    "questions = df['question'].tolist()\n",
    "answers = df['answer'].tolist()\n",
    "\n",
    "def add_color_to_tags(text):\n",
    "    \"\"\"\n",
    "    Adds background color to specific tags within the text based on a predefined color mapping.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text containing tags like <a>, <b>, etc.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with added inline CSS for background colors.\n",
    "    \"\"\"\n",
    "    tag_color_mapping = {\n",
    "        'a': 'yellow',       # You can customize colors as needed\n",
    "        'b': 'lightblue',\n",
    "        'c': 'lightgreen',\n",
    "        'd': 'lightcoral',\n",
    "        'e': 'lightcyan',\n",
    "        'f': 'orange',       # Extend as needed\n",
    "        # Add more tags if necessary\n",
    "    }\n",
    "    # Iterate over the tag-color mappings\n",
    "    for tag, color in tag_color_mapping.items():\n",
    "        # Regex to find the tag and replace it with the same tag having a style attribute\n",
    "        text = re.sub(\n",
    "            f'<{tag}>(.*?)</{tag}>',\n",
    "            f'<span style=\"background-color: {color};\">{r\"\\1\"}</span>',\n",
    "            text,\n",
    "            flags=re.DOTALL\n",
    "        )\n",
    "    return text\n",
    "\n",
    "def parse_csv_file(file_path):\n",
    "    \"\"\"\n",
    "    Parses the input CSV file and extracts questions and their corresponding answers.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the input CSV file.\n",
    "\n",
    "    Returns:\n",
    "        list of tuples: Each tuple contains (question, answer_text).\n",
    "    \"\"\"\n",
    "    qa_pairs = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            question = row.get('question', 'No question found.').strip()\n",
    "            answer_text = row.get('answer', 'No answer found.').strip()\n",
    "            qa_pairs.append((question, answer_text))\n",
    "    return qa_pairs\n",
    "\n",
    "def create_highlight_html(qa_pairs):\n",
    "    \"\"\"\n",
    "    Creates HTML content with highlighted questions and answers based on tags.\n",
    "\n",
    "    Args:\n",
    "        qa_pairs (list of tuples): Each tuple contains (question, answer_text).\n",
    "\n",
    "    Returns:\n",
    "        str: The complete HTML content as a string.\n",
    "    \"\"\"\n",
    "    html_content = \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <title>Question and Answer Highlights</title>\n",
    "        <style>\n",
    "            body {\n",
    "                font-family: Arial, sans-serif;\n",
    "                margin: 20px;\n",
    "                background-color: #f0f0f0;\n",
    "            }\n",
    "            .container {\n",
    "                background-color: #ffffff;\n",
    "                padding: 20px;\n",
    "                margin-bottom: 20px;\n",
    "                border-radius: 8px;\n",
    "                box-shadow: 0 2px 5px rgba(0,0,0,0.1);\n",
    "            }\n",
    "            .question {\n",
    "                font-size: 1.2em;\n",
    "                margin-bottom: 10px;\n",
    "            }\n",
    "            .answer {\n",
    "                margin-bottom: 10px;\n",
    "            }\n",
    "            .highlight {\n",
    "                background-color: #FFFF00; /* Default highlight color */\n",
    "                font-weight: bold; /* Bold text for emphasis */\n",
    "            }\n",
    "            /* Additional styles for specific tags can be added here if needed */\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "    <h1>Question and Answer Highlights</h1>\n",
    "    \"\"\"\n",
    "    for i, (question, answer_text) in enumerate(qa_pairs, 1):\n",
    "        try:\n",
    "            # Apply color to tags in question and answer\n",
    "            highlighted_question = add_color_to_tags(question)\n",
    "            highlighted_answer = add_color_to_tags(answer_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot process question-answer pair {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Build the HTML structure\n",
    "        html_content += f\"<div class='container'>\"\n",
    "        html_content += f\"<div class='question'><strong>Question:</strong> {highlighted_question}</div>\"\n",
    "        html_content += f\"<div class='answer'><strong>Answer:</strong> {highlighted_answer}</div>\"\n",
    "        html_content += \"</div>\\n\"\n",
    "\n",
    "    # Close the HTML tags\n",
    "    html_content += \"\"\"\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    return html_content\n",
    "\n",
    "def main():\n",
    "    input_file = df_path  # Use the defined df_path\n",
    "    output_file = save_html_path  # Use the defined save_html_path\n",
    "\n",
    "    # Parse the input CSV file to extract questions and answers\n",
    "    qa_pairs = parse_csv_file(input_file)\n",
    "\n",
    "    # Check if any QA pairs were found\n",
    "    if not qa_pairs:\n",
    "        print(\"No question-answer pairs were found in the input file.\")\n",
    "        return\n",
    "\n",
    "    # Generate the HTML content\n",
    "    html_content = create_highlight_html(qa_pairs)\n",
    "\n",
    "    # Write the HTML content to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(html_content)\n",
    "\n",
    "    print(f\"HTML content has been successfully written to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total QA Pairs Parsed: 200\n",
      "No ground truth answer found for ID 489\n",
      "No ground truth answer found for ID 1113\n",
      "Total Ground Truth Entries: 1317\n",
      "\n",
      "===== Analysis Statistics =====\n",
      "\n",
      "Total Responses Analyzed: 200\n",
      "Responses with Final Answer in Curly Brackets: 136 (68.00%)\n",
      "Responses without Final Answer in Curly Brackets: 64 (32.00%)\n",
      "Responses with Ground Truth Available: 200 (100.00%)\n",
      "Correct Answers: 94\n",
      "Incorrect Answers: 106\n",
      "Accuracy: 47.00%\n",
      "Responses without Ground Truth: 0\n",
      "\n",
      "----- Tag Statistics -----\n",
      "Total Tags Found: 501\n",
      "Average Number of Tags per Response: 2.50\n",
      "Average Length of Tag Content: 8.68 characters\n",
      "--------------------------\n",
      "\n",
      "===== End of Statistics =====\n",
      "\n",
      "Statistics analysis completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import json  # For handling JSONL\n",
    "import os\n",
    "\n",
    "def extract_parts_regular_cot(answer_text):\n",
    "    \"\"\"\n",
    "    Processes the answer text to extract answer reasoning and the final answer.\n",
    "\n",
    "    Args:\n",
    "        answer_text (str): The full answer text containing answer reasoning and final answer.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (answer_reasoning, final_answer, has_curly)\n",
    "               where answer_reasoning is the full model response,\n",
    "               final_answer is the extracted answer,\n",
    "               and has_curly is a boolean indicating if the final answer was in curly brackets.\n",
    "    \"\"\"\n",
    "    # Attempt to extract Final Answer from 'Final Answer:'\n",
    "    final_match = re.search(r'Final Answer:\\s*(\\S+)', answer_text, re.IGNORECASE)\n",
    "    if final_match and final_match.group(1).strip():\n",
    "        final_answer = final_match.group(1).strip()\n",
    "        has_curly = False\n",
    "    else:\n",
    "        # Fallback: Extract Final Answer from '{...}' in the reasoning\n",
    "        curly_match = re.search(r'\\{([\\d.]+)\\}', answer_text)\n",
    "        final_answer = curly_match.group(1).strip() if curly_match else \"\"\n",
    "        has_curly = bool(curly_match)\n",
    "\n",
    "    return answer_text.strip(), final_answer, has_curly\n",
    "\n",
    "def parse_csv_file(file_path):\n",
    "    \"\"\"\n",
    "    Parses the input CSV file and extracts questions, answers, and their corresponding IDs.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the input CSV file.\n",
    "\n",
    "    Returns:\n",
    "        list of tuples: Each tuple contains (id, question, answer_text).\n",
    "    \"\"\"\n",
    "    qa_pairs = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            question = row.get('question', 'No question found.').strip()\n",
    "            answer_text = row.get('answer', 'No answer found.').strip()\n",
    "            id_ = row.get('id')\n",
    "            if id_ is not None:\n",
    "                try:\n",
    "                    id_int = int(id_)\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping a row due to invalid 'id' (not an integer): {id_}\")\n",
    "                    continue\n",
    "                qa_pairs.append((id_int, question, answer_text))\n",
    "            else:\n",
    "                # Handle cases without 'id' by skipping\n",
    "                print(f\"Skipping a row due to missing 'id': {row}\")\n",
    "    return qa_pairs\n",
    "\n",
    "def read_ground_truth(jsonl_path):\n",
    "    \"\"\"\n",
    "    Reads the ground truth answers from a JSONL file and maps them by ID.\n",
    "\n",
    "    Args:\n",
    "        jsonl_path (str): Path to the ground truth JSONL file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping each ID to its ground truth answer.\n",
    "    \"\"\"\n",
    "    ground_truth = {}\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            id_ = data.get('id')\n",
    "            answer = data.get('answer')\n",
    "            if id_ is not None and answer is not None:\n",
    "                # Extract the last number or text after '####'\n",
    "                match = re.search(r'####\\s*([\\d.]+)', answer)\n",
    "                if match:\n",
    "                    ground_truth[id_] = match.group(1).strip()\n",
    "                else:\n",
    "                    print(f\"No ground truth answer found for ID {id_}\")\n",
    "            else:\n",
    "                print(f\"Invalid ground truth entry: {data}\")\n",
    "    return ground_truth\n",
    "\n",
    "def create_statistics(qa_pairs, ground_truth):\n",
    "    \"\"\"\n",
    "    Creates and prints statistics based on the QA pairs and ground truth.\n",
    "\n",
    "    Args:\n",
    "        qa_pairs (list of tuples): Each tuple contains (id, question, answer_text).\n",
    "        ground_truth (dict): A dictionary mapping each ID to its ground truth answer.\n",
    "    \"\"\"\n",
    "    total_responses = len(qa_pairs)\n",
    "    responses_with_curly = 0\n",
    "    responses_without_curly = 0\n",
    "    correct_answers = 0\n",
    "    incorrect_answers = 0\n",
    "    no_ground_truth = 0\n",
    "\n",
    "    # Variables for tag statistics\n",
    "    total_tags = 0\n",
    "    total_tag_length = 0\n",
    "    tag_counts = []  # List to store number of tags per response\n",
    "    tag_lengths = []  # List to store lengths of tag content across all responses\n",
    "\n",
    "    for id_, question, answer_text in qa_pairs:\n",
    "        try:\n",
    "            answer_reasoning, final_answer, has_curly = extract_parts_regular_cot(answer_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot extract parts for question ID {id_}: {e}\")\n",
    "            continue\n",
    "\n",
    "        if has_curly:\n",
    "            responses_with_curly += 1\n",
    "        else:\n",
    "            responses_without_curly += 1\n",
    "\n",
    "        # Extract tags and their content\n",
    "        tags_in_response = re.findall(r'<([A-Za-z]+\\d*)>(.*?)</\\1>', answer_text)\n",
    "        number_of_tags = len(tags_in_response)\n",
    "        tag_counts.append(number_of_tags)\n",
    "        total_tags += number_of_tags\n",
    "\n",
    "        for tag, content in tags_in_response:\n",
    "            content_length = len(content)\n",
    "            tag_lengths.append(content_length)\n",
    "            total_tag_length += content_length\n",
    "\n",
    "        # Retrieve ground truth answer\n",
    "        gt_answer = ground_truth.get(id_)\n",
    "        if gt_answer is None:\n",
    "            no_ground_truth += 1\n",
    "            continue\n",
    "\n",
    "        # Compare final_answer with ground truth\n",
    "        if final_answer == gt_answer:\n",
    "            correct_answers += 1\n",
    "        else:\n",
    "            incorrect_answers += 1\n",
    "\n",
    "    # Calculate additional metrics\n",
    "    accuracy_percentage = (correct_answers / (correct_answers + incorrect_answers) * 100) if (correct_answers + incorrect_answers) > 0 else 0\n",
    "    curly_percentage = (responses_with_curly / total_responses * 100) if total_responses > 0 else 0\n",
    "    no_curly_percentage = (responses_without_curly / total_responses * 100) if total_responses > 0 else 0\n",
    "    ground_truth_available = total_responses - no_ground_truth\n",
    "    ground_truth_available_percentage = (ground_truth_available / total_responses * 100) if total_responses > 0 else 0\n",
    "\n",
    "    # Calculate tag statistics\n",
    "    average_tags_per_response = (total_tags / total_responses) if total_responses > 0 else 0\n",
    "    average_tag_length = (total_tag_length / total_tags) if total_tags > 0 else 0\n",
    "\n",
    "    # Print the statistics\n",
    "    print(\"\\n===== Analysis Statistics =====\\n\")\n",
    "    print(f\"Total Responses Analyzed: {total_responses}\")\n",
    "    print(f\"Responses with Final Answer in Curly Brackets: {responses_with_curly} ({curly_percentage:.2f}%)\")\n",
    "    print(f\"Responses without Final Answer in Curly Brackets: {responses_without_curly} ({no_curly_percentage:.2f}%)\")\n",
    "    print(f\"Responses with Ground Truth Available: {ground_truth_available} ({ground_truth_available_percentage:.2f}%)\")\n",
    "    print(f\"Correct Answers: {correct_answers}\")\n",
    "    print(f\"Incorrect Answers: {incorrect_answers}\")\n",
    "    print(f\"Accuracy: {accuracy_percentage:.2f}%\")\n",
    "    print(f\"Responses without Ground Truth: {no_ground_truth}\")\n",
    "\n",
    "    # Tag Statistics\n",
    "    print(\"\\n----- Tag Statistics -----\")\n",
    "    print(f\"Total Tags Found: {total_tags}\")\n",
    "    print(f\"Average Number of Tags per Response: {average_tags_per_response:.2f}\")\n",
    "    print(f\"Average Length of Tag Content: {average_tag_length:.2f} characters\")\n",
    "    print(\"--------------------------\\n\")\n",
    "    print(\"===== End of Statistics =====\\n\")\n",
    "\n",
    "def main():\n",
    "    input_csv = '/Users/log/Github/textual_grounding/logan/results/GSM8K/llama/mermaid/mermaid_get_answer_llama3.1_20240926_215344.csv'  # Replace with your input CSV file path\n",
    "    ground_truth_file = '/Users/log/Github/textual_grounding/data/GSM8K/test.jsonl'  # Path to the ground truth JSONL file\n",
    "\n",
    "    # Check if input files exist\n",
    "    if not os.path.isfile(input_csv):\n",
    "        print(f\"Input CSV file not found: {input_csv}\")\n",
    "        return\n",
    "    if not os.path.isfile(ground_truth_file):\n",
    "        print(f\"Ground truth JSONL file not found: {ground_truth_file}\")\n",
    "        return\n",
    "\n",
    "    # Parse the input CSV file to extract IDs, questions, and answers\n",
    "    qa_pairs = parse_csv_file(input_csv)\n",
    "    print(f\"Total QA Pairs Parsed: {len(qa_pairs)}\")  # Debug: Print the number of QA pairs parsed\n",
    "\n",
    "    # Read the ground truth answers\n",
    "    ground_truth = read_ground_truth(ground_truth_file)\n",
    "    print(f\"Total Ground Truth Entries: {len(ground_truth)}\")  # Debug: Print the number of ground truth entries\n",
    "\n",
    "    # Check if any QA pairs were found\n",
    "    if not qa_pairs:\n",
    "        print(\"No question-answer pairs were found in the input file.\")\n",
    "        return\n",
    "\n",
    "    # Generate and print the statistics\n",
    "    create_statistics(qa_pairs, ground_truth)\n",
    "\n",
    "    print(\"Statistics analysis completed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 'prompt' is a list in entry ID 110. Attempting to join into a string.\n",
      "Warning: 'prompt' is a list in entry ID 111. Attempting to join into a string.\n",
      "Warning: 'prompt' is a list in entry ID 112. Attempting to join into a string.\n",
      "Warning: 'prompt' is a list in entry ID 113. Attempting to join into a string.\n",
      "JSON file has been updated successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Define the path to your JSON file\n",
    "input_file = '/Users/log/Github/textual_grounding/data/AIW/test.json'\n",
    "\n",
    "# Load the JSON data from the file\n",
    "with open(input_file, 'r', encoding='utf-8') as file:\n",
    "    try:\n",
    "        data = json.load(file)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "# Process each entry in the JSON data\n",
    "for idx, entry in enumerate(data):\n",
    "    # Get the current prompt\n",
    "    prompt = entry.get('prompt', '')\n",
    "\n",
    "    # Check if prompt is a string\n",
    "    if isinstance(prompt, str):\n",
    "        delimiter = 'have?'\n",
    "        index = prompt.find(delimiter)\n",
    "\n",
    "        if index != -1:\n",
    "            # Truncate the prompt after \"have?\"\n",
    "            truncated_prompt = prompt[:index + len(delimiter)]\n",
    "            entry['prompt'] = truncated_prompt\n",
    "        else:\n",
    "            print(f\"Warning: 'have?' not found in prompt of entry ID {entry.get('id', 'Unknown')}. Prompt left unchanged.\")\n",
    "    elif isinstance(prompt, list):\n",
    "        print(f\"Warning: 'prompt' is a list in entry ID {entry.get('id', 'Unknown')}. Attempting to join into a string.\")\n",
    "        # Attempt to join the list into a single string\n",
    "        joined_prompt = ' '.join(str(item) for item in prompt)\n",
    "        delimiter = 'have?'\n",
    "        index = joined_prompt.find(delimiter)\n",
    "\n",
    "        if index != -1:\n",
    "            truncated_prompt = joined_prompt[:index + len(delimiter)]\n",
    "            entry['prompt'] = truncated_prompt\n",
    "        else:\n",
    "            print(f\"Warning: 'have?' not found after joining prompt in entry ID {entry.get('id', 'Unknown')}. Prompt left unchanged.\")\n",
    "    else:\n",
    "        print(f\"Warning: 'prompt' is neither a string nor a list in entry ID {entry.get('id', 'Unknown')}. Prompt left unchanged.\")\n",
    "\n",
    "    # Rename 'right_answer' to 'answer' if it exists\n",
    "    if 'right_answer' in entry:\n",
    "        entry['answer'] = entry.pop('right_answer')\n",
    "\n",
    "# Save the updated data back to the same JSON file\n",
    "with open(input_file, 'w', encoding='utf-8') as file:\n",
    "    json.dump(data, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"JSON file has been updated successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
