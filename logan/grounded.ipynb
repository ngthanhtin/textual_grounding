{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import anthropic\n",
    "import ollama\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from google.generativeai.types import RequestOptions\n",
    "from google.api_core import retry\n",
    "from typing import List, Tuple\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import datetime\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, TimeoutError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mega_prompt = \"\"\"\n",
    "You are a helpful assistant tasked with tagging key facts in the text and then using those facts to answer the final question. Your goal is to analyze the input question, identify distinct key points needed to answer the question, and then wrap each of these points in custom HTML-like tags like <fact1>, <fact2>, <fact3>, etc and then use these tags to answer the question. After you have tagged the question in tags, use these tags in your reasoning process to answer the question. The <fact> tags should be interweaved in the sentences in your reasoning.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "1. **Read and Understand the Input Question**:\n",
    "   - Carefully analyze the input question to understand its context.\n",
    "   - Identify distinct facts, entities, or concepts that contribute to the meaning of the question.\n",
    "\n",
    "2. **Identify Key Points (Facts)**:\n",
    "   - Each distinct and meaningful segment of the question that provides important information should be considered a \"fact.\"\n",
    "   - This can include the subject, object, context, or any qualifiers that make the question specific.\n",
    "\n",
    "3. **Tag Each Fact**:\n",
    "   - Assign a unique tag to each fact in the form <fact1>, <fact2>, etc.\n",
    "   - Wrap each identified key point in these tags.\n",
    "   - The tags should start from <fact1> for the first key point, and increment for each new fact identified.\n",
    "\n",
    "4. **Formatting Requirements**:\n",
    "   - Maintain the original structure of the question as much as possible.\n",
    "   - Make sure each tag encapsulates the entire key point clearly without splitting phrases unnecessarily.\n",
    "\n",
    "### Example\n",
    "\n",
    "#### Input Question:\n",
    "\"Does climate change positively affect polar bear populations in the Arctic? Anwser Options: Yes, No, Depends\"\n",
    "\n",
    "#### Step-by-Step Identification:\n",
    "1. Fact 1: \"Climate change\" (main topic causing the effect)\n",
    "2. Fact 2: \"Polar bear populations\" (who is affected)\n",
    "3. Fact 3: \"In the Arctic\" (the location/context)\n",
    "\n",
    "#### Reformatted Output:\n",
    "\"How does <fact1>climate change</fact1> affect <fact2>polar bear populations</fact2> in <fact3>the Arctic</fact3>?\"\n",
    "\n",
    "#### Answer Reasoning\n",
    "\"<fact1>Climate change</fact1> primarily affects <fact2>polar bears</fact2> through the loss of <fact3>sea ice</fact3>, which is crucial for their hunting and survival. As <fact3>Arctic ice</fact3> melts earlier and forms later due to <fact1>rising temperatures</fact1>, <fact2>polar bears</fact2> face reduced access to their main prey (seals), leading to nutritional stress, longer fasting periods, and <fact2>overall population decline</fact2>. The impact of <fact1>climate change</fact1> on <fact2>polar bear populations</fact2> is therefore negative, so the answer is {No}\"\n",
    "\n",
    "### Final Output Format\n",
    "\n",
    "Ensure that the final output is:\n",
    "- Grammatically correct.\n",
    "- Properly formatted with each key point enclosed in <fact> tags.\n",
    "- Consistent with the original meaning of the input question.\n",
    "- Final answer is enclosed in curly braces e.g. {answer}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_zero_prompt = \"\"\"\n",
    "# General Instructions\n",
    "You are a helpful assistant tasked with tagging key facts in the text and then using those facts to answer the final question. Your first goal is to analyze the input question and identify distinct key points needed to answer the question and then wrap each of these points in custom HTML-like tags like <fact1>, <fact2>, <fact3>. After you have tagged the reformatted question in tags, use these tags in your reasoning process to answer the question. The <tags> should be interweaved in the sentences in your reasoning. You are only concerned with highlighting facts that are essential to answering the question. You should not tag irrelevant information. Once you have determined the answer to the question, you should put the concise version of your final answer in curly braces. For example, {3} or {True} or {A}.\n",
    "\n",
    "Your response should follow this format:\n",
    "\n",
    "Reformatted Question:\n",
    ".... (reformatted question with tags)\n",
    "\n",
    "Answer Reasoning:\n",
    ".... (reasoning with tags)\n",
    "\n",
    "Final Answer: {answer}\n",
    "\n",
    "# Tagging Information\n",
    "Each tag should be named according to the type of information it represents. There are multiple distinct types of tags:\n",
    "\n",
    "- <person> for people or characters\n",
    "- <location> for places or regions\n",
    "- <number> for numerical values\n",
    "- <time> for time-related information\n",
    "- <concept> for abstract ideas\n",
    "- <object> for physical things\n",
    "\n",
    "\n",
    "## Tagging Examples \n",
    "\n",
    "### Person Example\n",
    "For the text \"Adam worked as a volunteer firefighter in order to help serve his community.\", you would tag the text as follows: \"<person>Adam worked as a volunteer firefighter</person> in order to help serve his community.\"\n",
    "\n",
    "The text in the <person> tag should include key information relevant to that person. While acceptable, it does not always need to be just their name. \n",
    "\n",
    "### Location Example\n",
    "For the text \"What is the capital of France?\", you would tag the question as follows: \"What is the <location>capital of France</location>?\"\n",
    "\n",
    "### Number Example\n",
    "For the text \"The average temperature in the desert is 110 degrees Fahrenheit.\", you would tag the text as follows: \"The average temperature in the desert is <number>110 degrees Fahrenheit</number>.\"\n",
    "\n",
    "The text in the <number> tag should include any information relevant to a quantity of something. \n",
    "\n",
    "### Time Example\n",
    "For the text \"The earliest recorded human writing was composed nearly 4,000 years ago, sometime around 2000 B.C.\" would be tagged as \"The earliest recorded human writing was composed nearly <time1>4,000 years</time1> ago, sometime around <time2>2000 B.C.</time2>\"\n",
    "\n",
    "### Concept Example\n",
    "For the text \"The field of machine learning is a rapidly growing area of study.\", you would tag the text as follows: \"The field of <concept>machine learning</concept> is a rapidly growing area of study.\"\n",
    "\n",
    "### Object Example\n",
    "For the text \"The astronaut used a special tool to repair the damaged satellite.\", you would tag the text as follows: \"The astronaut used a <object1>special tool</object2> to repair the <object2>damaged satellite</object2>.\"\n",
    "\n",
    "## What NOT to Tag\n",
    "Given an inputted question, there are large amounts of possible tags that could be used. However, not all of these tags are relevant to the final question. You should only tag information that is ESSENTIAL to answering the question. Here is an example of a question that has been over-tagged:\n",
    "\n",
    "Original Question:\n",
    "Kaden is a computer science major at NYU where he studies subjects such as physics, math, and the principles of programming languages. He has a pet dog named Max who is 3 years old. Kaden has a part-time job at the local grocery store where he works every Saturday and Sunday. What major ocean does Kaden live near?\n",
    "\n",
    "Reformatted Question:\n",
    "\"<person1>Kaden</person1> is a <concept1>computer science major</concept1> at <location1>NYU</location1> where he studies subjects such as <concept2>physics</concept2>, <concept3>math</concept3>, and the <concept4>principles of programming languages</concept4>. <person2>He has a pet dog</person2> named <person3>Max</person3> who is <time1>3 years old</time1>. <person4>Kaden has a part-time job</person4> at the <location2>local grocery store</location2> where he works every <time2>Saturday</time2> and <time3>Sunday</time3>. What <object1>major ocean</object1> does <person1>Kaden</person1> live near?\"\n",
    "\n",
    "While this question does have valid tags, almost all of them are not relevant to the question. Here is an exmaple of the same question with only the relevant tags:\n",
    "\n",
    "Reformatted Question:\n",
    "\"<person1>Kaden</person1> is a computer science major at <location1>NYU</location1> where he studies subjects such as physics, math, and the principles of programming languages. He has a pet dog named Max who is 3 years old. Kaden has a part-time job at the local grocery store where he works every Saturday and Sunday. What <object1>major ocean</object1> does <person1>Kaden</person1> live near?\"\n",
    "\n",
    "Here is an example answer for the properly formatted question:\n",
    "\"Since <person1>Kaden</person1> attends <location1>NYU</location1>, he lives in New York City. Given that New York City is on the east coast of the United States, the nearest major ocean that <person1>Kaden</person1> lives near is the Atlantic Ocean. \n",
    "Final answer: {Atlantic Ocean}\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_facts = \"\"\"\n",
    "# General Instructions\n",
    "You are a helpful assistant tasked with extracting key facts from the text. Your goal is to analyze the input question and identify distinct sections of the text needed to answer the question. You are only concerned with highlighting facts that are ESSENTIAL to answering the question. You should ignore irrelevant information that does not help answer the final question. \n",
    "\n",
    "Reasoning Process:\n",
    "....\n",
    "\n",
    "Key Facts:\n",
    "....\n",
    "\n",
    "# Fact Extraction Details\n",
    "\n",
    "## Properly Done Example\n",
    "Here is an example of properly extracting the information needed to answer the final question:\n",
    "\n",
    "Original Question:\n",
    "Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?\n",
    "\n",
    "Reasoning Process:\n",
    "Identifying the important information in this problem centers on determining what leads to finding the difference between the wallet's cost and Betty's total available money.\n",
    "The wallet's cost of $100 serves as the target amount, establishing the baseline for all calculations. Understanding Betty's available money requires summing all her financial sources. Her initial savings, described as half the needed amount, translates to $50. The parents' contribution of $15 directly adds to this sum. The grandparents' contribution, defined as twice the parents' amount, equals $30 and represents another essential component.\n",
    "Each numerical value and relationship presented in the problem contributes directly to calculating Betty's total available funds. The absence of any single piece - the wallet cost, initial savings, parental contribution, or the relationship between parental and grandparental gifts - would make it impossible to determine the remaining amount needed.\n",
    "\n",
    "Key Facts:\n",
    "- The wallet costs $100\n",
    "- Betty has half of the money needed\n",
    "- Bett's parents gave her $15\n",
    "- Betty's Grandparents gave twice as much as parents\n",
    "\n",
    "## What NOT to Extract\n",
    "Given an inputted question, there are large amounts of possible facts that could be used. However, not all of these citations are relevant to the final question. You should only extract information that is ESSENTIAL to answering the question. Here is an example of a question that has too many extracted facts:\n",
    "\n",
    "Original Question:\n",
    "Kaden is a computer science major at NYU where he studies subjects such as physics, math, and the principles of programming languages. He has a pet dog named Max who is 3 years old. Kaden has a part-time job at the local grocery store where he works every Saturday and Sunday. What major ocean does Kaden live near?\n",
    "\n",
    "Reasoning Process:\n",
    "....\n",
    "\n",
    "Key Facts:\n",
    "- Kaden is a computer science major\n",
    "- Kaden attends NYU\n",
    "- Kaden studies physics, math and programming languages\n",
    "- Kaden has a dog\n",
    "- Max, the dog, is 3 years old\n",
    "- Kaden has a part time job\n",
    "- Kaden works at a grocery store\n",
    "- Kaden works every Saturday and Sunday\n",
    "\n",
    "While these are all valid pieces of information, almost all of them are not relevant to the question. Here is what the key facts section should actually look like:\n",
    "\n",
    "Key Facts:\n",
    "- Kaden attends NYU\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_facts_reasoning = \"\"\"\n",
    "# General Instructions\n",
    "You are a helpful assistant tasked with extracting key facts from the text. Your goal is to analyze the input question and identify distinct sections of the text needed to answer the question. You are only concerned with identifying quotes that are ESSENTIAL to answering the question. You should ignore irrelevant information that does not help answer the final question. You should extract the exact parts of the text, not summarized versions of the text. After you have decided what specific quotes to use, tag the original question with xml tags around those quotes. Here is what your response should look like:\n",
    "\n",
    "### Fact Extraction Reasoning:\n",
    "....\n",
    "\n",
    "### Key Facts:\n",
    "....\n",
    "\n",
    "### Reformatted Question:\n",
    "...\n",
    "\n",
    "# Fact Extraction Details\n",
    "Each block of text has a large amount of information that could be a valid fact. However, you should only extract the most important quotes. Try to keep each quote as short as possible while still maintaining the essential information.\n",
    "\n",
    "## Properly Done Example\n",
    "Here is an example of properly extracting the information needed to answer the final question:\n",
    "\n",
    "### Original Question:\n",
    "Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?\n",
    "\n",
    "### Fact Extraction Reasoning:\n",
    "Identifying the important information in this problem centers on determining what leads to finding the difference between the wallet's cost and Betty's total available money.\n",
    "The wallet's cost of $100 serves as the target amount, establishing the baseline for all calculations. Understanding Betty's available money requires summing all her financial sources. Her initial savings, described as half the needed amount, translates to $50. The parents' contribution of $15 directly adds to this sum. The grandparents' contribution, defined as twice the parents' amount, equals $30 and represents another essential component.\n",
    "Each numerical value and relationship presented in the problem contributes directly to calculating Betty's total available funds. The absence of any single piece - the wallet cost, initial savings, parental contribution, or the relationship between parental and grandparental gifts - would make it impossible to determine the remaining amount needed.\n",
    "\n",
    "### Key Facts:\n",
    "<fact1>wallet which costs $100</fact1>\n",
    "<fact2>Betty has only half of the money</fact2>\n",
    "<fact3>parents decided to give her $15</fact3>\n",
    "<fact4>grandparents twice as much as her parents</fact4>\n",
    "\n",
    "### Reformatted Question:\n",
    "Betty is saving money for a new <fact1>wallet which costs $100</fact1>. <fact2>Betty has only half of the money</fact2> she needs. Her <fact3>parents decided to give her $15</fact3> for that purpose, and her <fact4>grandparents twice as much as her parents</fact4>. How much more money does Betty need to buy the wallet?\n",
    "\n",
    "## What NOT to Extract\n",
    "Given an inputted question, there are large amounts of possible facts that could be used. However, not all of these citations are relevant to the final question. You should only extract information that is essential to answering the question. Here is an example of a question that has too many extracted facts:\n",
    "\n",
    "### Original Question:\n",
    "Kaden is a computer science major at NYU where he studies subjects such as physics, math, and the principles of programming languages. He has a pet dog named Max who is 3 years old. Kaden has a part-time job at the local grocery store where he works every Saturday and Sunday. What major ocean does Kaden live near?\n",
    "\n",
    "### Fact Extraction Reasoning:\n",
    "....\n",
    "\n",
    "### Key Facts:\n",
    "<fact1>Kaden is a computer science major</fact1>\n",
    "<fact2>NYU</fact2>\n",
    "<fact3>he studies subjects such as physics, math, and the principles of programming languages</fact3>\n",
    "<fact4>pet dog named Max</fact4>\n",
    "<fact5>Max, the dog, is 3 years old</fact5>\n",
    "<fact6>Kaden has a part-time job</fact6>\n",
    "<fact7>local grocery store</fact7>\n",
    "<fact8>he works every Saturday and Sunday</fact8>\n",
    "\n",
    "### Reformatted Question:\n",
    "...\n",
    "\n",
    "While these are all valid pieces of information, almost none of them are relevant to the question. We only want information that is relevant to the final question, which in this case is \"What major ocean does Kaden live near?\" This is what the key facts section and the associated reformatted question should actually look like:\n",
    "\n",
    "### Key Facts:\n",
    "<fact1>Kaden is a computer science major at NYU</fact1>\n",
    "\n",
    "### Reformatted Question \n",
    "<fact1>Kaden is a computer science major at NYU</fact1> where he studies subjects such as physics, math, and the principles of programming languages. He has a pet dog named Max who is 3 years old. Kaden has a part-time job at the local grocery store where he works every Saturday and Sunday. What major ocean does Kaden live near?\n",
    "### Tagg An\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_facts_only = \"\"\"\n",
    "# General Instructions\n",
    "You are a helpful assistant tasked with extracting key facts from the text. Your goal is to analyze the input question and identify distinct sections of the text needed to answer the question. You are only concerned with identifying quotes that are ESSENTIAL to answering the question. You should ignore irrelevant information that does not help answer the final question. You should extract the exact parts of the text, not summarized versions of the text. Try to keep the length of your key facts as short as possible without losing important context. Specific numbers, locations, details, etc tend to be good facts to extract. After you have decided what specific quotes to use from the text, tag the original question with xml tags around those quotes. If the question includes multiple choice questions, make sure to include those in the formatted question. Here is what your response should look like:\n",
    "\n",
    "### Fact Extraction Reasoning:\n",
    "....\n",
    "\n",
    "### Key Facts:\n",
    "....\n",
    "\n",
    "### Reformatted Question:\n",
    "...\n",
    "\n",
    "# Fact Extraction Details\n",
    "Each block of text has a large amount of information that could be a valid fact. However, you should only extract the most important quotes. Try to keep each quote as short as possible while still maintaining the essential information.\n",
    "\n",
    "# Properly Done Examples\n",
    "Here are examples of properly extracting the information needed to answer the final question:\n",
    "\n",
    "## Example 1\n",
    "### Original Question:\n",
    "The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph. In a golf tournament, there were seven golfers: Eve, Rob, Dan, Mel, Ana, Eli, and Ada. Ada finished above Rob. Eve finished below Rob. Mel finished above Eli. Ada finished below Dan. Ana finished third. Eli finished second. Options: (A) Eve finished first (B) Rob finished first (C) Dan finished first (D) Mel finished first (E) Ana finished first (F) Eli finished first (G) Ada finished first\n",
    "\n",
    "### Fact Extraction Reasoning:\n",
    "Given that all the answer options contain information about all seven golfers and there is no immediate obvious answer based off the given answer options, we need to identify the relative positions of each of the golfers. The key facts in the statement are the positions of Eve, Rob, Dan, Mel, Ana, Eli, and Ada.\n",
    "\n",
    "### Key Facts:\n",
    "<fact1>Ada finished above Rob</fact1>\n",
    "<fact2>Eve finished below Rob</fact2>\n",
    "<fact3>Mel finished above Eli</fact3>\n",
    "<fact4>Ada finished below Dan</fact4>\n",
    "<fact5>Ana finished third</fact5>\n",
    "<fact6>Eli finished second</fact6>\n",
    "\n",
    "### Reformatted Question:\n",
    "The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph. In a golf tournament, there were seven golfers: Eve, Rob, Dan, Mel, Ana, Eli, and Ada. <fact1>Ada finished above Rob</fact1>. <fact2>Eve finished below Rob</fact2>. <fact3>Mel finished above Eli</fact3>. <fact4>Ada finished below Dan</fact4>. <fact5>Ana finished third</fact5>. <fact6>Eli finished second</fact6>. Options: (A) Eve finished first (B) Rob finished first (C) Dan finished first (D) Mel finished first (E) Ana finished first (F) Eli finished first (G) Ada finished first\n",
    "\n",
    "## Example 2\n",
    "### Original Question:\n",
    "Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?\n",
    "A) $10\n",
    "B) $15\n",
    "C) $20\n",
    "D) $25\n",
    "\n",
    "### Fact Extraction Reasoning:\n",
    "Identifying the important information in this problem centers on determining what leads to finding the difference between the wallet's cost and Betty's total available money.\n",
    "The wallet's cost of $100 serves as the target amount, establishing the baseline for all calculations. Understanding Betty's available money requires summing all her financial sources. Her initial savings, described as half the needed amount, translates to $50. The parents' contribution of $15 directly adds to this sum. The grandparents' contribution, defined as twice the parents' amount, equals $30 and represents another essential component.\n",
    "Each numerical value and relationship presented in the problem contributes directly to calculating Betty's total available funds. The absence of any single piece - the wallet cost, initial savings, parental contribution, or the relationship between parental and grandparental gifts - would make it impossible to determine the remaining amount needed.\n",
    "\n",
    "### Key Facts:\n",
    "<fact1>wallet which costs $100</fact1>\n",
    "<fact2>Betty has only half of the money</fact2>\n",
    "<fact3>parents decided to give her $15</fact3>\n",
    "<fact4>grandparents twice as much as her parents</fact4>\n",
    "\n",
    "### Reformatted Question:\n",
    "Betty is saving money for a new <fact1>wallet which costs $100</fact1>. <fact2>Betty has only half of the money</fact2> she needs. Her <fact3>parents decided to give her $15</fact3> for that purpose, and her <fact4>grandparents twice as much as her parents</fact4>. How much more money does Betty need to buy the wallet?\n",
    "A) $10\n",
    "B) $15\n",
    "C) $20\n",
    "D) $25\n",
    "\n",
    "## Example 3\n",
    "### Original Question:\n",
    "Are Doctors of Homeopathy more likely than Doctors of Internal Medicine to recommend Quartz as a treatment?\n",
    "\n",
    "### Fact Extraction Reasoning:\n",
    "The key facts in this question are the types of doctors being compared and the treatment they are likely to recommend. The specific types of doctors are Doctors of Homeopathy and Doctors of Internal Medicine. The treatment being considered is Quartz. If any of these facts were missing, it would be impossible to answer the question.\n",
    "\n",
    "### Key Facts:\n",
    "<fact1>Doctors of Homeopathy</fact1>\n",
    "<fact2>Doctors of Internal Medicine</fact2>\n",
    "<fact3>Quartz</fact3>\n",
    "\n",
    "### Reformatted Question: \n",
    "Are <fact1>Doctors of Homeopathy</fact1> more likely than <fact2>Doctors of Internal Medicine</fact2> to recommend <fact3>Quartz</fact3> as a treatment?\n",
    "\n",
    "## Example 4\n",
    "### Original Question:\n",
    "Alice and Bob are sister and brother. Alice has 4 sisters and Bob has 1 brother. How many sisters does Bob have?\n",
    "\n",
    "### Fact Extraction Reasoning:\n",
    "In this question, it's given that Alice and Bob are sister and brother and that Alice has 4 sisters. The question asks for the number of sisters Bob has, which is the missing piece of information needed to answer the question. The key facts in this question are the relationships between Alice and Bob and the number of sisters between them. Given that the question does not ask about the number of brothers Bob has, this information is not relevant to the final answer and does not need to be tagged.\n",
    "\n",
    "### Key Facts:\n",
    "<fact1>Alice and Bob are sister and brother</fact1>\n",
    "<fact2>Alice has 4 sisters</fact2>\n",
    "\n",
    "### Reformatted Question:\n",
    "<fact1>Alice and Bob are sister and brother</fact1>. <fact2>Alice has 4 sisters</fact2> and Bob has 1 brother. How many sisters does Bob have?\n",
    "\n",
    "## Example 5\n",
    "### Original Question:\n",
    "Are both Kurram Garhi and Trojkrsti located in the same country?\n",
    "\n",
    "### Fact Extraction Reasoning:\n",
    "The question asks about the location of two places, Kurram Garhi and Trojkrsti, and whether they are in the same country. The key facts in this question are the names of the two places. Without this information, it would be impossible to determine if they are in the same country.\n",
    "\n",
    "### Key Facts:\n",
    "<fact1>Kurram Garhi</fact1>\n",
    "<fact2>Trojkrsti</fact2>\n",
    "\n",
    "### Reformatted Question:\n",
    "Are both <fact1>Kurram Garhi</fact1> and <fact2>Trojkrsti</fact2> located in the same country?\n",
    "\n",
    "## Example 6\n",
    "### Original Question:\n",
    "Two friends plan to walk along a 43-km trail, starting at opposite ends of the trail at the same time. If Friend P's rate is 15% faster than Friend Q's, how many kilometers will Friend P have walked when they pass each other? A)21, B)21.5, C)22, D)22.5, E)23\n",
    "\n",
    "### Fact Extraction Reasoning:\n",
    "The key facts in this question are the length of the trail, the starting positions of the friends and the relative speeds of the friends. Without any of these pieces of information, it would be impossible to solve the problem.\n",
    "\n",
    "### Key Facts:\n",
    "<fact1>43-km trail</fact1>\n",
    "<fact2>starting at opposite ends of the trail at the same time</fact2>\n",
    "<fact3>P's rate is 15% faster than Friend Q's</fact3>\n",
    "\n",
    "### Reformatted Question:\n",
    "Two friends plan to walk along a <fact1>43-km trail</fact1>, <fact2>starting at opposite ends of the trail at the same time</fact2>. If Friend P's rate is <fact3>15% faster than Friend Q's</fact3>, how many kilometers will Friend P have walked when they pass each other? A)21, B)21.5, C)22, D)22.5, E)23\n",
    "\n",
    "# What NOT to Extract\n",
    "Given an inputted question, there are large amounts of possible facts that could be used. However, not all of these citations are relevant to the final question. You should only extract information that is essential to answering the question. Here is an example of a question that has too many extracted facts:\n",
    "\n",
    "## Negative Example\n",
    "### Original Question:\n",
    "Kaden is a computer science major at NYU where he studies subjects such as physics, math, and the principles of programming languages. He has a pet dog named Max who is 3 years old. Kaden has a part-time job at the local grocery store where he works every Saturday and Sunday. What major ocean does Kaden live near?\n",
    "\n",
    "### Fact Extraction Reasoning:\n",
    "....\n",
    "\n",
    "### Key Facts:\n",
    "<fact1>Kaden is a computer science major</fact1>\n",
    "<fact2>NYU</fact2>\n",
    "<fact3>he studies subjects such as physics, math, and the principles of programming languages</fact3>\n",
    "<fact4>pet dog named Max</fact4>\n",
    "<fact5>3 years old</fact5>\n",
    "<fact6>Kaden has a part-time job</fact6>\n",
    "<fact7>local grocery store</fact7>\n",
    "<fact8>he works every Saturday and Sunday</fact8>\n",
    "\n",
    "### Reformatted Question:\n",
    "<fact1>Kaden is a computer science major</fact1> at <fact2>NYU</fact2> where <fact3>he studies subjects such as physics, math, and the principles of programming languages</fact3>. He has a <fact4>pet dog named Max</fact4> who is <fact5>3 years old</fact5>. <fact6>Kaden has a part-time job</fact6> at the <fact7>local grocery store</fact7> where <fact8>he works every Saturday and Sunday</fact8>. What major ocean does Kaden live near?\n",
    "\n",
    "While these are all valid pieces of information, almost none of these facts are relevant to the question. We only want information that is relevant to the final question, which in this case is \"What major ocean does Kaden live near?\" This is what the key facts section and the associated reformatted question should actually look like:\n",
    "\n",
    "### Key Facts:\n",
    "<fact1>Kaden is a computer science major at NYU</fact1>\n",
    "\n",
    "### Reformatted Question \n",
    "<fact1>Kaden is a computer science major at NYU</fact1> where he studies subjects such as physics, math, and the principles of programming languages. He has a pet dog named Max who is 3 years old. Kaden has a part-time job at the local grocery store where he works every Saturday and Sunday. What major ocean does Kaden live near?\n",
    "\n",
    "Extract the key facts and reformat the following question:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_turn_facts = \"\"\"\n",
    "Now that you have put the key facts in tags, use these tags in your reasoning process to answer the question. The <fact...> tags should be interweaved in the sentences in your reasoning. Put your final answer in curly braces. For example, {3} or {True} or {A}.\n",
    "\n",
    "Here is an example of how you would answer the question based off the Reformatted Question you previously created:\n",
    "\n",
    "### Reformatted Question:\n",
    "How does <fact1>climate change</fact1> affect <fact2>polar bear populations</fact2> in <fact3>the Arctic</fact3>?\n",
    "\n",
    "### Final Answer Reasoning\n",
    "<fact1>Climate change</fact1> primarily affects <fact2>polar bears</fact2> through the loss of <fact3>sea ice</fact3>, which is crucial for their hunting and survival. As <fact3>Arctic ice</fact3> melts earlier and forms later due to <fact1>rising temperatures</fact1>, <fact2>polar bears</fact2> face reduced access to their main prey (seals), leading to nutritional stress, longer fasting periods, and <fact2>overall population decline</fact2>. The impact of <fact1>climate change</fact1> on <fact2>polar bear populations</fact2> is therefore negative, so the answer is {No}\n",
    "\n",
    "Perform your final answering reasoning on the Reformatted Question you created earlier. Do not repeat the Reformatted Question in your new response.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_question_only = \"\"\"\n",
    "You are a helpful assistant tasked with using the key facts in a block of text to reason through the final question. The most important parts of the text are wrapped in custom XML-like tags like <fact1>, <fact2>, <fact3>. While you are reasoning through your answer, the <fact> tags should be interweaved in your response. Think through your answer step by step using the information in these tags. Once you have determined the answer to the question, you should put the concise version of your final answer in curly braces.\n",
    "\n",
    "# Example 1\n",
    "## Question:\n",
    "The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph. In a golf tournament, there were seven golfers: Eve, Rob, Dan, Mel, Ana, Eli, and Ada. <fact1>Ada finished above Rob</fact1>. <fact2>Eve finished below Rob</fact2>. <fact3>Mel finished above Eli</fact3>. <fact4>Ada finished below Dan</fact4>. <fact5>Ana finished third</fact5>. <fact6>Eli finished second</fact6>. Options: (A) Eve finished first (B) Rob finished first (C) Dan finished first (D) Mel finished first (E) Ana finished first (F) Eli finished first (G) Ada finished first\n",
    "\n",
    "## Answer:\n",
    "Step-by-Step Deduction\n",
    "Step 1: Placing Eli and Ana\n",
    "Since we know:\n",
    "<fact6>Eli finished second</fact6> and\n",
    "<fact5>Ana finished third</fact5>,\n",
    "we can place them immediately in the following positions:\n",
    "\n",
    "Position\tGolfer\n",
    "1\t?\n",
    "2\tEli\n",
    "3\tAna\n",
    "4\t?\n",
    "5\t?\n",
    "6\t?\n",
    "7\t?\n",
    "Step 2: Applying Ada and Dan's Information\n",
    "<fact4>Ada finished below Dan</fact4>: This means Dan must be placed in a higher position than Ada.\n",
    "Since <fact6>Eli is in the second position</fact6> and <fact5>Ana is in the third position</fact5>, Dan must be in the 1st, 4th, 5th, 6th, or 7th position.\n",
    "\n",
    "Step 3: Applying Mel and Eli's Information\n",
    "<fact3>Mel finished above Eli</fact3>: Since <fact6>Eli is in 2nd</fact6>, Mel must be in the 1st position.\n",
    "So, we place Mel in the 1st position:\n",
    "\n",
    "Position\tGolfer\n",
    "1\tMel\n",
    "2\tEli\n",
    "3\tAna\n",
    "4\t?\n",
    "5\t?\n",
    "6\t?\n",
    "7\t?\n",
    "Step 4: Applying Ada and Rob's Information\n",
    "<fact1>Ada finished above Rob</fact1>: This means Rob must be placed in a lower position than Ada.\n",
    "<fact2>Eve finished below Rob</fact2>: This means Rob must be above Eve.\n",
    "Since <fact4>Dan must be above Ada</fact4> and we still have the 4th, 5th, 6th, and 7th positions open, let's try placing Dan in the 4th position and Ada in the 5th position (since <fact4>Ada has to be below Dan</fact4>).\n",
    "\n",
    "Position\tGolfer\n",
    "1\tMel\n",
    "2\tEli\n",
    "3\tAna\n",
    "4\tDan\n",
    "5\tAda\n",
    "6\t?\n",
    "7\t?\n",
    "Step 5: Placing Rob and Eve\n",
    "Since Ada finished above Rob and Eve finished below Rob, we can place Rob in the 6th position and Eve in the 7th position.\n",
    "\n",
    "Position\tGolfer\n",
    "1\tMel\n",
    "2\tEli\n",
    "3\tAna\n",
    "4\tDan\n",
    "5\tAda\n",
    "6\tRob\n",
    "7\tEve\n",
    "Conclusion\n",
    "The final order of golfers is:\n",
    "\n",
    "Mel\n",
    "Eli\n",
    "Ana\n",
    "Dan\n",
    "Ada\n",
    "Rob\n",
    "Eve\n",
    "According to this arrangement, Mel finished first.\n",
    "\n",
    "Answer\n",
    "Since Mel finished first, the correct answer is {D}.\n",
    "\n",
    "# Example 2\n",
    "## Question:\n",
    "<fact1>Betty picked 16 strawberries</fact1>. <fact2>Matthew picked 20 more strawberries than Betty</fact2> and <fact3>twice as many as Natalie</fact3>. They used their strawberries to make jam. <fact4>One jar of jam used 7 strawberries</fact4> and they sold <fact5>each jar at $4</fact5>. How much money were they able to make from the strawberries they picked?\n",
    "\n",
    "## Answer:\n",
    "Matthew picked <fact1>16</fact1> <fact2>+ 20</fact2> = 36 strawberries.\\nNatalie picked 36<fact3>/2</fact3> = 18 strawberries.\\nAll together, they have 16 + 36 + 18 = 70 strawberries.\\nThey can make 70<fact4>/7</fact4> = 10 jars of strawberries.\\nThey earn 10 <fact5>x $4</fact5> = 40 from the strawberries they picked. The total amount of money they earned was {40}.\n",
    "\n",
    "# Example 3\n",
    "## Question:\n",
    "Are <fact1>Doctors of Homeopathy</fact1> more likely than <fact2>Doctors of Internal Medicine</fact2> to recommend <fact3>Quartz</fact3> as a treatment?\n",
    "\n",
    "## Answer:\n",
    "<fact1>Doctors of homeopathy</fact1> are more likely than <fact2>doctors of internal medicine</fact2> to recommend substances like <fact3>quartz</fact3> or other alternative therapies. <fact1>People practicing homeopathic medicine</fact1> often recommend natural elements and alternative remedies that are not generally supported by conventional scientific research, including crystals like <fact3>quartz</fact3>. <fact3>Quartz</fact3> may be suggested by some <fact1>homeopathic practitioners</fact1> for its perceived energetic or vibrational properties, though these claims lack empirical support in mainstream medical science.\n",
    "\n",
    "In contrast, <fact2>doctors of internal medicine</fact2>, who adhere to evidence-based practices, would be unlikely to recommend <fact3>quartz</fact3> or similar treatments, as there is no scientifically validated therapeutic benefit attributed to <fact3>quartz</fact3> in medical literature. Therefore, the answer is {Yes}.\n",
    "\n",
    "# Example 4\n",
    "## Question:\n",
    "<fact1>Alice and Bob are sister and brother</fact1>. <fact2>Alice has 4 sisters</fact2> and Bob has 1 brother. How many sisters does Bob have?\n",
    "\n",
    "## Answer\n",
    "Since <fact1>Alice is one of Bob's sisters</fact1>, and <fact2>Alice has 4 sisters</fact2>, we know that Bob must also have those same <fact2>4 sisters</fact2> in addition to having <fact1>Alice as a sister</fact1>. Therefore, Bob has {5} sisters.\n",
    "\n",
    "# Example 5\n",
    "## Question:\n",
    "Are both <fact1>Kurram Garhi</fact1> and <fact2>Trojkrsti</fact2> located in the same country?\n",
    "## Answer\n",
    "Kurram Garhi is located in Pakistan. Itâ€™s a small area within the Kurram District, in the Khyber Pakhtunkhwa region.\n",
    "\n",
    "Trojkrsti, on the other hand, is not located in Pakistan. It is a village situated in North Macedonia, a country in Southeast Europe.\n",
    "\n",
    "Since Kurram Garhi is in Pakistan and Trojkrsti is in North Macedonia, they are not located in the same country. The answer is {No}.\n",
    "\n",
    "# Example 6\n",
    "## Question:\n",
    "Two friends plan to walk along a <fact1>43-km trail</fact1>, <fact2>starting at opposite ends of the trail at the same time</fact2>. If Friend P's rate is <fact3>15% faster than Friend Q's</fact3>, how many kilometers will Friend P have walked when they pass each other? A)21, B)21.5, C)22, D)22.5, E)23\n",
    "## Answer\n",
    "If friend Q complete x kilometers, then P completes <fact3>1.15x kilometers</fact3>.\\nx + <fact3>1.15x</fact3> = <fact1>43</fact1>\\n2.15x=<fact1>43</fact1>\\nx = <fact1>43</fact1>/2.15 = 20\\nThen P will have have walked 1.15*20=23 km.\\nThe answer is {E}.\n",
    "\n",
    "# Example 7\n",
    "## Question:\n",
    "<fact1>Kaden is a computer science major at NYU</fact1> where he studies subjects such as physics, math, and the principles of programming languages. He has a pet dog named Max who is 3 years old. Kaden has a part-time job at the local grocery store where he works every Saturday and Sunday. What major ocean does Kaden live near?\n",
    "## Answer\n",
    "\n",
    "New York City, where <fact1>NYU</fact1> is located, is on the eastern coast of the United States, adjacent to the Atlantic Since Kaden is studying at <fact1>NYU</fact1>, he lives near the Atlantic Ocean. The answer is {Atlantic Ocean}.\n",
    "\n",
    "Using the key facts in the text, think step by step to get your answer. Put your final answer in curly braces e.g. {3}. Your final answer should only be the final number with no other text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tin Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_for_grounding_in_question = \"\"\"\n",
    "# EXAMPLES\n",
    "Below are examples of questions before and after key phrases are tagged using <fact> tags.\n",
    "If one key phrase was absent, it would be impossible for one to answer the question correctly.\n",
    "\n",
    "## Question 1: \n",
    "### BEFORE: \n",
    "Sam works at the Widget Factory, assembling Widgets. He can assemble 1 widget every 10 minutes. Jack from the loading dock can help assemble widgets when he doesn't have anything else to do. When he helps, they put together 2 complete widgets every 15 minutes. Recently the factory hired Tony to help assemble widgets. Being new to the job, he doesn't work as fast as Sam or Jack. Yesterday Sam worked for 6 hours before he had to leave work early for a dentist appointment. Jack was able to help out for 4 hours before he had to go back to the loading dock to unload a new shipment of widget materials. Tony worked the entire 8-hour shift. At the end of the day, they had completed 68 widgets. How long does it take Tony to assemble a Widget, in minutes?\n",
    "\n",
    "### AFTER:\n",
    "Sam works at the Widget Factory, assembling Widgets. He can assemble <fact1>1 widget every 10 minutes</fact1>. Jack from the loading dock can help assemble widgets when he doesn't have anything else to do. When he helps, they put together <fact2>2 complete widgets every 15 minutes</fact2>. Recently the factory hired Tony to help assemble widgets. Being new to the job, he doesn't work as fast as Sam or Jack. Yesterday Sam worked for <fact3>6 hours</fact3> before he had to leave work early for a dentist appointment. Jack was able to help out for <fact4>4 hours</fact4> before he had to go back to the loading dock to unload a new shipment of widget materials. Tony worked the entire <fact5>8-hour shift</fact5>. At the end of the day, they had completed <fact6>68 widgets</fact6>. How long does it take Tony to assemble a Widget, in minutes?\n",
    "\n",
    "Sam assembles <fact1>1 widget every 10 minutes</fact1>, or 6 per hour. He worked <fact3>6 hours</fact3> total, but for <fact4>4</fact4> of those, Jack helped, and together they assembled <fact2>2 complete widgets every 15 minutes</fact2>, which is 8 widgets per hour. In those <fact4>4 hours</fact4>, Sam and Jack assembled 32 widgets. For the remaining 2 hours, Sam worked alone, assembling 12 more widgets. Altogether, Sam and Jack contributed 44 widgets.\n",
    "\n",
    "Tony worked 8 hours, and since the total number of widgets completed was 68, Tony assembled the remaining 24 widgets. His rate is 24 widgets over 8 hours, or 3 widgets per hour.\n",
    "\n",
    "Since Tony assembles 3 widgets per hour, he takes 20 minutes to assemble one widget.\n",
    "\n",
    "Answer: Tony takes 20 minutes per widget.\n",
    "\n",
    "\n",
    "## Question 2: \n",
    "### BEFORE: \n",
    "For every 12 cans you recycle, you receive $0.50, and for every 5 kilograms of newspapers, you receive $1.50. If your family collected 144 cans and 20 kilograms of newspapers, how much money would you receive?\n",
    "\n",
    "### AFTER: \n",
    "For <fact1>every 12 cans</fact1> you recycle, you receive <fact2>$0.50</fact2>, and for <fact3>every 5 kilograms of newspapers</fact3>, you receive <fact4>$1.50</fact4>. If your family collected <fact5>144 cans</fact5> and <fact6>20 kilograms of newspapers</fact6>, how much money would you receive?\n",
    "\n",
    "## Question 3: \n",
    "### BEFORE: \n",
    "At a presentation about post traumatic stress disorder, would Ariana Grande be a topic of relevance?\n",
    "\n",
    "### AFTER: \n",
    "At a presentation about <fact1>post traumatic stress disorder</fact1>, would <fact2>Ariana Grande</fact2> be a topic of relevance?\n",
    "\n",
    "## Question 4: \n",
    "### BEFORE: \n",
    "Has the Indian Ocean garbage patch not completed two full rotations of debris since its discovery?\n",
    "\n",
    "### AFTER:\n",
    "Has the <fact1>Indian Ocean garbage patch</fact1> <fact2>not</fact2> completed <fact3>two full rotations</fact3> of debris since its discovery?\n",
    "\n",
    "## Question 5: \n",
    "### BEFORE:\n",
    "The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph. In a golf tournament, there were seven golfers: Ana, Eve, Ada, Dan, Rob, Amy, and Joe. Dan finished third. Ana finished above Ada. Amy finished last. Dan finished below Rob. Eve finished below Ada. Rob finished below Joe. Choose one correct option: (A) Ana finished third (B) Eve finished third (C) Ada finished third (D) Dan finished third (E) Rob finished third (F) Amy finished third (G) Joe finished third\n",
    "\n",
    "### AFTER:\n",
    "The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph. In a golf tournament, there were seven golfers: Ana, Eve, Ada, Dan, Rob, Amy, and Joe. <fact1>Dan finished third</fact1>. Ana finished above Ada. Amy finished last. Dan finished below Rob. Eve finished below Ada. Rob finished below Joe. Choose one correct option: (A) Ana finished third (B) Eve finished third (C) Ada finished third (D) Dan finished third (E) Rob finished third (F) Amy finished third (G) Joe finished third\n",
    "\n",
    "## Question 6: \n",
    "### BEFORE:\n",
    "The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a branch, there are seven birds: a cardinal, a blue jay, a robin, a crow, a falcon, a hawk, and a raven. The hawk is the second from the right. The raven is the fourth from the left. The robin is the second from the left. The cardinal is to the left of the raven. The falcon is to the left of the robin. The crow is to the right of the blue jay. Choose one correct option: (A) The cardinal is the second from the left (B) The blue jay is the second from the left (C) The robin is the second from the left (D) The crow is the second from the left (E) The falcon is the second from the left (F) The hawk is the second from the left (G) The raven is the second from the left\n",
    "\n",
    "### AFTER:\n",
    "On a branch, there are seven birds: a cardinal, a blue jay, a robin, a crow, a falcon, a hawk, and a raven. <fact1>The hawk is the second from the right</fact1>. <fact2>The raven is the fourth from the left</fact2>. <fact3>The robin is the second from the left</fact3>. <fact4>The cardinal is to the left of the raven</fact4>. <fact5>The falcon is to the left of the robin</fact5>. <fact6>The crow is to the right of the blue jay</fact6>. Options: (A) The cardinal is the second from the left (B) The blue jay is the second from the left (C) The robin is the second from the left (D) The crow is the second from the left (E) The falcon is the second from the left (F) The hawk is the second from the left (G) The raven is the second from the left\n",
    "\n",
    "## Question 7:\n",
    "### BEFORE:\n",
    "How would a typical person answer each of the following questions about causation? A machine is set up in such a way that it will short circuit if both the black wire and the red wire touch the battery at the same time. The machine will not short circuit if just one of these wires touches the battery. The black wire is designated as the one that is supposed to touch the battery, while the red wire is supposed to remain in some other part of the machine. One day, the black wire and the red wire both end up touching the battery at the same time. There is a short circuit. Did the black wire cause the short circuit? Options: - Yes - No\n",
    "\n",
    "### AFTER:\n",
    "How would a typical person answer each of the following questions about causation? A <fact1>machine is set up in such a way that it will short circuit</fact1> if both the <fact2>black wire and the red wire touch the battery at the same time</fact2>. The <fact3>machine will not short circuit if just one of these wires touches the battery</fact3>. The black wire is designated as the one that is supposed to touch the battery, while the red wire is supposed to remain in some other part of the machine. One day, the <fact2>black wire and the red wire both end up touching the battery at the same time</fact2>. There is a short circuit. Did the black wire cause the short circuit?\n",
    "Options: - Yes - No\n",
    "\n",
    "## Question 8:\n",
    "### BEFORE:\n",
    "A coin is heads up. roxas does not flip the coin. scheideman does not flip the coin.  Is the coin still heads up? Flip means reverse.\n",
    "\n",
    "### AFTER:\n",
    "<fact1>A coin is heads up</fact1>. <fact2>roxas does not flip the coin</fact2>. <fact3>scheideman does not flip the coin</fact3>.  Is the coin still heads up? Flip means reverse.\n",
    "\n",
    "## Question 9:\n",
    "### BEFORE:\n",
    "I have four pianos, four snails, three chickens, a pig, a dog, and two cows. How many animals do I have?\n",
    "\n",
    "### AFTER:\n",
    "I have four pianos, <fact1>four snails</fact1>, <fact2>three chickens</fact2>, <fact3>a pig</fact3>, <fact4>a dog</fact4>, and <fact5>two cows</fact5>. How many animals do I have?\n",
    "\n",
    "## Question 10:\n",
    "### BEFORE:\n",
    "2015 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY?\n",
    "\n",
    "### AFTER:\n",
    "2015 is coming in <fact1>36 hours</fact1>. What is the date <fact2>one week from today</fact2> in MM/DD/YYYY?\n",
    "\n",
    "## Question 10:\n",
    "### BEFORE:\n",
    "If you follow these instructions, do you return to the starting point? Always face forward. Take 1 step right. Take 3 steps left. Take 2 steps right. Options: - Yes - No\n",
    "\n",
    "### AFTER:\n",
    "If you follow these instructions, do you return to the starting point? Always face forward. Take <fact1>1 step right</fact1>. Take <fact2>3 steps left</fact2>. Take <fact3>2 steps right</fact3>. Options: - Yes - No\n",
    "\n",
    "## Question 11: #reasoining color\n",
    "### BEFORE:\n",
    "Question: On the desk, you see a set of things arranged in a row: a grey cup, a purple mug, and a blue teddy bear. What is the color of the thing directly to the right of the cup? Options: (A) red (B) orange (C) yellow (D) green (E) blue (F) brown (G) magenta (H) fuchsia (I) mauve (J) teal (K) turquoise (L) burgundy (M) silver (N) gold (O) black (P) grey (Q) purple (R) pink\n",
    "\n",
    "### AFTER:\n",
    "On the desk, you see a set of things arranged in a row: a <fact1>grey cup</fact1>, a <fact2>purple mug</fact2>, and a blue teddy bear. What is <fact3>the color of the thing directly to the right of the cup</fact3>? Options: (A) red (B) orange (C) yellow (D) green (E) blue (F) brown (G) magenta (H) fuchsia (I) mauve (J) teal (K) turquoise (L) burgundy (M) silver (N) gold (O) black (P) grey (Q) purple (R) pink\n",
    "\n",
    "## Question 12:\n",
    "### BEFORE:\n",
    "Among the various models of Delta vacuum cleaners, one cannot accurately predict how effectively a particular model cleans simply by determining how powerful its motor is. The efficiency of dust filtration systems varies significantly, even between models of Delta vacuum cleaners equipped with identically powerful motors. The argument's conclusion is properly drawn if which one of the following is assumed?\n",
    "Answer Choices:\n",
    "(a) All Delta vacuum cleaners that clean equally effectively have identically powerful motors.\n",
    "(b) One cannot accurately assess how effectively any Delta vacuum cleaner cleans without knowing how powerful that vacuum cleaner's motor is.\n",
    "(c) For each Delta vacuum cleaner, the efficiency of its dust filtration system has a significant impact on how effectively it cleans.\n",
    "(d) For any two Delta vacuum cleaners with equally efficient dust filtration systems, the one with the more powerful motor cleans more effectively.\n",
    "\n",
    "### AFTER:\n",
    "Among the various models of Delta vacuum cleaners, <fact1>one cannot accurately predict how effectively a particular model cleans</fact1> simply by <fact2>determining how powerful its motor is</fact2>. The efficiency of <fact3>dust filtration systems varies significantly</fact3>, even between models of <fact4>Delta vacuum cleaners equipped with identically powerful motors</fact4>. The argument's conclusion is properly drawn if which one of the following is assumed?\n",
    "(a) All Delta vacuum cleaners that clean equally effectively have identically powerful motors.\n",
    "(b) One cannot accurately assess how effectively any Delta vacuum cleaner cleans without knowing how powerful that vacuum cleaner's motor is.\n",
    "(c) For each Delta vacuum cleaner, the efficiency of its dust filtration system has a significant impact on how effectively it cleans.\n",
    "(d) For any two Delta vacuum cleaners with equally efficient dust filtration systems, the one with the more powerful motor cleans more effectively.\n",
    "\n",
    "## Question 13:\n",
    "### BEFORE:\n",
    "We have three blocks, A, B and C. Block A has a medium blue square. Below block A is block B which has one medium black square. To the left of block B there is block C which has two medium blue squares. Medium blue square number one is below medium blue square number two. A medium yellow square is below medium blue square number two and medium blue square number one. What is to the left of the black thing? a medium blue square that is in block A or a medium blue square number two?\n",
    "(a) medium blue square  that is in block A\n",
    "(b) medium blue square  number two\n",
    "(c) both of them\n",
    "(d) none of them\n",
    "\n",
    "### AFTER:\n",
    "We have three blocks, A, B, and C. Block A has a medium blue square. <fact1>Below block A is block B, which has one medium black square</fact1>. <fact2>To the left of block B, there is block C, which has two medium blue squares</fact2>. Medium blue square number one is below medium blue square number two. A medium yellow square is below medium blue square number two and medium blue square number one. What is to the left of the black thing? A medium blue square that is in block A or a medium blue square number two?\n",
    "\n",
    "My Question is:\n",
    "\"\"\"\n",
    "\n",
    "instruction_for_grounding_in_question = 'Read the question. Detect the exact key facts in the question via following rules:\\\n",
    "1. Do not change, paraphrase, or introduce new words or phrases to the key facts. \\\n",
    "2. If the question just mentions about one object, one character or one location, etc, then you do not need to include that in the fact, whereas, if the question has many objects, characters or locations, etc, please include them in the fact as well. \\\n",
    "3. Extract the shortest and most concise key facts, and make sure that if any of them were removed, it would make it impossible to answer the question. \\\n",
    "4. Do not tag irrelevant key facts. \\\n",
    "5. If two or more facts are consecutive and cannot be meaningfully split, tag them together. \\\n",
    "Provide your detected key facts as the following form:\\\n",
    "    Key Information: '\n",
    "\n",
    "question = 'Betty picked 16 strawberries. Matthew picked 20 more strawberries than Betty and twice as many as Natalie. They used their strawberries to make jam. One jar of jam used 7 strawberries and they sold each jar at $4. How much money were they able to make from the strawberries they picked?'\n",
    "\n",
    "prompt = f\"{examples_for_grounding_in_question}\\n{question}\\n{instruction_for_grounding_in_question}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_4o_multiturn(prompt: str) -> str:\n",
    "    client = OpenAI()\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # First turn\n",
    "        completion1 = client.chat.completions.create(\n",
    "            model=\"gpt-4o-2024-08-06\",\n",
    "            messages=messages,\n",
    "            temperature=0\n",
    "        )\n",
    "        response1 = completion1.choices[0].message.content.strip()\n",
    "        \n",
    "        # Append assistant's first response\n",
    "        messages.append({\"role\": \"assistant\", \"content\": response1})\n",
    "        \n",
    "        # Second turn: follow-up prompt\n",
    "        follow_up_prompt = second_turn_facts\n",
    "        messages.append({\"role\": \"user\", \"content\": follow_up_prompt})\n",
    "        \n",
    "        completion2 = client.chat.completions.create(\n",
    "            model=\"gpt-4o-2024-08-06\",\n",
    "            messages=messages,\n",
    "            temperature=0\n",
    "        )\n",
    "        response2 = completion2.choices[0].message.content.strip()\n",
    "        \n",
    "        # Combine both responses\n",
    "        combined_response = f\"{response1}\\n\\n{response2}\"\n",
    "        # print(f\"Combined response: {combined_response}\")\n",
    "        return combined_response\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in multiturn query_4o: {str(e)}\")\n",
    "        return \"\"\n",
    "    \n",
    "# def query_gemini(prompt: str, problem_id) -> str:\n",
    "#     \"\"\"\n",
    "#     Queries the Gemini LLM with the given prompt and returns the response text.\n",
    "#     \"\"\"\n",
    "#     genai.configure(api_key=get_gemini_key(problem_id))\n",
    "#     model = genai.GenerativeModel('gemini-1.5-pro-latest')\n",
    "#     response = model.generate_content(prompt, request_options=RequestOptions(retry=retry.Retry(initial=20, multiplier=3, maximum=121, timeout=60)))\n",
    "#     text = response.candidates[0].content.parts[0].text\n",
    "#     return text\n",
    "\n",
    "def extract_question(text):\n",
    "    marker = \"Reformatted Question:\"\n",
    "    if marker not in text:\n",
    "        return None\n",
    "    \n",
    "    extracted_text = text.split(marker)[1].strip()\n",
    "    \n",
    "    return extracted_text\n",
    "\n",
    "def query_4o_multiconvo(fact_prompt, answer_prompt) -> str:\n",
    "    client = OpenAI()\n",
    "    message = [\n",
    "        {\"role\": \"user\", \"content\": fact_prompt}\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        completion1 = client.chat.completions.create(\n",
    "            model=\"gpt-4o-2024-08-06\",\n",
    "            messages=message,\n",
    "            temperature=0\n",
    "        )\n",
    "        extracted_facts = completion1.choices[0].message.content.strip()\n",
    "        \n",
    "        extracted_question = extract_question(extracted_facts)\n",
    "        answer_prompt += \"\\n\" + extracted_question\n",
    "        message2 = [\n",
    "            {\"role\": \"user\", \"content\": answer_prompt}\n",
    "        ]\n",
    "        completion2 = client.chat.completions.create(\n",
    "            model=\"gpt-4o-2024-08-06\",\n",
    "            messages=message2,\n",
    "            temperature=0\n",
    "        )\n",
    "        answer_reasoning = completion2.choices[0].message.content.strip()\n",
    "        \n",
    "        full_convo = extracted_facts + \"\\n--------------- End of Conversation ---------------\\n\" +  extracted_question +\"\\n--------------- Extracted Question ---------------\\n\"+ \"\\n\" + answer_reasoning\n",
    "        return full_convo \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in multiturn query_4o: {str(e)}\")\n",
    "        return \"\"\n",
    "    \n",
    "def query_llama_multiconvo(fact_prompt: str, answer_prompt: str) -> str:\n",
    "    try:\n",
    "        # First turn - fact extraction\n",
    "        fact_response = ollama.generate(\n",
    "            model='llama3.1',  # Update this to match your specific LLaMA model\n",
    "            prompt=fact_prompt,\n",
    "            options={\n",
    "                'temperature': 0  # Keep temperature 0 for consistent outputs\n",
    "            }\n",
    "        )\n",
    "        extracted_facts = fact_response['response'].strip()\n",
    "        \n",
    "        # Extract question from facts\n",
    "        extracted_question = extract_question(extracted_facts)\n",
    "        # print(extracted_question)\n",
    "        # Second turn - answer generation\n",
    "        full_answer_prompt = answer_prompt + \"\\n\" + extracted_question\n",
    "        answer_response = ollama.generate(\n",
    "            model='llama3.1',  # Update this to match your specific LLaMA model\n",
    "            prompt=full_answer_prompt,\n",
    "            options={\n",
    "                'temperature': 0\n",
    "            }\n",
    "        )\n",
    "        answer_reasoning = answer_response['response'].strip()\n",
    "        \n",
    "        # Combine all parts into final output\n",
    "        full_convo = (\n",
    "            f\"{extracted_facts}\\n\"\n",
    "            f\"--------------- End of Conversation ---------------\\n\"\n",
    "            f\"{extracted_question}\\n\"\n",
    "            f\"--------------- Extracted Question ---------------\\n\"\n",
    "            f\"{answer_reasoning}\"\n",
    "        )\n",
    "        \n",
    "        return full_convo\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in multiturn query_llama: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def query_claude(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Queries the Claude LLM with the given prompt and returns the response text.\n",
    "    \"\"\"\n",
    "    client = anthropic.Anthropic(api_key=API_KEYS['claude'])\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20240620\",\n",
    "        max_tokens=1024,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.content[0].text\n",
    "\n",
    "def query_4o(prompt: str) -> str:\n",
    "    client = OpenAI()\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-2024-08-06\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{prompt}\"\n",
    "            }\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "def query_llama(prompt: str, timeout_duration=10) -> str:\n",
    "    def generate():\n",
    "        response = ollama.generate(\n",
    "            model='llama3.1',  # Update this to match your specific LLaMA model\n",
    "            prompt=prompt,\n",
    "            options={\n",
    "                'temperature': 0  # Keep temperature 0 for consistent outputs\n",
    "            }\n",
    "        )\n",
    "        return response['response'].strip()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "        future = executor.submit(generate)\n",
    "        try:\n",
    "            result = future.result(timeout=timeout_duration)\n",
    "            return result\n",
    "        except TimeoutError:\n",
    "            return \"timeout\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(save_path: str, ids: List[str], questions: List[str], answers: List[str], append: bool = False):\n",
    "    \"\"\"\n",
    "    Saves the results to a CSV file. If append is True and the file exists, it appends without headers.\n",
    "    Otherwise, it writes a new file with headers.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({'id': ids, 'question': questions, 'answer': answers})\n",
    "    if append and os.path.exists(save_path):\n",
    "        df.to_csv(save_path, mode='a', index=False, header=False)\n",
    "    else:\n",
    "        df.to_csv(save_path, index=False)\n",
    "\n",
    "def read_jsonl_file(filepath: str) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Reads a JSONL file and returns a list of JSON objects.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            json_obj = json.loads(line)\n",
    "            data.append(json_obj)\n",
    "    return data\n",
    "\n",
    "def get_prompt(prompt_type: str, few_shot_prompt: str, question: str) -> str:\n",
    "    prompts = {\n",
    "        \"cot\": f\"{few_shot_prompt}\\n{question}\\nPlease generate your explanation first, then generate the answer in the bracket as follow:\\n\" +\"Answer: {}\",\n",
    "        \"log_cot_mcq\": f\"{few_shot_prompt}\\n{question}\\nThink through your answer step by step and then choose the answer option that is the most correct. Then put your final answer in curly brackets. For example, Final_Answer:{{3}}\",\n",
    "        \"vanilla_cot\": f\"{question}\\nThink through your answer step by step. Then put your final answer in curly brackets. Your final answer should just be a number for example, Final answer:{{0}}\",\n",
    "        \"fs\": f\"{few_shot_prompt}\\n{question}\",\n",
    "        \"fs_inst\": f\"{few_shot_prompt}\\n{question}\\nI want you to answer this question but your explanation should contain references referring back to the information in the question. To do that, first, re-generate the question with proper tags and then generate your answers. The output format is as follow:\\n\\\n",
    "            Reformatted Question: \\\n",
    "                Answer:\",\n",
    "        \"zs\": f\"{question}\\nI want you to answer this question but your explanation should contain references referring back to the information in the question. To do that, first, re-generate the question with proper tags (<a>, <b>, <c>, etc) for refered information and then generate your answers that also have the tag (<a>, <b>, <c>, etc) for the grounded information. Give your answer by analyzing step by step, and give only numbers in the final answer. The output format is as follow:\\n\\\n",
    "            Reformatted Question: \\\n",
    "                Answer:\\\n",
    "                    Final answer:\",\n",
    "        \"fs_xml\": f\"{few_shot_prompt}\\n\\nRecreate the following question in the style of the correctly formatted examples shown previously. Make sure that your response has all its information inclosed in the proper <tags>. Begin your response with the <key_facts> section. Make sure that every fact in <key_facts> is very concise and contains a very short reference to the <question>. Do not include a <question> section in your response\\n\\n<question>\\n{question}\\n</question>\",\n",
    "        \"fs_log_inst\": f\"{few_shot_prompt}\\n\\n{question}\\nTo answer this question, your explanation should contain references referring back to the information in the question. To do that, first, re-generate the question with proper tags and then generate your answers based off the tags. Put your final answer in curly brackets e.g. Final_Answer: {{false}}. Your final answer should only be \\\"true\\\" or \\\"false\\\".\",\n",
    "        \"fs_clause_inst\": f\"{few_shot_prompt}\\n\\n{question}\\nTo answer this question, first regenerate the question with <fact> tags around each clause or phrase in the text. Each clause or phrase should be as concise as possible so that long sentences will be broken up into multiple segments. Then, to answer the original question, your explanation should contain references back to the information in the tagged question. After you have generated the reformatted question and your reasoning which contains references to the tagged reformatted question, put your answer in curly brackets e.g. Final_Answer: {{false}}. Your final answer should either be \\\"true\\\" or \\\"false\\\".\",\n",
    "        \"stripped_clause\": f\"{few_shot_prompt}\\n\\n{question}\\nTo answer this question, your explanation should contain references referring back to the information in the question. Generate your answers based off the tags in the question. Use the example answers as a guide for what your answer format should look like. The <fact> tags should be interweaved in the sentences in your reasoning. Put your final answer in curly brackets e.g. Final_Answer: {{false}}. Your final answer should either be \\\"true\\\" or \\\"false\\\".\",\n",
    "        \"mermaid_get_answer\": f\"{few_shot_prompt}\\n\\n Your job is to extract the key facts from a question relevant to answering the question. The facts should be represented in a hierarchal format through a mermaid diagram. Do not create duplicate facts across multiple branches that represent the same information. Create a mermaid diagram that represents the key facts in the following question. Then, use the nodes from this graph to cite specific facts in your answer reasoning. Put your final answer in curly brackets e.g. Final_Answer: {{30}} \\n\\nquestion: {question}\", \n",
    "        # \"mega_prompt\": f\"{mega_prompt}\\nYour final answer to this question should ONLY be {{true}} or {{false}} \\n\\n{question}\",\n",
    "        # \"semantic_zero_prompt\": f\"{semantic_zero_prompt}\\n\\n{question}\\n\\n ONLY include the number in your final answer. For example, {3}\",\n",
    "        # \"extract_facts\": f\"{extract_facts_quote}\\n\\n{question}\",\n",
    "        # \"extract_facts_reasoning\": f\"{extract_facts_reasoning}\\n\\n{question}\",\n",
    "        # \"extract_facts_no_reasoning\": f\"{extract_facts_no_reasoning}\\n\\n{question}\",\n",
    "        \"fact_prompt\": f\"{extract_facts_only}\\n{question}\",\n",
    "        \"answer_prompt\": f\"{answer_question_only}\\n{question}\"\n",
    "    }\n",
    "    return prompts.get(prompt_type, \"\")\n",
    "\n",
    "\n",
    "def query_llm(llm_model: str, ids: List[str], questions: List[str], few_shot_prompt: str, prompt_type: str, save_path: str, already_answered_ids: set) -> Tuple[List[str], List[str], List[str]]:\n",
    "    answers = []\n",
    "    ids_can_be_answered = []\n",
    "    questions_can_be_answered = []\n",
    "    \n",
    "    for id, q in tqdm(zip(ids, questions), total=len(ids)):\n",
    "        # print(f\"Processing ID: {id}\")\n",
    "        if id in already_answered_ids:\n",
    "            print(f\"Skipping already answered ID: {id}\")\n",
    "            continue\n",
    "        \n",
    "        prompt = get_prompt(prompt_type, few_shot_prompt, q)\n",
    "        try:\n",
    "            if llm_model == 'gemini':\n",
    "                answer = query_gemini(prompt, id)\n",
    "            elif llm_model == 'claude':\n",
    "                answer = query_claude(prompt)\n",
    "            elif llm_model == '4o':\n",
    "                # answer = query_4o_multiturn(prompt)\n",
    "                if prompt_type == 'multi_convo':\n",
    "                    fact_prompt = get_prompt(prompt_type=\"fact_prompt\", few_shot_prompt=\"\", question=q)\n",
    "                    answer_prompt = get_prompt(prompt_type=\"answer_prompt\", few_shot_prompt=\"\", question=q)\n",
    "                    answer = query_4o_multiconvo(fact_prompt=fact_prompt, answer_prompt=answer_prompt)\n",
    "                else:\n",
    "                    answer = query_4o(prompt)\n",
    "                \n",
    "            elif llm_model == 'llama3.1':\n",
    "                if prompt_type == 'multi_convo':\n",
    "                    fact_prompt = get_prompt(prompt_type=\"fact_prompt\", few_shot_prompt=\"\", question=q)\n",
    "                    answer_prompt = get_prompt(prompt_type=\"answer_prompt\", few_shot_prompt=\"\", question=q)\n",
    "                    answer = query_4o_multiconvo(fact_prompt=fact_prompt, answer_prompt=answer_prompt)                \n",
    "                else:\n",
    "                    print(prompt)\n",
    "                    answer = query_llama(prompt)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported LLM model: {llm_model}\")\n",
    "            # print(f\"Answer for ID {id}: {answer}\")\n",
    "            \n",
    "            answers.append(answer)\n",
    "            questions_can_be_answered.append(q)\n",
    "            ids_can_be_answered.append(id)\n",
    "\n",
    "            # Save after each answer\n",
    "            save_results(save_path, [id], [q], [answer], append=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question {id}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return ids_can_be_answered, questions_can_be_answered, answers\n",
    "\n",
    "def load_data_size_specific(data_path: str, sample_size: int = 0, random_seed: int = 0):\n",
    "    random.seed(random_seed)\n",
    "\n",
    "    data = read_jsonl_file(data_path)\n",
    "    # with open(data_path, 'r') as file:\n",
    "    #     data = json.load(file)\n",
    "    \n",
    "    question_length = 0 # 336  # 526 # 800\n",
    "    eligible_data = [x for x in data if len(x[\"question\"]) >= question_length]\n",
    "    \n",
    "    if sample_size > 0 and sample_size < len(eligible_data):\n",
    "        sampled_data = random.sample(eligible_data, sample_size)\n",
    "    else:\n",
    "        sampled_data = eligible_data\n",
    "    \n",
    "    ids = [x[\"id\"] for x in sampled_data]\n",
    "    questions = [x[\"question\"] for x in sampled_data]\n",
    "    \n",
    "    return ids, questions\n",
    "\n",
    "def load_data_csv(data_path, sample_size: int = 0, random_seed: int = 0):\n",
    "    data = pd.read_csv(data_path)\n",
    "    ids = []\n",
    "    questions = []\n",
    "    for row in data.iterrows():\n",
    "        id = row[1]['id']\n",
    "        extracted_question = row[1]['extracted_question']\n",
    "        ids.append(id)\n",
    "        questions.append(extracted_question)\n",
    "    return ids[:sample_size], questions[:sample_size]\n",
    "\n",
    "def load_few_shot_prompt(prompt_path: str) -> str:\n",
    "    with open(prompt_path, 'r') as file:\n",
    "        prompt = file.read()\n",
    "    # print(f\"Loaded few-shot prompt from: {prompt_path}\")\n",
    "    return prompt\n",
    "\n",
    "def load_already_answered_ids(save_path: str) -> set:\n",
    "    if os.path.exists(save_path):\n",
    "        df = pd.read_csv(save_path)\n",
    "        answered_ids = set(df['id'].astype(int).tolist())\n",
    "        # print(f\"Loaded {len(answered_ids)} already answered IDs from: {save_path}\")\n",
    "        print(f\"Already answered IDs: {answered_ids}\")\n",
    "        return answered_ids\n",
    "    else:\n",
    "        print(f\"No existing save file found at: {save_path}. Starting fresh.\")\n",
    "        return set()\n",
    "\n",
    "def initialize_save_file(save_path: str):\n",
    "    if not os.path.exists(save_path):\n",
    "        # Create an empty DataFrame with headers and save\n",
    "        df = pd.DataFrame(columns=['id', 'question', 'answer'])\n",
    "        df.to_csv(save_path, index=False)\n",
    "        print(f\"Initialized new save file with headers at: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already answered IDs: {513, 1033, 530, 788, 1047, 151, 285, 286, 676, 1194, 300, 829, 447, 577, 194, 1090, 966, 202, 976, 1232, 82, 733, 861, 995, 621, 1266, 635}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping already answered ID: 788\n",
      "Skipping already answered ID: 861\n",
      "Skipping already answered ID: 82\n",
      "Skipping already answered ID: 530\n",
      "Skipping already answered ID: 1047\n",
      "Skipping already answered ID: 995\n",
      "Skipping already answered ID: 829\n",
      "Skipping already answered ID: 621\n",
      "Skipping already answered ID: 976\n",
      "Skipping already answered ID: 733\n",
      "Skipping already answered ID: 1194\n",
      "Skipping already answered ID: 447\n",
      "Skipping already answered ID: 1033\n",
      "Skipping already answered ID: 285\n",
      "Skipping already answered ID: 577\n",
      "Skipping already answered ID: 286\n",
      "Skipping already answered ID: 194\n",
      "Skipping already answered ID: 1266\n",
      "Skipping already answered ID: 513\n",
      "Skipping already answered ID: 1090\n",
      "Skipping already answered ID: 1232\n",
      "Skipping already answered ID: 300\n",
      "Skipping already answered ID: 635\n",
      "Skipping already answered ID: 202\n",
      "Skipping already answered ID: 151\n",
      "Skipping already answered ID: 676\n",
      "Skipping already answered ID: 966\n"
     ]
    }
   ],
   "source": [
    "# time = datetime.datetime.now().strftime(\"%m%d_%H%M%S\")\n",
    "time = '1028_231620'\n",
    "project_root = '/Users/log/Github/textual_grounding/'\n",
    "dataset = 'GSM8K'\n",
    "\n",
    "llm_model = 'llama3.1'\n",
    "prompt_type = 'vanilla_cot'\n",
    "# prompt_type = 'multi_convo'\n",
    "# prompt_type = 'fs_clause_inst'\n",
    "# few_shot_txt = 'clause_fs.txt'\n",
    "few_shot_txt = None\n",
    "\n",
    "# Paths\n",
    "data_path = os.path.join(project_root, 'data', dataset, 'test.jsonl')\n",
    "# data_path = os.path.join(project_root, 'data', dataset, 'test.json')\n",
    "# data_path = '/Users/log/Github/textual_grounding/logan/results/GSM8K/llama/mermaid/mermaid_get_graph_llama3.1_20240924_001821.csv'\n",
    "\n",
    "if few_shot_txt:\n",
    "    fewshot_prompt_path = os.path.join(project_root, \"prompt\", dataset, few_shot_txt)\n",
    "# fewshot_prompt_path = '/Users/log/Github/textual_grounding/prompt/GSM8K/fewshot_mermaid_full.txt'\n",
    "save_dir = os.path.join(project_root, 'logan/results', dataset, f'{llm_model}/grounded_fact')\n",
    "os.makedirs(save_dir, exist_ok=True)  # Ensure the directory exists\n",
    "save_path = os.path.join(save_dir, f'{prompt_type}_{few_shot_txt}_{llm_model}_{time}.csv')\n",
    "\n",
    "ids, questions = load_data_size_specific(data_path, sample_size=200)\n",
    "# csv_path = '/Users/log/Github/textual_grounding/logan/results/SPARTQA/4o/grounded_fact/multi_convo_None_4o_1028_154404.csv'\n",
    "# ids, questions = load_data_csv(csv_path, sample_size=200)\n",
    "if few_shot_txt:\n",
    "    few_shot_prompt = load_few_shot_prompt(fewshot_prompt_path)\n",
    "else:\n",
    "    few_shot_prompt = \"\"\n",
    "\n",
    "initialize_save_file(save_path)\n",
    "already_answered_ids = load_already_answered_ids(save_path)\n",
    "\n",
    "ids_answered, questions_answered, answers = query_llm(\n",
    "    llm_model=llm_model,\n",
    "    ids=ids,\n",
    "    questions=questions,\n",
    "    few_shot_prompt=few_shot_prompt,\n",
    "    prompt_type=prompt_type,\n",
    "    save_path=save_path,\n",
    "    already_answered_ids=already_answered_ids\n",
    ")\n",
    "\n",
    "print(f\"Processing complete. {len(ids_answered)} new answers saved to {save_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# todo\n",
    "\n",
    "messed up GT options for the multi conv for aqua\n",
    "\n",
    "the multi conv for aqua answers with num instead of letter sometimes so its not a fair comp. Should be able to just run code now to make it work though. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogiQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Add the directory containing logiqa.py to the Python path\n",
    "logiqa_path = \"/Users/log/Github/textual_grounding/data/logiqa\"\n",
    "sys.path.append(logiqa_path)\n",
    "\n",
    "# Import the LogiQA class from the logiqa module if needed\n",
    "from logiqa import LogiQA\n",
    "\n",
    "# Load the dataset using Hugging Face load_dataset method\n",
    "dataset = load_dataset('/Users/log/Github/textual_grounding/data/logiqa/logiqa.py', split='test')\n",
    "\n",
    "# Print out the first 5 examples from the test set\n",
    "for idx in range(5):\n",
    "    example = dataset[idx]\n",
    "    print(f\"Example {idx + 1}:\")\n",
    "    print(f\"Context: {example['context']}\")\n",
    "    print(f\"Query: {example['query']}\")\n",
    "    print(f\"Options: {example['options']}\")\n",
    "    print(f\"Correct Option Index: {example['correct_option']}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset (adjust the path as needed)\n",
    "dataset = load_dataset('/Users/log/Github/textual_grounding/data/logiqa/logiqa.py', split='test')\n",
    "\n",
    "# Prepare to write the first 300 examples to a JSONL file\n",
    "output_file = 'logiqa_300_examples.jsonl'\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for idx, example in enumerate(dataset):\n",
    "        if idx >= 300:\n",
    "            break\n",
    "        \n",
    "        # Create the \"question\" field by concatenating context, query, and options\n",
    "        context = example['context']\n",
    "        query = example['query']\n",
    "        options = example['options']\n",
    "        options_str = \" \".join([f\"({chr(65 + i)}) {opt}\" for i, opt in enumerate(options)])\n",
    "        question = f\"{context} {query}\\n{options_str}\"\n",
    "        \n",
    "        # Create the dictionary for the current example\n",
    "        example_dict = {\n",
    "            \"id\": idx,\n",
    "            \"question\": question,\n",
    "            \"answer\": chr(65 + example['correct_option'])  # Convert index to letter (A, B, C, D)\n",
    "        }\n",
    "        \n",
    "        # Write the example as a JSON object to the JSONL file\n",
    "        f.write(json.dumps(example_dict) + '\\n')\n",
    "\n",
    "print(f\"Saved 300 examples to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grounded Visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total QA Pairs Parsed: 200\n",
      "Total Ground Truth Entries: 1319\n",
      "HTML content has been successfully written to multi_conv_GSM8K_llama8b.html\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "\n",
    "def add_color_to_tags_new(text):\n",
    "    # Find all unique tags in the text using regex\n",
    "    tags = set(re.findall(r'<([A-Za-z]+\\d*)>', text))\n",
    "\n",
    "    # Predefined color palette\n",
    "    color_palette = [\n",
    "        'lightyellow', 'lightblue', 'lightgreen', 'lightcoral',\n",
    "        'lightcyan', 'lightpink', 'lightsalmon', 'lightgray',\n",
    "        'lightgoldenrodyellow', 'lightseagreen', 'lightskyblue',\n",
    "        'lightsteelblue',\n",
    "        'lavender', 'peachpuff', 'paleturquoise', 'wheat', 'mistyrose'\n",
    "    ]\n",
    "\n",
    "    # Dictionary to hold tag-color mapping\n",
    "    tag_color_mapping = {}\n",
    "\n",
    "    # Assign colors to tags, cycling through the color palette if necessary\n",
    "    for i, tag in enumerate(sorted(tags)):\n",
    "        color = color_palette[i % len(color_palette)]\n",
    "        tag_color_mapping[tag] = color\n",
    "\n",
    "    # Function to replace tags with styled spans including class names\n",
    "    def replace_tag(match):\n",
    "        tag = match.group(1)\n",
    "        content = match.group(2)\n",
    "        color = tag_color_mapping.get(tag, 'lightgray')  # Default color if not found\n",
    "        return f'<span class=\"{tag}\" style=\"background-color: {color}; font-weight: bold;\">{content}</span>'\n",
    "\n",
    "    # Regex to find tags and replace them with styled spans\n",
    "    tag_regex = re.compile(r'<([A-Za-z]+\\d*)>\\s*([\\s\\S]*?)\\s*</\\1>')\n",
    "\n",
    "    # Replace all tags with styled spans\n",
    "    text = tag_regex.sub(replace_tag, text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def parse_csv_file(file_path):\n",
    "    qa_pairs = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            question = row.get('question', 'No question found.').strip()\n",
    "            answer_text = row.get('answer', 'No answer found.').strip()\n",
    "            id_ = row.get('id')\n",
    "            if id_ is not None:\n",
    "                try:\n",
    "                    id_int = int(id_)\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping a row due to invalid 'id' (not an integer): {id_}\")\n",
    "                    continue\n",
    "                qa_pairs.append((id_int, question, answer_text))\n",
    "            else:\n",
    "                # Handle cases without 'id' by skipping\n",
    "                print(f\"Skipping a row due to missing 'id': {row}\")\n",
    "    return qa_pairs\n",
    "\n",
    "\n",
    "def read_ground_truth(jsonl_path):\n",
    "    ground_truth = {}\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        # data_list = json.load(f)  # Load the entire JSON content\n",
    "        # for data in data_list:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            id_ = data.get('id')\n",
    "            answer = str(data.get('answer')).lower()\n",
    "            if id_ is not None and answer is not None:\n",
    "\n",
    "                # GSM ONLY\n",
    "                answer = answer.split('####')[1].strip()\n",
    "                ground_truth[id_] = answer\n",
    "            else:\n",
    "                print(f\"Invalid ground truth entry: {data}\")\n",
    "                \n",
    "            \n",
    "    return ground_truth\n",
    "\n",
    "# def read_ground_truth_option(jsonl_path):\n",
    "    ground_truth = {}\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        # data_list = json.load(f)  # Load the entire JSON content\n",
    "        # for data in data_list:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            id_ = data.get('id')\n",
    "            answer = str(data.get('answer')).lower()\n",
    "            options = data.get('options')\n",
    "            for option in options:\n",
    "                # print(option, answer)\n",
    "                if option[0].lower() == answer:\n",
    "                    answer = option\n",
    "                    print\n",
    "                    break\n",
    "            if id_ is not None and answer is not None:\n",
    "\n",
    "                # GSM ONLY\n",
    "                # answer = answer.split('####')[1].strip()\n",
    "                # ground_truth[id_] = answer\n",
    "                numbers_only = re.sub(r'[^0-9]', '', answer)\n",
    "                ground_truth[id_] = numbers_only\n",
    "            else:\n",
    "                print(f\"Invalid ground truth entry: {data}\")\n",
    "    return ground_truth\n",
    "\n",
    "def create_highlight_html_new(qa_pairs, ground_truth, ground_truth_option=None):\n",
    "    html_content = \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <title>Question and Answer Highlights</title>\n",
    "        <style>\n",
    "            body {\n",
    "                font-family: Arial, sans-serif;\n",
    "                margin: 20px;\n",
    "                background-color: #f0f0f0;\n",
    "            }\n",
    "            .container {\n",
    "                background-color: #ffffff;\n",
    "                padding: 20px;\n",
    "                margin-bottom: 20px;\n",
    "                border-radius: 8px;\n",
    "                box-shadow: 0 2px 5px rgba(0,0,0,0.1);\n",
    "            }\n",
    "            .question {\n",
    "                font-size: 1.2em;\n",
    "                margin-bottom: 10px;\n",
    "            }\n",
    "            .full-response, .final-answer, .ground-truth-answer {\n",
    "                margin-bottom: 10px;\n",
    "                white-space: pre-wrap; /* Add this line to preserve newlines */\n",
    "            }\n",
    "            .final-answer {\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            .ground-truth-answer {\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            /* Styles for the highlighted spans */\n",
    "            .highlighted {\n",
    "                padding: 2px 4px;\n",
    "                border-radius: 3px;\n",
    "                display: inline-block;\n",
    "            }\n",
    "            /* Styles for the summary section */\n",
    "            .summary {\n",
    "                background-color: #e0ffe0;\n",
    "                padding: 15px;\n",
    "                border: 2px solid #00cc00;\n",
    "                border-radius: 8px;\n",
    "                font-size: 1.2em;\n",
    "                margin-top: 30px;\n",
    "            }\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "    <h1>Question and Answer Highlights</h1>\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize counters for correct and total answers\n",
    "    correct_answers = 0\n",
    "    total_answers = 0\n",
    "\n",
    "    for i, (id_, question, answer_text) in enumerate(qa_pairs, 1):\n",
    "        try:\n",
    "            full_response = answer_text.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot process answer for question ID {id_}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Apply color to tags in the full_response\n",
    "        highlighted_response = add_color_to_tags_new(full_response)\n",
    "        \n",
    "        # Replace newline characters with <br> tags to ensure they are rendered in HTML\n",
    "        highlighted_response = highlighted_response.replace('\\n', '<br>')\n",
    "\n",
    "        # Extract the final answer within curly brackets {}\n",
    "        # final_answer_match = re.search(r'\\{([^}]+)\\}', full_response)\n",
    "        final_answer_match = re.search(r'\\{([^}]+)\\}(?=[^}]*$)', full_response, re.DOTALL)\n",
    "        \n",
    "        if final_answer_match:\n",
    "            final_answer = final_answer_match.group(1).replace(',', '').replace('$', '').strip().lower()\n",
    "            # Currentyl removes everything excecpt numbers\n",
    "            extracted = final_answer_match.group(1)\n",
    "            final_answer = re.sub(r'[^\\d.]', '', extracted)\n",
    "            \n",
    "            # only get first letter for mcq\n",
    "            # final_answer_mcq = final_answer[0]\n",
    "            if \"no\" in final_answer or \"false\" in final_answer:\n",
    "                final_answer = \"false\"\n",
    "            elif \"yes\" in final_answer or \"true\" in final_answer:\n",
    "                final_answer = \"true\"\n",
    "            \n",
    "        else:\n",
    "            final_answer = \"\"\n",
    "\n",
    "        # Retrieve ground truth answer\n",
    "        gt_answer = str(ground_truth.get(id_))\n",
    "        gt_answer = gt_answer.replace(',', '').replace('$', '')\n",
    "        # print(f\"id: {id_}: {gt_answer}\")\n",
    "        if gt_answer is None:\n",
    "            gt_answer_display = \"<span style='color: gray;'>Ground truth not available.</span>\"\n",
    "            is_correct = False\n",
    "        else:\n",
    "            # option = ground_truth_option.get(id_)\n",
    "            # In case conversion fails, fallback to string comparison\n",
    "            is_correct = final_answer == gt_answer\n",
    "            final_answer_display = final_answer\n",
    "            gt_answer_display = gt_answer\n",
    "\n",
    "        # Style the final answer based on correctness\n",
    "        if is_correct:\n",
    "            highlighted_final_answer = f\"<span style='font-size:1.1em; color: green;'>{final_answer_display}</span>\"\n",
    "            correct_answers += 1\n",
    "        else:\n",
    "            highlighted_final_answer = f\"<span style='font-size:1.1em; color: red;'>{final_answer_display}</span>\"\n",
    "        total_answers += 1\n",
    "\n",
    "        # Display ground truth answer\n",
    "        if gt_answer is not None:\n",
    "            ground_truth_html = f\"<div class='ground-truth-answer'><strong>Ground Truth Answer:</strong> {gt_answer_display}</div>\"\n",
    "        else:\n",
    "            ground_truth_html = f\"<div class='ground-truth-answer'><strong>Ground Truth Answer:</strong> Not available.</div>\"\n",
    "\n",
    "        # Build the HTML structure\n",
    "        html_content += f\"<div class='container'>\"\n",
    "        html_content += f\"<div class='question'><strong>Question:</strong> {question}</div>\"\n",
    "        html_content += f\"<div class='full-response'>{highlighted_response}</div>\"\n",
    "        html_content += f\"<div class='final-answer'><strong>Final Answer:</strong> {highlighted_final_answer}</div>\"\n",
    "        html_content += f\"{ground_truth_html}\"\n",
    "        html_content += \"</div>\\n\"\n",
    "\n",
    "    # After processing all QA pairs, add the summary section\n",
    "    summary_percentage = (correct_answers / total_answers * 100) if total_answers > 0 else 0\n",
    "    summary_html = f\"\"\"\n",
    "    <div class='summary'>\n",
    "        <strong>Summary:</strong> Correct Answers: {correct_answers} / {total_answers} ({summary_percentage:.2f}%)\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    # Close the HTML tags\n",
    "    html_content += \"\"\"\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    output_html = summary_html + html_content\n",
    "    return output_html\n",
    "\n",
    "\n",
    "def main():\n",
    "    input_csv = '/Users/log/Github/textual_grounding/logan/results/GSM8K/llama3.1/grounded_fact/multi_convo_None_llama3.1_1028_224532.csv'  \n",
    "    ground_truth_file = '/Users/log/Github/textual_grounding/data/GSM8K/test.jsonl'  \n",
    "    output_file = 'multi_conv_GSM8K_llama8b.html' \n",
    "\n",
    "    # Check if input files exist\n",
    "    if not os.path.isfile(input_csv):\n",
    "        print(f\"Input CSV file not found: {input_csv}\")\n",
    "        return\n",
    "    if not os.path.isfile(ground_truth_file):\n",
    "        print(f\"Ground truth JSON file not found: {ground_truth_file}\")\n",
    "        return\n",
    "\n",
    "    # Parse the input CSV file to extract IDs, questions, and answers\n",
    "    qa_pairs = parse_csv_file(input_csv)\n",
    "    print(f\"Total QA Pairs Parsed: {len(qa_pairs)}\")  # Debug: Print the number of QA pairs parsed\n",
    "\n",
    "    # Read the ground truth answers\n",
    "    ground_truth = read_ground_truth(ground_truth_file)\n",
    "    # ground_truth_options = read_ground_truth_option(ground_truth_file)\n",
    "    # print(ground_truth)\n",
    "    # print(ground_truth_options)\n",
    "    # ground_truth_options = None\n",
    "    print(f\"Total Ground Truth Entries: {len(ground_truth)}\")  # Debug: Print the number of ground truth entries\n",
    "\n",
    "    # Check if any QA pairs were found\n",
    "    if not qa_pairs:\n",
    "        print(\"No question-answer pairs were found in the input file.\")\n",
    "        return\n",
    "\n",
    "    # Generate the HTML content\n",
    "    html_content = create_highlight_html_new(qa_pairs, ground_truth)\n",
    "\n",
    "    # Write the HTML content to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(html_content)\n",
    "\n",
    "    print(f\"HTML content has been successfully written to {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoT - Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total QA Pairs Parsed: 25\n",
      "Total Ground Truth Entries: 3591\n",
      "HTML content has been successfully written to extract_facts_SPARTQA_4o.html\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_final_answer(answer_text):\n",
    "    # Regex pattern to match anything inside curly braces\n",
    "    final_answer_pattern = re.compile(r'\\{([^}]+)\\}')\n",
    "    \n",
    "    # Find all matches of text inside curly braces\n",
    "    matches = list(final_answer_pattern.finditer(answer_text))\n",
    "    \n",
    "    # If we have at least one match, get the content of the last match\n",
    "    if matches:\n",
    "        final_answer = matches[-1].group(1).strip()\n",
    "        if final_answer[-1] == '\\\\':\n",
    "            final_answer = final_answer[:-1]\n",
    "        return final_answer\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def parse_csv_file(file_path):\n",
    "    qa_pairs = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            question = row.get('question', 'No question found.').strip()\n",
    "            answer_text = row.get('answer', 'No answer found.').strip()\n",
    "            id_ = row.get('id')\n",
    "            if id_ is not None:\n",
    "                try:\n",
    "                    id_int = int(id_)\n",
    "                    qa_pairs.append((id_int, question, answer_text))\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping a row due to invalid 'id' (not an integer): {id_}\")\n",
    "            else:\n",
    "                print(f\"Skipping a row due to missing 'id': {row}\")\n",
    "    return qa_pairs\n",
    "\n",
    "def read_ground_truth(jsonl_path):\n",
    "    ground_truth = {}\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            id_ = data.get('id')\n",
    "            answer = data.get('answer')\n",
    "            if id_ is not None and answer is not None:\n",
    "                # GSM ONLY\n",
    "                # answer = answer.split('####')[1].strip()\n",
    "                ground_truth[id_] = answer\n",
    "            else:\n",
    "                print(f\"Invalid ground truth entry: {data}\")\n",
    "    return ground_truth\n",
    "\n",
    "def create_simple_html(qa_pairs, ground_truth):\n",
    "    html_content = \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <title>Question and Answer Comparison</title>\n",
    "        <style>\n",
    "            body {\n",
    "                font-family: Arial, sans-serif;\n",
    "                margin: 20px;\n",
    "                background-color: #f9f9f9;\n",
    "            }\n",
    "            .container {\n",
    "                background-color: #ffffff;\n",
    "                padding: 15px 20px;\n",
    "                margin-bottom: 15px;\n",
    "                border-radius: 6px;\n",
    "                box-shadow: 0 1px 3px rgba(0,0,0,0.1);\n",
    "            }\n",
    "            .question {\n",
    "                font-size: 1.1em;\n",
    "                margin-bottom: 10px;\n",
    "                color: #333333;\n",
    "            }\n",
    "            .answer-text {\n",
    "                background-color: #f4f4f4;\n",
    "                padding: 10px;\n",
    "                border-left: 4px solid #2196F3;\n",
    "                margin-bottom: 10px;\n",
    "                white-space: pre-wrap;\n",
    "                font-family: Consolas, \"Courier New\", monospace;\n",
    "            }\n",
    "            .final-answer, .ground-truth-answer {\n",
    "                margin-bottom: 5px;\n",
    "            }\n",
    "            .final-answer span.correct {\n",
    "                color: green;\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            .final-answer span.incorrect {\n",
    "                color: red;\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            .ground-truth-answer {\n",
    "                color: #555555;\n",
    "            }\n",
    "            .summary {\n",
    "                background-color: #e0ffe0;\n",
    "                padding: 15px;\n",
    "                border: 2px solid #00cc00;\n",
    "                border-radius: 8px;\n",
    "                font-size: 1.2em;\n",
    "                margin-top: 30px;\n",
    "            }\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "    <h1>Question and Answer Comparison</h1>\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize counters for correct and total answers\n",
    "    correct_answers = 0\n",
    "    total_answers = 0\n",
    "\n",
    "    for id_, question, answer_text in qa_pairs:\n",
    "        final_answer = extract_final_answer(answer_text)\n",
    "        gt_answer = ground_truth.get(id_)\n",
    "\n",
    "        if gt_answer is None:\n",
    "            gt_answer_display = \"<span style='color: gray;'>Ground truth not available.</span>\"\n",
    "            is_correct = False\n",
    "        else:\n",
    "            # Normalize both final_answer and gt_answer for comparison\n",
    "            try:\n",
    "                final_answer_num = float(final_answer.replace(',', '').replace('$', ''))\n",
    "                if isinstance(gt_answer, list):  # Handle list of answers if applicable\n",
    "                    gt_answer_num = float(gt_answer[0].replace(',', '').replace('$', ''))\n",
    "                else:\n",
    "                    gt_answer_num = float(str(gt_answer).replace(',', '').replace('$', ''))\n",
    "                is_correct = final_answer_num == gt_answer_num\n",
    "                # Format numbers with commas and two decimal places if needed\n",
    "                final_answer_display = f\"{final_answer_num:,.2f}\" if not final_answer_num.is_integer() else f\"{int(final_answer_num):,}\"\n",
    "                gt_answer_display = f\"{gt_answer_num:,.2f}\" if not gt_answer_num.is_integer() else f\"{int(gt_answer_num):,}\"\n",
    "            except (ValueError, TypeError):\n",
    "                # Fallback to string comparison if conversion fails\n",
    "                is_correct = final_answer.strip().lower() == str(gt_answer).strip().lower()\n",
    "                final_answer_display = final_answer\n",
    "                gt_answer_display = gt_answer\n",
    "\n",
    "        # Style the final answer based on correctness\n",
    "        if is_correct:\n",
    "            final_answer_html = f\"<span class='correct'>{final_answer_display}</span>\"\n",
    "            correct_answers += 1\n",
    "        else:\n",
    "            final_answer_html = f\"<span class='incorrect'>{final_answer_display}</span>\"\n",
    "        total_answers += 1\n",
    "\n",
    "        # Display ground truth answer\n",
    "        if gt_answer is not None:\n",
    "            ground_truth_html = f\"<div class='ground-truth-answer'><strong>Ground Truth Answer:</strong> {gt_answer_display}</div>\"\n",
    "        else:\n",
    "            ground_truth_html = f\"<div class='ground-truth-answer'><strong>Ground Truth Answer:</strong> Not available.</div>\"\n",
    "\n",
    "        # Build the HTML structure for each QA pair\n",
    "        html_content += f\"<div class='container'>\"\n",
    "        html_content += f\"<div class='question'><strong>Question:</strong> {question}</div>\"\n",
    "        html_content += f\"<div class='answer-text'><strong>Model Response:</strong><br>{answer_text}</div>\"\n",
    "        html_content += f\"<div class='final-answer'><strong>Final Answer:</strong> {final_answer_html}</div>\"\n",
    "        html_content += f\"{ground_truth_html}\"\n",
    "        html_content += \"</div>\\n\"\n",
    "\n",
    "    # Add the summary section\n",
    "    summary_percentage = (correct_answers / total_answers * 100) if total_answers > 0 else 0\n",
    "    summary_html = f\"\"\"\n",
    "    <div class='summary'>\n",
    "        <strong>Summary:</strong> Correct Answers: {correct_answers} / {total_answers} ({summary_percentage:.2f}%)\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    # html_content += summary_html\n",
    "\n",
    "    # Close the HTML tags\n",
    "    html_content += \"\"\"\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    final_content = summary_html + html_content\n",
    "    return final_content\n",
    "\n",
    "def main():\n",
    "    input_csv = '/Users/log/Github/textual_grounding/logan/results/logical_deduction_seven_objects/4o/grounded_fact/multi_convo_None_4o_1028_200316.csv'  \n",
    "    ground_truth_file = '/Users/log/Github/textual_grounding/data/logical_deduction_seven_objects/test.json'  # Path to the ground truth JSONL file\n",
    "    output_file = 'extract_facts_SPARTQA_4o.html'  # Desired output HTML file path\n",
    "\n",
    "    # Check if input files exist\n",
    "    if not os.path.isfile(input_csv):\n",
    "        print(f\"Input CSV file not found: {input_csv}\")\n",
    "        return\n",
    "    if not os.path.isfile(ground_truth_file):\n",
    "        print(f\"Ground truth JSONL file not found: {ground_truth_file}\")\n",
    "        return\n",
    "\n",
    "    # Parse the input CSV file to extract IDs, questions, and answers\n",
    "    qa_pairs = parse_csv_file(input_csv)\n",
    "    print(f\"Total QA Pairs Parsed: {len(qa_pairs)}\")\n",
    "\n",
    "    # Read the ground truth answers\n",
    "    ground_truth = read_ground_truth(ground_truth_file)\n",
    "    print(f\"Total Ground Truth Entries: {len(ground_truth)}\")\n",
    "\n",
    "    # Check if any QA pairs were found\n",
    "    if not qa_pairs:\n",
    "        print(\"No question-answer pairs were found in the input file.\")\n",
    "        return\n",
    "\n",
    "    # Generate the HTML content\n",
    "    html_content = create_simple_html(qa_pairs, ground_truth)\n",
    "\n",
    "    # Write the HTML content to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(html_content)\n",
    "\n",
    "    print(f\"HTML content has been successfully written to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"tasksource/spartqa-mchoice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total QA Pairs Parsed: 200\n",
      "No ground truth answer found for ID 489\n",
      "No ground truth answer found for ID 1113\n",
      "Total Ground Truth Entries: 1317\n",
      "\n",
      "===== Analysis Statistics =====\n",
      "\n",
      "Total Responses Analyzed: 200\n",
      "Responses with Final Answer in Curly Brackets: 136 (68.00%)\n",
      "Responses without Final Answer in Curly Brackets: 64 (32.00%)\n",
      "Responses with Ground Truth Available: 200 (100.00%)\n",
      "Correct Answers: 94\n",
      "Incorrect Answers: 106\n",
      "Accuracy: 47.00%\n",
      "Responses without Ground Truth: 0\n",
      "\n",
      "----- Tag Statistics -----\n",
      "Total Tags Found: 501\n",
      "Average Number of Tags per Response: 2.50\n",
      "Average Length of Tag Content: 8.68 characters\n",
      "--------------------------\n",
      "\n",
      "===== End of Statistics =====\n",
      "\n",
      "Statistics analysis completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import json  # For handling JSONL\n",
    "import os\n",
    "\n",
    "def extract_parts_regular_cot(answer_text):\n",
    "    # Attempt to extract Final Answer from 'Final Answer:'\n",
    "    final_match = re.search(r'Final Answer:\\s*(\\S+)', answer_text, re.IGNORECASE)\n",
    "    if final_match and final_match.group(1).strip():\n",
    "        final_answer = final_match.group(1).strip()\n",
    "        has_curly = False\n",
    "    else:\n",
    "        # Fallback: Extract Final Answer from '{...}' in the reasoning\n",
    "        curly_match = re.search(r'\\{([\\d.]+)\\}', answer_text)\n",
    "        final_answer = curly_match.group(1).strip() if curly_match else \"\"\n",
    "        has_curly = bool(curly_match)\n",
    "\n",
    "    return answer_text.strip(), final_answer, has_curly\n",
    "\n",
    "def parse_csv_file(file_path):\n",
    "    qa_pairs = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            question = row.get('question', 'No question found.').strip()\n",
    "            answer_text = row.get('answer', 'No answer found.').strip()\n",
    "            id_ = row.get('id')\n",
    "            if id_ is not None:\n",
    "                try:\n",
    "                    id_int = int(id_)\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping a row due to invalid 'id' (not an integer): {id_}\")\n",
    "                    continue\n",
    "                qa_pairs.append((id_int, question, answer_text))\n",
    "            else:\n",
    "                # Handle cases without 'id' by skipping\n",
    "                print(f\"Skipping a row due to missing 'id': {row}\")\n",
    "    return qa_pairs\n",
    "\n",
    "def read_ground_truth(jsonl_path):\n",
    "    ground_truth = {}\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            id_ = data.get('id')\n",
    "            answer = data.get('answer')\n",
    "            if id_ is not None and answer is not None:\n",
    "                # Extract the last number or text after '####'\n",
    "                match = re.search(r'####\\s*([\\d.]+)', answer)\n",
    "                if match:\n",
    "                    ground_truth[id_] = match.group(1).strip()\n",
    "                else:\n",
    "                    print(f\"No ground truth answer found for ID {id_}\")\n",
    "            else:\n",
    "                print(f\"Invalid ground truth entry: {data}\")\n",
    "    return ground_truth\n",
    "\n",
    "def create_statistics(qa_pairs, ground_truth):\n",
    "    total_responses = len(qa_pairs)\n",
    "    responses_with_curly = 0\n",
    "    responses_without_curly = 0\n",
    "    correct_answers = 0\n",
    "    incorrect_answers = 0\n",
    "    no_ground_truth = 0\n",
    "\n",
    "    # Variables for tag statistics\n",
    "    total_tags = 0\n",
    "    total_tag_length = 0\n",
    "    tag_counts = []  # List to store number of tags per response\n",
    "    tag_lengths = []  # List to store lengths of tag content across all responses\n",
    "\n",
    "    for id_, question, answer_text in qa_pairs:\n",
    "        try:\n",
    "            answer_reasoning, final_answer, has_curly = extract_parts_regular_cot(answer_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot extract parts for question ID {id_}: {e}\")\n",
    "            continue\n",
    "\n",
    "        if has_curly:\n",
    "            responses_with_curly += 1\n",
    "        else:\n",
    "            responses_without_curly += 1\n",
    "\n",
    "        # Extract tags and their content\n",
    "        tags_in_response = re.findall(r'<([A-Za-z]+\\d*)>(.*?)</\\1>', answer_text)\n",
    "        number_of_tags = len(tags_in_response)\n",
    "        tag_counts.append(number_of_tags)\n",
    "        total_tags += number_of_tags\n",
    "\n",
    "        for tag, content in tags_in_response:\n",
    "            content_length = len(content)\n",
    "            tag_lengths.append(content_length)\n",
    "            total_tag_length += content_length\n",
    "\n",
    "        # Retrieve ground truth answer\n",
    "        gt_answer = ground_truth.get(id_)\n",
    "        if gt_answer is None:\n",
    "            no_ground_truth += 1\n",
    "            continue\n",
    "\n",
    "        # Compare final_answer with ground truth\n",
    "        if final_answer == gt_answer:\n",
    "            correct_answers += 1\n",
    "        else:\n",
    "            incorrect_answers += 1\n",
    "\n",
    "    # Calculate additional metrics\n",
    "    accuracy_percentage = (correct_answers / (correct_answers + incorrect_answers) * 100) if (correct_answers + incorrect_answers) > 0 else 0\n",
    "    curly_percentage = (responses_with_curly / total_responses * 100) if total_responses > 0 else 0\n",
    "    no_curly_percentage = (responses_without_curly / total_responses * 100) if total_responses > 0 else 0\n",
    "    ground_truth_available = total_responses - no_ground_truth\n",
    "    ground_truth_available_percentage = (ground_truth_available / total_responses * 100) if total_responses > 0 else 0\n",
    "\n",
    "    # Calculate tag statistics\n",
    "    average_tags_per_response = (total_tags / total_responses) if total_responses > 0 else 0\n",
    "    average_tag_length = (total_tag_length / total_tags) if total_tags > 0 else 0\n",
    "\n",
    "    # Print the statistics\n",
    "    print(\"\\n===== Analysis Statistics =====\\n\")\n",
    "    print(f\"Total Responses Analyzed: {total_responses}\")\n",
    "    print(f\"Responses with Final Answer in Curly Brackets: {responses_with_curly} ({curly_percentage:.2f}%)\")\n",
    "    print(f\"Responses without Final Answer in Curly Brackets: {responses_without_curly} ({no_curly_percentage:.2f}%)\")\n",
    "    print(f\"Responses with Ground Truth Available: {ground_truth_available} ({ground_truth_available_percentage:.2f}%)\")\n",
    "    print(f\"Correct Answers: {correct_answers}\")\n",
    "    print(f\"Incorrect Answers: {incorrect_answers}\")\n",
    "    print(f\"Accuracy: {accuracy_percentage:.2f}%\")\n",
    "    print(f\"Responses without Ground Truth: {no_ground_truth}\")\n",
    "\n",
    "    # Tag Statistics\n",
    "    print(\"\\n----- Tag Statistics -----\")\n",
    "    print(f\"Total Tags Found: {total_tags}\")\n",
    "    print(f\"Average Number of Tags per Response: {average_tags_per_response:.2f}\")\n",
    "    print(f\"Average Length of Tag Content: {average_tag_length:.2f} characters\")\n",
    "    print(\"--------------------------\\n\")\n",
    "    print(\"===== End of Statistics =====\\n\")\n",
    "\n",
    "def main():\n",
    "    input_csv = '/Users/log/Github/textual_grounding/logan/results/GSM8K/llama/mermaid/mermaid_get_answer_llama3.1_20240926_215344.csv'  # Replace with your input CSV file path\n",
    "    ground_truth_file = '/Users/log/Github/textual_grounding/data/GSM8K/test.jsonl'  # Path to the ground truth JSONL file\n",
    "\n",
    "    # Check if input files exist\n",
    "    if not os.path.isfile(input_csv):\n",
    "        print(f\"Input CSV file not found: {input_csv}\")\n",
    "        return\n",
    "    if not os.path.isfile(ground_truth_file):\n",
    "        print(f\"Ground truth JSONL file not found: {ground_truth_file}\")\n",
    "        return\n",
    "\n",
    "    # Parse the input CSV file to extract IDs, questions, and answers\n",
    "    qa_pairs = parse_csv_file(input_csv)\n",
    "    print(f\"Total QA Pairs Parsed: {len(qa_pairs)}\")  # Debug: Print the number of QA pairs parsed\n",
    "\n",
    "    # Read the ground truth answers\n",
    "    ground_truth = read_ground_truth(ground_truth_file)\n",
    "    print(f\"Total Ground Truth Entries: {len(ground_truth)}\")  # Debug: Print the number of ground truth entries\n",
    "\n",
    "    # Check if any QA pairs were found\n",
    "    if not qa_pairs:\n",
    "        print(\"No question-answer pairs were found in the input file.\")\n",
    "        return\n",
    "\n",
    "    # Generate and print the statistics\n",
    "    create_statistics(qa_pairs, ground_truth)\n",
    "\n",
    "    print(\"Statistics analysis completed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 'prompt' is a list in entry ID 110. Attempting to join into a string.\n",
      "Warning: 'prompt' is a list in entry ID 111. Attempting to join into a string.\n",
      "Warning: 'prompt' is a list in entry ID 112. Attempting to join into a string.\n",
      "Warning: 'prompt' is a list in entry ID 113. Attempting to join into a string.\n",
      "JSON file has been updated successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Define the path to your JSON file\n",
    "input_file = '/Users/log/Github/textual_grounding/data/AIW/test.json'\n",
    "\n",
    "# Load the JSON data from the file\n",
    "with open(input_file, 'r', encoding='utf-8') as file:\n",
    "    try:\n",
    "        data = json.load(file)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "# Process each entry in the JSON data\n",
    "for idx, entry in enumerate(data):\n",
    "    # Get the current prompt\n",
    "    prompt = entry.get('prompt', '')\n",
    "\n",
    "    # Check if prompt is a string\n",
    "    if isinstance(prompt, str):\n",
    "        delimiter = 'have?'\n",
    "        index = prompt.find(delimiter)\n",
    "\n",
    "        if index != -1:\n",
    "            # Truncate the prompt after \"have?\"\n",
    "            truncated_prompt = prompt[:index + len(delimiter)]\n",
    "            entry['prompt'] = truncated_prompt\n",
    "        else:\n",
    "            print(f\"Warning: 'have?' not found in prompt of entry ID {entry.get('id', 'Unknown')}. Prompt left unchanged.\")\n",
    "    elif isinstance(prompt, list):\n",
    "        print(f\"Warning: 'prompt' is a list in entry ID {entry.get('id', 'Unknown')}. Attempting to join into a string.\")\n",
    "        # Attempt to join the list into a single string\n",
    "        joined_prompt = ' '.join(str(item) for item in prompt)\n",
    "        delimiter = 'have?'\n",
    "        index = joined_prompt.find(delimiter)\n",
    "\n",
    "        if index != -1:\n",
    "            truncated_prompt = joined_prompt[:index + len(delimiter)]\n",
    "            entry['prompt'] = truncated_prompt\n",
    "        else:\n",
    "            print(f\"Warning: 'have?' not found after joining prompt in entry ID {entry.get('id', 'Unknown')}. Prompt left unchanged.\")\n",
    "    else:\n",
    "        print(f\"Warning: 'prompt' is neither a string nor a list in entry ID {entry.get('id', 'Unknown')}. Prompt left unchanged.\")\n",
    "\n",
    "    # Rename 'right_answer' to 'answer' if it exists\n",
    "    if 'right_answer' in entry:\n",
    "        entry['answer'] = entry.pop('right_answer')\n",
    "\n",
    "# Save the updated data back to the same JSON file\n",
    "with open(input_file, 'w', encoding='utf-8') as file:\n",
    "    json.dump(data, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"JSON file has been updated successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
