{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# import ollama\n",
    "# import google.generativeai as genai\n",
    "import anthropic\n",
    "# import ollama\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from google.generativeai.types import RequestOptions\n",
    "from google.api_core import retry\n",
    "from typing import List, Tuple\n",
    "import json\n",
    "import datetime\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "import visualize\n",
    "import pandas as pd\n",
    "from utils.utils import add_color_to_tags, extract_parts_0, extract_parts_1\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant tasked with identifying and tagging key facts in the text that are required to answer the final question. Your goal is to analyze the input question, identify distinct key points needed to answer the question, and then wrap each of these points in custom HTML-like tags like <fact1>, <fact2>, <fact3>, and so on. After you have tagged the \n",
    "\n",
    "Instructions:\n",
    "\n",
    "1. **Read and Understand the Input Question**:\n",
    "   - Carefully analyze the input question to understand its context.\n",
    "   - Identify distinct facts, entities, or concepts that contribute to the meaning of the question.\n",
    "\n",
    "2. **Identify Key Points (Facts)**:\n",
    "   - Each distinct and meaningful segment of the question that provides important information should be considered a \"fact.\"\n",
    "   - This can include the subject, object, context, or any qualifiers that make the question specific.\n",
    "\n",
    "3. **Tag Each Fact**:\n",
    "   - Assign a unique tag to each fact in the form <fact1>, <fact2>, etc.\n",
    "   - Wrap each identified key point in these tags.\n",
    "   - The tags should start from <fact1> for the first key point, and increment for each new fact identified.\n",
    "\n",
    "4. **Formatting Requirements**:\n",
    "   - Maintain the original structure of the question as much as possible.\n",
    "   - Make sure each tag encapsulates the entire key point clearly without splitting phrases unnecessarily.\n",
    "\n",
    "### Example\n",
    "\n",
    "#### Input Question:\n",
    "\"How does climate change affect polar bear populations in the Arctic?\"\n",
    "\n",
    "#### Step-by-Step Identification:\n",
    "1. Fact 1: \"Climate change\" (main topic causing the effect)\n",
    "2. Fact 2: \"Polar bear populations\" (who is affected)\n",
    "3. Fact 3: \"In the Arctic\" (the location/context)\n",
    "\n",
    "#### Reformatted Output:\n",
    "\"How does <fact1>climate change</fact1> affect <fact2>polar bear populations</fact2> in <fact3>the Arctic</fact3>?\"\n",
    "\n",
    "### Additional Guidelines\n",
    "\n",
    "- If there are any ambiguous segments that could either be tagged separately or together, prefer grouping them if it preserves the original meaning better.\n",
    "- Do not tag common words or phrases unless they contain specific, factual information.\n",
    "- The output should be clean and readable, with each tag highlighting an important component of the question.\n",
    "\n",
    "### Examples for Practice\n",
    "\n",
    "1. **Input**: \"What are the main causes of deforestation in the Amazon rainforest?\"\n",
    "   - **Output**: \"What are the <fact1>main causes</fact1> of <fact2>deforestation</fact2> in the <fact3>Amazon rainforest</fact3>?\"\n",
    "\n",
    "2. **Input**: \"When was the first artificial satellite launched by the Soviet Union?\"\n",
    "   - **Output**: \"When was the <fact1>first artificial satellite</fact1> launched by the <fact2>Soviet Union</fact2>?\"\n",
    "\n",
    "3. **Input**: \"Why is regular exercise important for cardiovascular health?\"\n",
    "   - **Output**: \"Why is <fact1>regular exercise</fact1> important for <fact2>cardiovascular health</fact2>?\"\n",
    "\n",
    "### Edge Cases\n",
    "\n",
    "- **Simple Questions**: For questions with only one main fact, use just one <fact1> tag.\n",
    "- **Complex Questions**: For longer questions, ensure that the tagging remains logical and does not clutter the text.\n",
    "\n",
    "### Final Output Format\n",
    "\n",
    "Ensure that the final output is:\n",
    "- Grammatically correct.\n",
    "- Properly formatted with each key point enclosed in <fact> tags.\n",
    "- Consistent with the original meaning of the input question.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(save_path: str, ids: List[str], questions: List[str], answers: List[str], append: bool = False):\n",
    "    \"\"\"\n",
    "    Saves the results to a CSV file. If append is True and the file exists, it appends without headers.\n",
    "    Otherwise, it writes a new file with headers.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({'id': ids, 'question': questions, 'answer': answers})\n",
    "    if append and os.path.exists(save_path):\n",
    "        df.to_csv(save_path, mode='a', index=False, header=False)\n",
    "    else:\n",
    "        df.to_csv(save_path, index=False)\n",
    "\n",
    "def read_jsonl_file(filepath: str) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Reads a JSONL file and returns a list of JSON objects.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            json_obj = json.loads(line)\n",
    "            data.append(json_obj)\n",
    "    return data\n",
    "\n",
    "def get_prompt(prompt_type: str, few_shot_prompt: str, question: str) -> str:\n",
    "    \"\"\"\n",
    "    Constructs the prompt based on the prompt type.\n",
    "    \"\"\"\n",
    "    prompts = {\n",
    "        \"cot\": f\"{few_shot_prompt}\\n{question}\\nPlease generate your explanation first, then generate the answer in the bracket as follow:\\n\" +\"Answer: {}\",\n",
    "        \"log_cot_mcq\": f\"{few_shot_prompt}\\n{question}\\nThink through your answer step by step and then choose the answer option that is the most correct. Then put your final answer in curly brackets. For example, Final_Answer:{{3}}\",\n",
    "        \"log_cot\": f\"{few_shot_prompt}\\n{question}\\nThink through your answer step by step. Then put your final answer in curly brackets. For example, Final_Answer:{{false}}. Your final answer should either be \\\"true\\\" or \\\"false\\\".\",\n",
    "        \"fs\": f\"{few_shot_prompt}\\n{question}\",\n",
    "        \"fs_inst\": f\"{few_shot_prompt}\\n{question}\\nI want you to answer this question but your explanation should contain references referring back to the information in the question. To do that, first, re-generate the question with proper tags and then generate your answers. The output format is as follow:\\n\\\n",
    "            Reformatted Question: \\\n",
    "                Answer:\",\n",
    "        \"zs\": f\"{question}\\nI want you to answer this question but your explanation should contain references referring back to the information in the question. To do that, first, re-generate the question with proper tags (<a>, <b>, <c>, etc) for refered information and then generate your answers that also have the tag (<a>, <b>, <c>, etc) for the grounded information. Give your answer by analyzing step by step, and give only numbers in the final answer. The output format is as follow:\\n\\\n",
    "            Reformatted Question: \\\n",
    "                Answer:\\\n",
    "                    Final answer:\",\n",
    "        \"fs_xml\": f\"{few_shot_prompt}\\n\\nRecreate the following question in the style of the correctly formatted examples shown previously. Make sure that your response has all its information inclosed in the proper <tags>. Begin your response with the <key_facts> section. Make sure that every fact in <key_facts> is very concise and contains a very short reference to the <question>. Do not include a <question> section in your response\\n\\n<question>\\n{question}\\n</question>\",\n",
    "        \"fs_log_inst\": f\"{few_shot_prompt}\\n\\n{question}\\nTo answer this question, your explanation should contain references referring back to the information in the question. To do that, first, re-generate the question with proper tags and then generate your answers based off the tags. Put your final answer in curly brackets e.g. Final_Answer: {{false}}. Your final answer should only be \\\"true\\\" or \\\"false\\\".\",\n",
    "        \"fs_clause_inst\": f\"{few_shot_prompt}\\n\\n{question}\\nTo answer this question, first regenerate the question with <fact> tags around each clause or phrase in the text. Each clause or phrase should be as concise as possible so that long sentences will be broken up into multiple segments. Then, to answer the original question, your explanation should contain references back to the information in the tagged question. After you have generated the reformatted question and your reasoning which contains references to the tagged reformatted question, put your answer in curly brackets e.g. Final_Answer: {{false}}. Your final answer should either be \\\"true\\\" or \\\"false\\\".\",\n",
    "        \"stripped_clause\": f\"{few_shot_prompt}\\n\\n{question}\\nTo answer this question, your explanation should contain references referring back to the information in the question. Generate your answers based off the tags in the question. Use the example answers as a guide for what your answer format should look like. The <fact> tags should be interweaved in the sentences in your reasoning. Put your final answer in curly brackets e.g. Final_Answer: {{false}}. Your final answer should either be \\\"true\\\" or \\\"false\\\".\",\n",
    "        \"mermaid_get_answer\": f\"{few_shot_prompt}\\n\\n Your job is to extract the key facts from a question relevant to answering the question. The facts should be represented in a hierarchal format through a mermaid diagram. Do not create duplicate facts across multiple branches that represent the same information. Create a mermaid diagram that represents the key facts in the following question. Then, use the nodes from this graph to cite specific facts in your answer reasoning. Put your final answer in curly brackets e.g. Final_Answer: {{30}} \\n\\nquestion: {question}\", \n",
    "    }\n",
    "    return prompts.get(prompt_type, \"\")\n",
    "\n",
    "# cycle through all the keys \n",
    "# def get_gemini_key(problem_id):\n",
    "#     GOOGLE_KEYS = [\n",
    "#         'AIzaSyBQ7zvIZoET3199GNhuz86vKagn_JCEOmk', # original - gen lang client\n",
    "#         'AIzaSyCEI-5U4z7-3q-uwlvkOrdT2e78aNmjnbg', # chat app\n",
    "#         'AIzaSyCvycd0yZZ4GSj47qDLk4JoPemvzUSfvio', # project 1\n",
    "#         'AIzaSyD5xNbDkaJMEMBpWEXYNq5SheF6omdKpzg', # project 2\n",
    "#         'AIzaSyAjcrp_otRjGsj0YvB1cUc2BMng6KSEZwU', # project 3\n",
    "#         'AIzaSyB43xEllzAqGJjz-ExIGadXpUQllQ6PiI4', # project 4\n",
    "#         'AIzaSyDDTCn4lKul4vMj9GmEGJBxZFHb6QZSoA8', # project 5\n",
    "#         'AIzaSyB1sNUXN9CNpRWwqQnwVBBzMF37kYCNOIY', # project 6\n",
    "#         'AIzaSyBqRruZh4d4jq8q6FtUci71nOkqcVlpNLM', # project 7\n",
    "#         'AIzaSyATMO-YWZX4qtMru-NKcodolGr_4kKme5U', # project 8\n",
    "#         'AIzaSyBbKx5spKBPS2tVaUje2Vc1e2v7T6ouUGc', # project 9\n",
    "#         'AIzaSyCPb6W1e7uNI6UoSDTkJRmvkNbl1Tzgpmg', # project 10\n",
    "#         'AIzaSyDqj50lzn-YYIZ92NID4MKgReTeSEJgZuk', # project 11 --\n",
    "#         'AIzaSyBXO1lqmulX82oJjgGh4EPWWcGunxlFjFg', # project 12\n",
    "#         'AIzaSyBt95gM49zINc5l0cZKy285wvtc-kTUTt0', # project 13\n",
    "#         'AIzaSyBf4ty3TH3UC0-TvE-UwhMcrYePZS8_lNs', # project 14\n",
    "#         'AIzaSyDOLgV0DQN7jQwvUbpYyr7jjz8TPYLzdDc', # project 15\n",
    "#     ]\n",
    "#     index = problem_id%len(GOOGLE_KEYS)\n",
    "#     print(f\"getting key from {index}\")\n",
    "#     key = GOOGLE_KEYS[index]\n",
    "#     return key\n",
    "\n",
    "# def query_gemini(prompt: str, problem_id) -> str:\n",
    "#     \"\"\"\n",
    "#     Queries the Gemini LLM with the given prompt and returns the response text.\n",
    "#     \"\"\"\n",
    "#     genai.configure(api_key=get_gemini_key(problem_id))\n",
    "#     model = genai.GenerativeModel('gemini-1.5-pro-latest')\n",
    "#     response = model.generate_content(prompt, request_options=RequestOptions(retry=retry.Retry(initial=20, multiplier=3, maximum=121, timeout=60)))\n",
    "#     text = response.candidates[0].content.parts[0].text\n",
    "#     return text\n",
    "\n",
    "def query_claude(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Queries the Claude LLM with the given prompt and returns the response text.\n",
    "    \"\"\"\n",
    "    client = anthropic.Anthropic(api_key=API_KEYS['claude'])\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20240620\",\n",
    "        max_tokens=1024,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.content[0].text\n",
    "\n",
    "def query_4o(prompt: str) -> str:\n",
    "    client = OpenAI()\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-2024-08-06\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{prompt}\"\n",
    "            }\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "def query_llm(llm_model: str, ids: List[str], questions: List[str], few_shot_prompt: str, prompt_type: str, save_path: str, already_answered_ids: set) -> Tuple[List[str], List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Queries the specified LLM for each question, skipping already answered ones.\n",
    "    Saves each response immediately after it's obtained.\n",
    "    Returns lists of answered IDs, questions, and answers.\n",
    "    \"\"\"\n",
    "    answers = []\n",
    "    ids_can_be_answered = []\n",
    "    questions_can_be_answered = []\n",
    "    \n",
    "    for id, q in tqdm(zip(ids, questions), total=len(ids)):\n",
    "        # print(f\"Processing ID: {id}\")\n",
    "        if id in already_answered_ids:\n",
    "            print(f\"Skipping already answered ID: {id}\")\n",
    "            continue\n",
    "        \n",
    "        prompt = get_prompt(prompt_type, few_shot_prompt, q)\n",
    "        try:\n",
    "            if llm_model == 'gemini':\n",
    "                answer = query_gemini(prompt, id)\n",
    "            elif llm_model == 'claude':\n",
    "                answer = query_claude(prompt)\n",
    "            elif llm_model == '4o':\n",
    "                answer = query_4o(prompt)\n",
    "            elif llm_model == 'llama3.1':\n",
    "                answer = ollama.generate(model='llama3.1', prompt=prompt)['response']\n",
    "                print(f\"Processed ID: {id}\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported LLM model: {llm_model}\")\n",
    "            # print(f\"Answer for ID {id}: {answer}\")\n",
    "            \n",
    "            # Append to lists\n",
    "            answers.append(answer)\n",
    "            questions_can_be_answered.append(q)\n",
    "            ids_can_be_answered.append(id)\n",
    "\n",
    "            # Save after each answer\n",
    "            save_results(save_path, [id], [q], [answer], append=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question {id}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return ids_can_be_answered, questions_can_be_answered, answers\n",
    "\n",
    "def load_data(data_path: str, sample_size: int = None) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Loads data from a JSONL file, optionally sampling a subset.\n",
    "    \"\"\"\n",
    "    data = read_jsonl_file(data_path)\n",
    "    print(f\"Loaded {len(data)} records from: {data_path}\")\n",
    "    if sample_size:\n",
    "        data = random.sample(data, sample_size)\n",
    "        print(f\"Sampled {sample_size} records.\")\n",
    "    questions = [x[\"question\"] for x in data]\n",
    "    ids = [x[\"id\"] for x in data]\n",
    "    return ids, questions\n",
    "\n",
    "def load_data_deterministic(data_path: str, sample_size: int = None) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Loads data from a JSONL file in a deterministic manner by sorting.\n",
    "    \"\"\"\n",
    "    data = read_jsonl_file(data_path)\n",
    "    print(f\"Loaded {len(data)} records from: {data_path}\")\n",
    "    if sample_size:\n",
    "        # Sort the data based on a consistent criterion (e.g., 'id' or 'question')\n",
    "        sorted_data = sorted(data, key=lambda x: x['id'])\n",
    "        # Take the first 'sample_size' items\n",
    "        data = sorted_data[:sample_size]\n",
    "        print(f\"Selected first {sample_size} records after sorting.\")\n",
    "    questions = [x[\"question\"] for x in data]\n",
    "    ids = [x[\"id\"] for x in data]\n",
    "    return ids, questions\n",
    "\n",
    "def load_data_size_specific(data_path: str, sample_size: int = 0):\n",
    "    data = read_jsonl_file(data_path)\n",
    "    # with open(data_path, 'r', encoding='utf-8') as file:\n",
    "    #     data = json.load(file)\n",
    "    random_data = data\n",
    "    print(random_data)\n",
    "    \n",
    "    # for idx, item in enumerate(random_data):\n",
    "    #     if 'id' not in item:\n",
    "    #         # Option 1: Use enumeration for simple integer IDs\n",
    "    #         item['id'] = idx + 1  # Starting IDs from 1\n",
    "    \n",
    "    question_length = 0 # 336  # 526 # 800\n",
    "    \n",
    "    questions = [x[\"question\"] for x in random_data if len(x[\"question\"]) >= question_length]\n",
    "    ids = [x[\"id\"] for x in random_data if len(x[\"question\"]) >= question_length]\n",
    "    return ids[:sample_size], questions[:sample_size]\n",
    "\n",
    "def load_few_shot_prompt(prompt_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Loads the few-shot prompt from a text file.\n",
    "    \"\"\"\n",
    "    with open(prompt_path, 'r') as file:\n",
    "        prompt = file.read()\n",
    "    # print(f\"Loaded few-shot prompt from: {prompt_path}\")\n",
    "    return prompt\n",
    "\n",
    "def load_already_answered_ids(save_path: str) -> set:\n",
    "    \"\"\"\n",
    "    Loads the set of IDs that have already been answered from the CSV file.\n",
    "    Returns an empty set if the file does not exist.\n",
    "    \"\"\"\n",
    "    if os.path.exists(save_path):\n",
    "        df = pd.read_csv(save_path)\n",
    "        answered_ids = set(df['id'].astype(int).tolist())\n",
    "        # print(f\"Loaded {len(answered_ids)} already answered IDs from: {save_path}\")\n",
    "        print(f\"Already answered IDs: {answered_ids}\")\n",
    "        return answered_ids\n",
    "    else:\n",
    "        print(f\"No existing save file found at: {save_path}. Starting fresh.\")\n",
    "        return set()\n",
    "\n",
    "def initialize_save_file(save_path: str):\n",
    "    \"\"\"\n",
    "    Initializes the CSV file with headers if it doesn't exist.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(save_path):\n",
    "        # Create an empty DataFrame with headers and save\n",
    "        df = pd.DataFrame(columns=['id', 'question', 'answer'])\n",
    "        df.to_csv(save_path, index=False)\n",
    "        print(f\"Initialized new save file with headers at: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 0, 'question': '<fact1>Is it common</fact1> <fact2>to see frost</fact2> <fact3>during some college commencements?</fact3>', 'answer': True, 'explanation': \"Yes. College commencement ceremonies often happen during the months of December, May, and sometimes June.  Frost isn't uncommon to see during the month of December, as it is the winter.\"}, {'id': 1, 'question': \"<fact1>Hydrogen's atomic number</fact1> <fact2>squared</fact2> <fact3>exceeds</fact3> <fact4>number of Spice Girls</fact4>?\", 'answer': False, 'explanation': 'No. Hydrogen is the first element and has an atomic number of one. To square a number, you multiply it by itself. The Spice Girls has five members.'}, {'id': 2, 'question': '<fact1>Would</fact1> <fact2>a pear</fact2> <fact3>sink</fact3> <fact4>in water?</fact4>', 'answer': False, 'explanation': 'No. The density of a raw pear is about 0.59 g/cm^3. The density of water is about 1 g/cm^3. Objects only sink if they are denser than the surrounding fluid.'}, {'id': 3, 'question': '<fact1>Do hamsters</fact1> <fact2>provide food</fact2> <fact3>for any animals?</fact3>', 'answer': True, 'explanation': 'Yes. Hamsters are prey animals. Prey animals provide food for predators.'}, {'id': 4, 'question': '<fact1>Are all limbs</fact1> <fact2>required</fact2> <fact3>for jujutsu?</fact3>', 'answer': False, 'explanation': 'No. Jujutsu is a Japanese martial art that uses unarmed combat to subdue opponents. Nick Newell, a congenital amputee, got his black belt after two straight submission wins. Fighter Aaron LaPointe has succeeded in martial arts with a fully paralyzed arm.'}, {'id': 5, 'question': '<fact1>Could a llama</fact1> <fact2>birth twice</fact2> <fact3>during War in Vietnam</fact3> <fact4>(1945-46)?</fact4>', 'answer': False, 'explanation': 'No. The War in Vietnam (1945-46) lasted around 6 months. The gestation period for a llama is 11 months.'}, {'id': 6, 'question': '<fact1>Does Linus Torvalds</fact1> <fact2>make money</fact2> <fact3>off of DirectX?</fact3>', 'answer': False, 'explanation': 'No. DirectX is a proprietary technology owned by Microsoft. Linus Torvalds is the creator and lead developer for the open-source Linux kernel. The Linux kernel is used in operating systems that are competitors of Microsoft Windows.'}, {'id': 7, 'question': '<fact1>Could Brooke Shields</fact1> <fact2>succeed</fact2> <fact3>at University of Pennsylvania?</fact3>', 'answer': True, 'explanation': 'Yes. Brooke Shields graduated from Princeton University. Princeton is ranked as the number 1 national college by US news. University of Pennsylvania is ranked as number 6 national college by US news. Princeton only admits around 6 percent of applicants as of 2018. University of Pennsylvania accepts around 9% of applicants as of 2018.'}, {'id': 8, 'question': '<fact1>Could a silverfish</fact1> <fact2>reach the top</fact2> <fact3>of the Empire State Building?</fact3>', 'answer': False, 'explanation': 'No. Silverfish cannot fly. Animals that cannot fly can only access objects at or near ground level without mechanical assistance. The top of the Empire State Building is \\t1,454 ft high.'}, {'id': 9, 'question': '<fact1>Are the majority</fact1> <fact2>of Reddit users</fact2> <fact3>familiar with</fact3> <fact4>the Pledge of Allegiance?</fact4>', 'answer': True, 'explanation': 'Yes. 55% of the Reddit user base comes from the United States. Congressional sessions open with the recital of the Pledge, as do many government meetings at local levels, and meetings held by many private organizations. All states except California, Hawaii, Iowa, Vermont, and Wyoming require a regularly scheduled recitation of the pledge in public schools.'}, {'id': 10, 'question': '<fact1>Do the anchors</fact1> <fact2>on Rede Globo</fact2> <fact3>speak Chinese?</fact3>', 'answer': False, 'explanation': 'No. Rede Globo is a Brazilian television network. The official language of Brazil is Portuguese.'}, {'id': 11, 'question': '<fact1>Can</fact1> <fact2>Reiki</fact2> <fact3>be stored</fact3> <fact4>in a bottle?</fact4>', 'answer': False, 'explanation': 'No. Reiki practitioners use a technique called palm healing or hands-on healing through which a \"universal energy\" is said to be transferred through the palms of the practitioner to the patient in order to encourage emotional or physical healing. Medications are typically stored in pill bottles.'}, {'id': 12, 'question': '<fact1>While viewing \"Scary Movie\"</fact1> <fact2>is the viewer</fact2> <fact3>likely to experience</fact3> <fact4>an increase in adrenaline?</fact4>', 'answer': False, 'explanation': 'No. Scary Movie is a film that is a comedy take on horror, intended to make viewers laugh but not afraid. Adrenaline is produced when a human is frightened or excited.'}, {'id': 13, 'question': \"<fact1>Would Marvel's Gateway</fact1> <fact2>be envious</fact2> <fact3>of the Doctor (Doctor Who)'s TARDIS machine?</fact3>\", 'answer': False, 'explanation': 'No. The Doctor (Doctor Who) used the TARDIS, a largely unreliable time traveling machine, to travel through time and space. Gateway is a Marvel comic character linked to the X-Men comics. Gateway has the power to create wormholes that allow him to travel through time and space.'}, {'id': 14, 'question': '<fact1>Is material from an aloe plant</fact1> <fact2>sometimes enclosed</fact2> <fact3>in petroleum-derived products?</fact3>', 'answer': True, 'explanation': 'Yes. Aloe vera gel is sometimes kept in plastic packaging. Plastic packaging is derived from petroleum.'}, {'id': 15, 'question': '<fact1>Could a bee hummingbird</fact1> <fact2>balance a scale</fact2> <fact3>with a single pea on it?</fact3>', 'answer': False, 'explanation': 'No. The average pea weighs between 0.1 and 0.36 grams. Female bee hummingbirds on average weigh 2.6 grams, while on average male bee hummingbirds weigh 1.95 grams.'}, {'id': 16, 'question': '<fact1>Would a broadcast</fact1> <fact2>from Spirit</fact2> <fact3>make the news</fact3> <fact4>in 2020?</fact4>', 'answer': True, 'explanation': 'Yes. In 2010, the Spirit rover sent what is recorded as its final transmission. In 2019, another rover made the news with its viral \"final words\".'}, {'id': 17, 'question': \"<fact1>Would characters</fact1> <fact2>in Harry Potter and the Philosopher's Stone</fact2> <fact3>be persecuted</fact3> <fact4>as pagans?</fact4>\", 'answer': True, 'explanation': \"Yes. Pagans are defined as people that hold beliefs other than those of the major world religions (Christianity, Islam, and Judaism). The characters in Harry Potter and the Philosopher's Stone practice magic. Islam explicitly forbid the practice of magic and has harsh consequences for it. Jezebel in Hebrew scripture was a worshiper of pagan Baal and was thrown from a window for her beliefs. Women accused of being witches were burned alive by Christians during the Salem Witch Trials.\"}, {'id': 18, 'question': '<fact1>Can</fact1> <fact2>music</fact2> <fact3>be used</fact3> <fact4>as a weapon?</fact4>', 'answer': True, 'explanation': 'Yes. Music is an art form whose medium is sound. Music can help elevate or subdue emotions. People connect to music through the sound. The military uses loud music to cause psychological disorientation and confusion . The military calls the use of loud disorienting music part of psychological operations.'}, {'id': 19, 'question': '<fact1>Can Harry Potter</fact1> <fact2>book a flight</fact2> <fact3>on Asiana Airlines?</fact3>', 'answer': False, 'explanation': 'No. Asiana Airlines is the second largest airline in South Korea. Harry Potter is a fictional character.'}, {'id': 20, 'question': \"<fact1>Karachi</fact1> <fact2>was a part</fact2> <fact3>of Alexander the Great's success</fact3>?\", 'answer': True, 'explanation': 'Yes. Karachi is a city in modern day Pakistan. Krokola was an ancient port located in what is now Karachi. Alexander the Great stationed his fleet in Krokola on his way to Babylon. Alexander the Great defeated Darius and conquered Babylon before expanding his empire.'}, {'id': 21, 'question': '<fact1>Was Jackson Pollock</fact1> <fact2>trained by</fact2> <fact3>Leonardo da Vinci?</fact3>', 'answer': False, 'explanation': 'No. Leonardo lived during the Italian Renaissance in the 17th century. Jackson Pollock lived during the 20th century.'}, {'id': 22, 'question': '<fact1>Will The Exorcist</fact1> <fact2>stimulate</fact2> <fact3>limbic system?</fact3>', 'answer': True, 'explanation': \"Yes. The limbic system of the brain contains regions that detect fear, control bodily functions and perceive sensory information. The Exorcist has been called one of the scariest movies of all time. The Exorcist ranked number 3 on the American Film Institute's 100 Years/100 Thrills list.\"}, {'id': 23, 'question': '<fact1>Would World War II</fact1> <fact2>have been the same</fact2> <fact3>without Alan Turing?</fact3>', 'answer': False, 'explanation': 'No. During WW2, the German Military used something called the Engima device to send messages secretly.  Alan Turing broke the Enigma code, allowing German messages to be understood and intercepted.'}, {'id': 24, 'question': '<fact1>Would</fact1> <fact2>a sophist</fact2> <fact3>use</fact3> <fact4>an épée?</fact4>', 'answer': False, 'explanation': 'No. A sophist is a specific kind of teacher in ancient Greece, in the fifth and fourth centuries BC. Sophists specialized in using the tools of philosophy and rhetoric, though other sophists taught subjects such as music, athletics and mathematics. An épée is a sword used in fencing. The épée was not developed until the 19th century.'}, {'id': 25, 'question': '<fact1>Did Dale Jr</fact1> <fact2>hug his dad</fact2> <fact3>after their last Daytona 500 together?</fact3>', 'answer': False, 'explanation': 'No. Dale Jr. and his father Dale Sr. last raced together at the Daytona 500 in 2001. During the 2001 Daytona 500 Dale Sr. suffered a basilar skull fracture and died.'}, {'id': 26, 'question': '<fact1>Can citrus</fact1> <fact2>grow</fact2> <fact3>in Ulaanbaatar?</fact3>', 'answer': False, 'explanation': 'No. Citrus can withstand short periods down to as cold as −10 °C (14 °F), but realistically temperatures not falling below −2 °C (28 °F) are required for successful cultivation. Ulaanbaatar has an average annual temperature of −0.4 °C or 31.3 °F.'}, {'id': 27, 'question': '<fact1>Does New York Harbor</fact1> <fact2>sit on a craton</fact2> <fact3>without volcanic activity?</fact3>', 'answer': False, 'explanation': 'No. New York Harbor is located on Laurentia craton.  The southwestern portion of Laurentia contains numerous large volcanic eruptions.'}, {'id': 28, 'question': '<fact1>Can the Palace of Westminster</fact1> <fact2>tell time</fact2> <fact3>in the dark?</fact3>', 'answer': True, 'explanation': 'Yes. The Palace of Westminster has Big Ben, a striking clock tower. Big Ben communicates the time via bells.'}, {'id': 29, 'question': '<fact1>In teenagers and young adults with depression</fact1>, <fact2>are SSRI medications</fact2> <fact3>less safe</fact3> <fact4>than they are for adults?</fact4>', 'answer': True, 'explanation': 'Yes. In teens, SSRI medications may increase the risk of suicidal thinking. In adults over 25, SSRI medications are regarded as generally safe.'}, {'id': 30, 'question': '<fact1>Does Marco Rubio</fact1> <fact2>have a close relationship</fact2> <fact3>with Allah?</fact3>', 'answer': False, 'explanation': 'No. Marco Rubio adheres to the religious sect of Christianity known as Catholicism. Catholics and other Christians worship God. Allah is worshiped by believers of Islam.'}, {'id': 31, 'question': '<fact1>Would Persephone</fact1> <fact2>be a good consultant</fact2> <fact3>to a landscape architect?</fact3>', 'answer': True, 'explanation': 'Yes. Persephone is a vegetation goddess.  A vegetation deity is a nature deity whose disappearance and reappearance, or life, death and rebirth, embodies the growth cycle of plants. Landscape architects deal with planning and laying out gardens and other plant life.'}, {'id': 32, 'question': '<fact1>Is Argon</fact1> <fact2>near Neon</fact2> <fact3>on the periodic table of elements?</fact3>', 'answer': True, 'explanation': 'Yes. Argon is a noble gas. Neon is a noble gas.  The noble gases are all clumped together on the periodic table of elements.'}, {'id': 33, 'question': \"<fact1>Did</fact1> <fact2>Thomas Greenhill's parents</fact2> <fact3>violate</fact3> <fact4>the concept of monogamy?</fact4>\", 'answer': False, 'explanation': 'No. Thomas Greenhill was a surgeon born to William and Elizabeth Greenhill. William and Elizabeth Greenhill had 39 children. Monogamy is a committed relationship between two people where usually they remain together for life. Thomas Greenhill was the last of his parents 39 children and was born shortly after his father died.'}, {'id': 34, 'question': '<fact1>Would it</fact1> <fact2>be uncommon</fact2> <fact3>for a high schooler</fact3> <fact4>to use the yellow pages?</fact4>', 'answer': True, 'explanation': \"Yes. High school students are between 14-19 years old.  Teenagers now are considered 'digital natives'.  'Digital natives' are individuals who have grown up during the computer age, being exposed to technology from early childhood.\"}, {'id': 35, 'question': '<fact1>Could an escapee</fact1> <fact2>swim nonstop</fact2> <fact3>from Alcatraz Island</fact3> <fact4>to Siberia?</fact4>', 'answer': False, 'explanation': 'No. Alcatraz Island was a San Francisco Bay prison. Siberia is over 5,217 miles away from San Francisco. The longest continuous swim record was 139 miles.'}, {'id': 36, 'question': '<fact1>Would the Titanic</fact1> <fact2>be well preserved</fact2> <fact3>at the bottom</fact3> <fact4>of the Gulf of Finland?</fact4>', 'answer': True, 'explanation': \"Yes. The bottom of the gulf is one of the world's largest ship cemeteries. Because of the low salinity and cold waters, and no shipworms, the ships are relatively well preserved. RMS Titanic was a British passenger ship.\"}, {'id': 37, 'question': '<fact1>Could a snowy owl</fact1> <fact2>survive</fact2> <fact3>in the Sonoran?</fact3>', 'answer': False, 'explanation': 'No. The Snowy owl is a bird native to the Arctic regions of North America. Temperatures in the North American Arctic range from -36.4F to 50F. The Sonoran is one of the hottest deserts in the world. The Sonoran Desert can get as hot as 118F.'}, {'id': 38, 'question': '<fact1>Would a rabbi</fact1> <fact2>worship martyrs</fact2> <fact3>Ranavalona I killed?</fact3>', 'answer': False, 'explanation': 'No. Rabbis are teachers of Judaism. Ranavalona I, ruler of Madagascar, killed many Christians that were later determined by the church to be martyrs. Judaism does not have a group of saints and martyrs that are prayed to like Christianity..'}, {'id': 39, 'question': '<fact1>Would an oil painter</fact1> <fact2>avoid reds</fact2> <fact3>from scale insects</fact3> <fact4>that live on a cactus?</fact4>', 'answer': True, 'explanation': 'Yes. Carmine is the product of an insect that lives on some cacti. Carmine is not stable in oil paints and its usage has been discontinued. Carmine is red.'}, {'id': 40, 'question': '<fact1>Was the Second Amendment</fact1> <fact2>to the United States Constitution</fact2> <fact3>written without consideration</fact3> <fact4>for black Americans?</fact4>', 'answer': True, 'explanation': 'Yes. The writers of the Constitutional Amendments did not view black people as legitimate human beings. The writers of the Constitutional Amendments believed that slavery benefited black slaves. The Constitutional Amendments were written for people that the writers considered human.'}, {'id': 41, 'question': '<fact1>Are all the elements</fact1> <fact2>plants need</fact2> <fact3>for photosynthesis</fact3> <fact4>present in</fact4> <fact5>atmosphere of Mars?</fact5>', 'answer': True, 'explanation': 'Yes. Plants need three elements for photosynthesis: Hydrogen, Oxygen, and Carbon. The atmosphere of Mars is composed of carbon dioxide, nitrogen, argon, and trace levels of water vapor, oxygen, carbon monoxide, hydrogen and other noble gases.'}, {'id': 42, 'question': \"<fact1>Can you</fact1> <fact2>find Bob Marley's face</fact2> <fact3>in most smoke shops?</fact3>\", 'answer': True, 'explanation': \"Yes. Bob Marley's face is on the packaging of a popular brand of rolling papers. Bob Marley is a popular graphic to print on t-shirts for sale to smokers.\"}, {'id': 43, 'question': \"<fact1>Did Pablo Escobar's nickname collection</fact1> <fact2>outshine</fact2> <fact3>Robert Moses Grove's?</fact3>\", 'answer': True, 'explanation': 'Yes. Robert Moses Grove was a baseball player nicknamed Lefty Grove. Pablo Escobar had several nicknames including: Don Pablo, El Padrino, and El Patrón.'}, {'id': 44, 'question': '<fact1>Has the Subway restaurant franchise</fact1> <fact2>had any connections</fact2> <fact3>with child abusers?</fact3>', 'answer': True, 'explanation': 'Yes. Subway hired Jared Fogle as a spokesman for their sandwich shops. Jared Fogle was convicted for having sex with minors and for possessing child pornography.'}, {'id': 45, 'question': '<fact1>Does autopilot</fact1> <fact2>rely on</fact2> <fact3>fossil fuels?</fact3>', 'answer': True, 'explanation': 'Yes. Autopilot is used in vehicles. Vehicles are powered by engines. Engines rely on fossil fuels to operate.'}, {'id': 46, 'question': '<fact1>Are psychiatric patients</fact1> <fact2>welcome to join</fact2> <fact3>the United States Air Force?</fact3>', 'answer': False, 'explanation': 'No. Having a history of mental illness disqualifies most people from joining the Armed Forces. Psychiatric patients are being seen for management of mental illness.'}, {'id': 47, 'question': '<fact1>Does Nicole Kidman</fact1> <fact2>know any Scientologists?</fact2>', 'answer': True, 'explanation': 'Yes. Nicole Kidman was married to Tom Cruise. Tom Cruise is a Scientologist.'}, {'id': 48, 'question': '<fact1>Was John Lennon</fact1> <fact2>known to be</fact2> <fact3>a good friend</fact3> <fact4>to Sasha Obama?</fact4>', 'answer': False, 'explanation': 'No. John Lennon died in 1980. Sasha Obama was born in 2001.'}, {'id': 49, 'question': '<fact1>Would the tunnels</fact1> <fact2>at CERN</fact2> <fact3>fit onto</fact3> <fact4>the High Speed 1 rails?</fact4>', 'answer': True, 'explanation': 'Yes. High Speed 1 (HS1), legally the Channel Tunnel Rail Link (CTRL), is a 67-mile (108 km) high-speed railway. The CERN collider is contained in a circular tunnel, with a circumference of 26.7 kilometres (16.6 mi). The circumference of a circle is the length of the enclosing boundary.'}, {'id': 50, 'question': '<fact1>Could an elephant</fact1> <fact2>easily defeat</fact2> <fact3>a male macaque?</fact3>', 'answer': True, 'explanation': 'Yes. Male macaques range from 16 to 28 inches tall with a weight between 12.13 to 39.7 pounds. Elephants are between 7 to 11 feet tall and weigh several thousand pounds. Elephants contain large, sharp tusks that can injure or kill other animals.'}, {'id': 51, 'question': '<fact1>Do</fact1> <fact2>the Ubuntu people</fact2> <fact3>speak</fact3> <fact4>Ubuntu?</fact4>', 'answer': False, 'explanation': 'No. Ubuntu is a of free and open-source software used on computers. The Ubuntu people are a tribe that lives in Africa. The Ubuntu people derive their language from Nguni Bantu. As of 2019 only 10% of households in Africa have a computer.'}, {'id': 52, 'question': '<fact1>Is cycling</fact1> <fact2>a high-risk activity</fact2> <fact3>for pelvis fractures?</fact3>', 'answer': False, 'explanation': 'No. Cycling is a low-impact activity . Stress fractures in a pelvic bone often develop as a result of repetitive, high-impact activity that puts stress on the pelvis, such as long-distance running or ballet.'}, {'id': 53, 'question': '<fact1>Is a platypus</fact1> <fact2>immune</fact2> <fact3>from cholera?</fact3>', 'answer': True, 'explanation': 'Yes. Cholera is a bacteria that damages the small intestines in humans. The intestines are part of the stomach of humans. A platypus does not have a stomach.'}, {'id': 54, 'question': '<fact1>Would a moose</fact1> <fact2>hypothetically be too much</fact2> <fact3>for a minotaur</fact3> <fact4>to devour whole?</fact4>', 'answer': True, 'explanation': 'Yes. A minotaur was a mythological beast with the head of a bull and body of a human. Human stomachs process about three to four pounds of food a day. A moose can weigh up to 1500 pounds. Bulls can consume around 33 pounds of food a day.'}, {'id': 55, 'question': '<fact1>Are there</fact1> <fact2>bones</fact2> <fact3>in an anchovy pizza?</fact3>', 'answer': True, 'explanation': 'Yes. Anchovies used on pizza are typically packed whole in oil or water.  Anchovies on pizza are not usually cut or filleted in any way.'}, {'id': 56, 'question': '<fact1>Can a Bengal cat</fact1> <fact2>survive</fact2> <fact3>eating only pancakes?</fact3>', 'answer': False, 'explanation': 'No. Bengal cats are carnivores. Pancakes contain no meat. Carnivores eat only meat to survive.'}, {'id': 57, 'question': '<fact1>Can the original name</fact1> <fact2>of the zucchini</fact2> <fact3>be typed</fact3> <fact4>on the top row</fact4> <fact5>of a QWERTY keyboard?</fact5>', 'answer': False, 'explanation': 'No. The original name for the zucchini in Mexican language or Nahuatl is ayokonetl. The top row of a QWERTY keyboard contains the keys q, w, e, r, t, y, u, i , o, and p.'}, {'id': 58, 'question': '<fact1>Does The Hague</fact1> <fact2>border</fact2> <fact3>multiple bodies of water?</fact3>', 'answer': False, 'explanation': 'No. The Hague is in the Netherlands.  The Hague is in the Western part of the Netherlands.  The Netherlands borders the North Sea to its west.'}, {'id': 59, 'question': '<fact1>Do</fact1> <fact2>people in middle school</fact2> <fact3>usually</fact3> <fact4>get breast exams?</fact4>', 'answer': False, 'explanation': 'No. Women should begin giving themselves breast exams at the age of 20. Middle school students are usually preteens or young teens.'}, {'id': 60, 'question': '<fact1>Did people in Korea</fact1> <fact2>under Japanese Rule</fact2> <fact3>watch a lot of Iron Chef?</fact3>', 'answer': False, 'explanation': 'No. The first televisions were sold in 1946. Iron Chef started airing in 1993.'}, {'id': 61, 'question': '<fact1>Would a bald eagle</fact1> <fact2>deliver an urgent message</fact2> <fact3>before a B-52?</fact3>', 'answer': False, 'explanation': 'No. A bald eagle can travel up to 99 MPH. The B-52 is a US air bomber that can travel up to 650 MPH.'}, {'id': 62, 'question': '<fact1>Does Snoopy</fact1> <fact2>look like</fact2> <fact3>Chance</fact3> <fact4>from Homeward Bound?</fact4>', 'answer': False, 'explanation': 'No. Chance from Homeward Bound is a golden retriever.  Snoopy is black and white. Golden Retrievers are yellow in color.'}, {'id': 63, 'question': '<fact1>Was Dioskourides</fact1> <fact2>a lapidary?</fact2>', 'answer': True, 'explanation': 'Yes. A lapidary is a person or machine who cuts gemstones; classically, it refers to a person who engraves gemstones. Dioskourides put his signature on a Roman amethyst ringstone with a portrait of Demosthenes circa late 1st century BC. Artists sign their work.'}, {'id': 64, 'question': '<fact1>Do</fact1> <fact2>most</fact2> <fact3>middle class families</fact3> <fact4>have</fact4> <fact5>butlers</fact5>?', 'answer': False, 'explanation': 'No. Butlers make about $60,000 per year on average for their work. Middle class income is between $48,000 and $145,000.'}, {'id': 65, 'question': \"<fact1>Was</fact1> <fact2>Hillary Clinton's deputy chief of staff</fact2> <fact3>in 2009</fact3> <fact4>baptised?</fact4>\", 'answer': False, 'explanation': \"No. Huma Abedin was Hillary Clinton's deputy chief of staff in 2009. Huma Abedin is an American Muslim. Baptism is a Christian tradition.\"}, {'id': 66, 'question': '<fact1>Does the New York Public Library</fact1> <fact2>sell</fact2> <fact3>Alpo products?</fact3>', 'answer': False, 'explanation': 'No. The New York Public Library is a public lending library system in New York City. Alpo produces pet food and related products.'}, {'id': 67, 'question': '<fact1>Did Solomon</fact1> <fact2>make up a bigger percentage</fact2> <fact3>of Islamic prophets</fact3> <fact4>than Kings of Judah?</fact4>', 'answer': False, 'explanation': 'No. According to The Quran, Solomon was one of 25 prophets. According to some Islamic hadiths, there have been as many as 124,000 prophets. Solomon was one of 20 Kings of Judah.'}, {'id': 68, 'question': '<fact1>Would Taylor Swift</fact1> <fact2>refer to Snoopy</fact2> <fact3>as oppa?</fact3>', 'answer': True, 'explanation': 'Yes. Oppa is a Korean word used by women to address a man who is 10 or more years older than her. Snoopy is 47 years old. Taylor Swift is 30 years old.'}, {'id': 69, 'question': '<fact1>Did</fact1> <fact2>Confederate States Army</fact2> <fact3>influence</fact3> <fact4>West Point fashion?</fact4>', 'answer': True, 'explanation': 'Yes. The Confederate States Army was clad in cadet gray uniforms. West Point uniforms are cadet gray and white. Confederate States Army uniforms contained Generally, the uniform jacket of the Confederate soldier was single breasted, made of gray or brown fabric, with a six to nine button front and hat.  West Point uniforms contain a standing collar, white trousers, and black shakos (known as a \"tarbucket hat\" in U.S. Army nomenclature).'}, {'id': 70, 'question': '<fact1>Does conservatism</fact1> <fact2>repulse</fact2> <fact3>Blaire White?</fact3>', 'answer': False, 'explanation': 'No. Blaire White is a Youtuber. Blaire White posts content that leans towards conservative politics.'}, {'id': 71, 'question': '<fact1>Do</fact1> <fact2>all parts</fact2> <fact3>of the aloe vera plant</fact3> <fact4>taste good?</fact4>', 'answer': False, 'explanation': 'No. There is a layer of yellow latex liquid between the outside of an aloe leaf and the gel inside. The latex inside aloe tastes very bitter.'}, {'id': 72, 'question': '<fact1>Does</fact1> <fact2>an individual oceanographer</fact2> <fact3>study</fact3> <fact4>many sciences?</fact4>', 'answer': True, 'explanation': 'Yes. Study of the oceans involve many fields or science. To properly study their specific topic of research, an oceanographer must understand how their science interacts with the other involved sciences.'}, {'id': 73, 'question': \"<fact1>Did</fact1> <fact2>Kurt Cobain's</fact2> <fact3>music genre</fact3> <fact4>survive</fact4> <fact5>after his death?</fact5>\", 'answer': True, 'explanation': \"Yes. Kurt Cobain was the lead singer of Nirvana. Nirvana's music is classified as Grunge rock. Kurt Cobain died on April 5, 1994. Some of the major Grunge rock bands included Alice in Chains, Pearl Jam, and Soundgarden. Alice in Chains and Pearl Jam released their latest albums in 2018 and 2020 respectively.\"}, {'id': 74, 'question': '<fact1>Did Immanuel Kant</fact1> <fact2>ever meet</fact2> <fact3>the 14th president of the United States?</fact3>', 'answer': False, 'explanation': 'No. Immanuel Kant died on Feb 12, 1804. Franklin Pierce was the 14th president of the United States. Franklin PIerce was born Nov 23, 1804.'}, {'id': 75, 'question': '<fact1>Would eliminating competition</fact1> <fact2>in the Japanese bulk carrier market</fact2> <fact3>be profitable</fact3> <fact4>for a steel company?</fact4>', 'answer': True, 'explanation': 'Yes. 62% of bulk carriers are built in Japan. Bulk carrier hulls are made of steel.'}, {'id': 76, 'question': '<fact1>Would a geographer</fact1> <fact2>use biochemistry</fact2> <fact3>in their work?</fact3>', 'answer': False, 'explanation': \"No. A geographer is a scientist whose area of study is geography, the study of Earth's natural environment and human society. Biochemistry is the study of chemical processes within and relating to living organisms.\"}, {'id': 77, 'question': '<fact1>Is most coffee</fact1> <fact2>produced</fact2> <fact3>South of the Equator?</fact3>', 'answer': True, 'explanation': 'Yes. The countries with the highest coffee production are in South America. Almost all of South America is in the Southern Hemisphere.'}, {'id': 78, 'question': '<fact1>Is it hard</fact1> <fact2>to get a BLT</fact2> <fact3>in Casablanca?</fact3>', 'answer': True, 'explanation': 'Yes. A BLT is a sandwich consisting of bacon, lettuce and tomato. Casablanca is predominantly Muslim. Islam forbids the consumption of pork and pork products.'}, {'id': 79, 'question': '<fact1>Could</fact1> <fact2>largest ship</fact2> <fact3>from Voyages of Christopher Columbus</fact3> <fact4>haul</fact4> <fact5>Statue of Liberty?</fact5>', 'answer': False, 'explanation': 'No. The largest ship Christopher Columbus used was the Santa Maria. The Santa Maria has a cargo capacity of 108 tons. The Statue of Liberty weighs 225 tons.'}, {'id': 80, 'question': \"<fact1>Would Jolly Green Giant's largest monument</fact1> <fact2>look impressive</fact2> <fact3>next to Pyrenees?</fact3>\", 'answer': False, 'explanation': 'No. The Jolly Green Giant monument in Blue Earth, Minnesota is 55.5 feet tall. The Pyrenees mountains are 11,168 feet high. The Pyrenees mountains are 305 miles wide.'}, {'id': 81, 'question': '<fact1>Did Karl Marx</fact1> <fact2>influence</fact2> <fact3>the communist party</fact3> <fact4>of China?</fact4>', 'answer': True, 'explanation': 'Yes. Communist ideology is the foundation of communist party of China. Marx produced a political pamphlet that has since come to be commonly known as the communist manifesto.'}, {'id': 82, 'question': '<fact1>Does Soylent</fact1> <fact2>use Pea</fact2> <fact3>for their source of protein?</fact3>', 'answer': False, 'explanation': 'No. Soylent is a meal replacement drink that offers 20mg protein. The protein in Soylent is derived from Soy.'}, {'id': 83, 'question': '<fact1>Do most religious people</fact1> <fact2>in Quebec</fact2> <fact3>refer to the Quran?</fact3>', 'answer': False, 'explanation': 'No. Christianity is the major religion in Quebec. Christians refer to the Bible as their book of reference.'}, {'id': 84, 'question': '<fact1>Does Family Guy</fact1> <fact2>take place</fact2> <fact3>on the American West Coast?</fact3>', 'answer': False, 'explanation': 'No. Family Guy takes place in the fictional town of Quahog, Rhode Island. Rhode Island is a state on the American East Coast.'}, {'id': 85, 'question': '<fact1>Are lengths</fact1> <fact2>measured</fact2> <fact3>in metres</fact3> <fact4>in the UK?</fact4>', 'answer': True, 'explanation': 'Yes. Metres are part of the metric system. The UK uses the metric system for measurements.'}, {'id': 86, 'question': \"<fact1>Did the first Duke of Valentinois</fact1> <fact2>play a key role</fact2> <fact3>in the Hundred Years' War?</fact3>\", 'answer': False, 'explanation': \"No. The Hundred Years' War was a conflict between England and France from 1337-1453. Cesare Borgia, the son of Pope Alexander VI, was the first Duke of Valentinois. Cesare Borgia was born in 1475.\"}, {'id': 87, 'question': '<fact1>Do gorillas</fact1> <fact2>fight with</fact2> <fact3>panda bears?</fact3>', 'answer': False, 'explanation': 'No. Gorilla distribution is limited to areas of the continent of Africa. Panda bears are found in the wild only in China.'}, {'id': 88, 'question': '<fact1>Was Christina Aguilera</fact1> <fact2>born in the forgotten borough?</fact2>', 'answer': True, 'explanation': 'Yes. Christina Maria Aguilera was born on December 18, 1980, in Staten Island, New York. Staten Island has sometimes been called \"the forgotten borough\" by inhabitants who feel neglected by the city government.'}, {'id': 89, 'question': \"<fact1>Is H's most common two letter pair partner</fact1> <fact2>a freebie</fact2> <fact3>in Wheel of Fortune bonus round?</fact3>\", 'answer': True, 'explanation': 'Yes. H forms the most common two letter pair in the English language along with the letter T. The Wheel of Fortune bonus round gives the player six free letters: R, S, T, L, N, E.'}, {'id': 90, 'question': '<fact1>Can</fact1> <fact2>a quarter</fact2> <fact3>fit inside</fact3> <fact4>of a human kidney?</fact4>', 'answer': True, 'explanation': 'Yes. Kidney stones are hard mineral deposits that can form in the kidneys. The largest kidney stone ever recorded was 13 cm wide. The diameter of a quarter is 2.4 cm.'}, {'id': 91, 'question': '<fact1>Does the James Webb Space Telescope</fact1> <fact2>fail astronomer</fact2> <fact3>in locating planet Krypton?</fact3>', 'answer': True, 'explanation': 'Yes. The James Webb Space Telescope is the most powerful telescope created. Krypton is a planet in the fictional Superman comic book series.'}, {'id': 92, 'question': \"<fact1>Was P. G. Wodehouse's</fact1> <fact2>favorite book</fact2> <fact3>The Hunger Games?</fact3>\", 'answer': False, 'explanation': 'No. P. G. Wodehouse died in 1975. The Hunger Games was published in 2008.'}, {'id': 93, 'question': '<fact1>Should</fact1> <fact2>a finished website</fact2> <fact3>have</fact3> <fact4>lorem ipsum paragraphs?</fact4>', 'answer': False, 'explanation': 'No. Lorem Ipsum paragraphs are meant to be temporary. Web designers always remove lorem ipsum paragraphs before launch.'}, {'id': 94, 'question': '<fact1>Did William Shakespeare</fact1> <fact2>read</fact2> <fact3>the Daily Mirror?</fact3>', 'answer': False, 'explanation': 'No. The Daily Mirror is a British tabloid founded in 1903. William Shakespeare died in 1616.'}, {'id': 95, 'question': '<fact1>Will more people</fact1> <fact2>go in and out</fact2> <fact3>of Taco Bell</fact3> <fact4>than a Roy Rogers</fact4> <fact5>each year?</fact5>', 'answer': True, 'explanation': 'Yes. Taco Bell has over 7,072 restaurants as of 2018. Roy Rogers had over 600 restaurants at its peak. Roy Rogers has 48 locations as of 2019.'}, {'id': 96, 'question': '<fact1>Is it safe</fact1> <fact2>to wear sandals</fact2> <fact3>in snow?</fact3>', 'answer': False, 'explanation': \"No. Sandals have open toes and don't completely cover the feet. Snow is very cold and direct exposure to skin can cause hypothermia. The feet need to be completely covered to walk through snow safely.\"}, {'id': 97, 'question': \"<fact1>Was Nikola Tesla's</fact1> <fact2>home country</fact2> <fact3>involved</fact3> <fact4>in the American Civil War?</fact4>\", 'answer': False, 'explanation': 'No. Nikola Tesla was born in the Austrian Empire. The American Civil War was a domestic American conflict.'}, {'id': 98, 'question': '<fact1>Did Ada Lovelace</fact1> <fact2>die tragically young</fact2> <fact3>for her era?</fact3>', 'answer': False, 'explanation': 'No. Ada Lovelace died at the age of 36 in 1852. The life expectancy in the 1800s was between 35 and 39 years old. Tuberculosis was one of the leading causes of death in the 1800s and a vaccine was not made until the 1900s.'}, {'id': 99, 'question': '<fact1>Can</fact1> <fact2>an Asian black bear</fact2> <fact3>use</fact3> <fact4>chopsticks?</fact4>', 'answer': False, 'explanation': \"No. Asian black bear are a species of bear found in asia.  Asian black bear don't have opposable thumbs. Chopsticks are eating utensils use requires opposable thumbs.\"}, {'id': 100, 'question': '<fact1>Can someone</fact1> <fact2>sell their time</fact2> <fact3>through the Toronto Star?</fact3>', 'answer': True, 'explanation': 'Yes. The Toronto Star has a classifieds section. Readers can advertise their own labor or services and thus their time .'}, {'id': 101, 'question': '<fact1>Were deaths</fact1> <fact2>from Apollo 13 mission</fact2> <fact3>eclipsed</fact3> <fact4>by other space missions?</fact4>', 'answer': True, 'explanation': 'Yes. Apollo 13 was a failed Moon landing in which the crew had faulty oxygen resources. All of the Apollo 13 crew returned safely to earth. The Challenger was a space shuttle that malfunctioned, killing all seven passengers. The Space Shuttle Columbia disaster had seven casualties.'}, {'id': 102, 'question': '<fact1>Can you purchase</fact1> <fact2>a dish with injera</fact2> <fact3>at Taco Bell?</fact3>', 'answer': False, 'explanation': 'No. Taco Bell serves a variety of Mexican and Tex-Mex foods that include tacos, burritos, quesadillas, and nachos. Injera is a sour fermented flatbread with a slightly spongy texture, traditionally made out of teff flour. Injera is part of Ethiopian cuisine.'}, {'id': 103, 'question': '<fact1>Is xenophobia</fact1> <fact2>hypothetically unimportant</fact2> <fact3>between Saladin</fact3> <fact4>and Ali Askari?</fact4>', 'answer': True, 'explanation': 'Yes. Xenophobia is the dislike of someone that is foreign or from a different background. Saladin was a Kurdish leader that became sultan of Egypt. Ali Askari was a Kurdish politician.'}, {'id': 104, 'question': '<fact1>Does Sockington</fact1> <fact2>enjoy</fact2> <fact3>onions?</fact3>', 'answer': False, 'explanation': 'No. Sockington is a domestic cat. Onions can cause toxicity in cats by breaking down their red blood cells.'}, {'id': 105, 'question': \"<fact1>Was Raphael's paintings</fact1> <fact2>influenced</fact2> <fact3>by the country of Guam?</fact3>\", 'answer': False, 'explanation': \"No. Raphael was a European painter that died in 1520. Magellan's 1521 voyage was Europe's first trip to Guam.\"}, {'id': 106, 'question': '<fact1>Could</fact1> <fact2>Oscar Wilde</fact2> <fact3>have operated</fact3> <fact4>a motor vehicle?</fact4>', 'answer': True, 'explanation': 'Yes. Motor vehicles were in use by the 1890s. Oscar Wilde lived until 1900.'}, {'id': 107, 'question': '<fact1>Does ontology</fact1> <fact2>require</fact2> <fact3>a scalpel?</fact3>', 'answer': False, 'explanation': 'No. A scalpel is used during surgery. Ontology is a philosophical domain, not a medical one.'}, {'id': 108, 'question': '<fact1>Was King Arthur</fact1> <fact2>at the beheading</fact2> <fact3>of Anne Boleyn?</fact3>', 'answer': False, 'explanation': 'No. King Arthur was a legendary British leader who, according to medieval histories and romances, led the defence of Britain against Saxon invaders in the late 5th and early 6th centuries. Anne Boleyn was beheaded May 19, 1536.'}, {'id': 109, 'question': '<fact1>Would Alexander Graham Bell</fact1> <fact2>hypothetically support</fact2> <fact3>Nazi eugenics?</fact3>', 'answer': True, 'explanation': 'Yes. Eugenics was the idea of selective breeding or sterilization to rid the human populace of certain traits. Nazis used eugenics to justify mass sterilization and mass murder. Alexander Graham Bell Alexander Graham Bell advocated against the use of sign language and hoped to eradicate deafness through selective breeding.'}, {'id': 110, 'question': \"<fact1>Was Harry Truman's presidency</fact1> <fact2>unaffected</fact2> <fact3>by the twenty-third Amendment</fact3> <fact4>to the US Constitution?</fact4>\", 'answer': True, 'explanation': 'Yes. The 23rd Amendment to the US Constitution was passed in 1961. Harry Truman was the President from 1945-1953.'}, {'id': 111, 'question': '<fact1>Would the trees</fact1> <fact2>in Auburn, New York</fact2> <fact3>be changing colors</fact3> <fact4>in September?</fact4>', 'answer': True, 'explanation': 'Yes. In New York, fall begins during the month of September.  When the weather gets cooler in the fall, leaves begin to change colors.'}, {'id': 112, 'question': '<fact1>Is Black Lives Matter</fact1> <fact2>connected with</fact2> <fact3>capsaicin?</fact3>', 'answer': True, 'explanation': 'Yes. Black Lives Matter has held numerous protests. Protesters at Black Lives Matter events have had pepper spray used against them by police. Capsaicin is the main ingredient of pepper spray.'}, {'id': 113, 'question': '<fact1>Was Achilles</fact1> <fact2>a direct descendent</fact2> <fact3>of Gaia?</fact3>', 'answer': True, 'explanation': 'Yes. Achilles was the son of a Nereid.  The Nereids were the 50 daughters of Nereus. Nereus was the eldest son of the union between Gaia and Pontus.'}, {'id': 114, 'question': '<fact1>Does Jerry Seinfeld</fact1> <fact2>hang out</fact2> <fact3>at the Budweiser Party Deck?</fact3>', 'answer': False, 'explanation': 'No. The Budweiser Party Deck is a social gathering spot in Yankee Stadium. Yankee Stadium is home to the New York Yankees baseball team. Jerry Seinfeld is a fan of the New York Mets.'}, {'id': 115, 'question': \"<fact1>Can you</fact1> <fact2>hide a basketball</fact2> <fact3>in a sand cat's ear?</fact3>\", 'answer': False, 'explanation': \"No. The diameter of a standard NBA basketball is around 9.5 inches. A sand cat's ear grows to 2.8 inches tall.\"}, {'id': 116, 'question': '<fact1>Could the Spice Girls</fact1> <fact2>compete against</fact2> <fact3>ŽRK Kumanovo?</fact3>', 'answer': False, 'explanation': \"No. The Spice Girls had 5 members. ŽRK Kumanovo is a women's handball club from Kumanovo in the Republic of Macedonia. Handball is a sport played by two teams of seven players each.\"}, {'id': 117, 'question': '<fact1>Does the swastika</fact1> <fact2>have positive uses?</fact2>', 'answer': True, 'explanation': 'Yes. The swastika is used in the Hindu religion to represent the sun. People practicing Hindu believe the swastika represents prosperity and good luck.'}, {'id': 118, 'question': \"<fact1>Was Black fly</fact1> <fact2>upstaged</fact2> <fact3>by another insect</fact3> <fact4>in Jeff Goldblum's 1986 film?</fact4>\", 'answer': True, 'explanation': 'Yes. Jeff Goldnlum starred in the 1986 movie The Fly. The fly used in the movie The Fly was a common Housefly. The Black fly is most closely related to Chironomidae since they both feed on mammals.'}, {'id': 119, 'question': \"<fact1>Did Linnaeus</fact1> <fact2>edit</fact2> <fact3>Darwin's draft</fact3> <fact4>of Origin of Species?</fact4>\", 'answer': False, 'explanation': 'No. Linnaeus died in 1778. Origin of Species was published in 1859.'}, {'id': 120, 'question': '<fact1>Do</fact1> <fact2>all cancer patients</fact2> <fact3>get disability?</fact3>', 'answer': False, 'explanation': 'No. All forms of cancer qualify as diagnoses that can result in disability. Disability is not determined by diagnosis, but by degree of impairment. Some cancer patients do not experience major impairment.'}, {'id': 121, 'question': '<fact1>Did the death of Helen Palmer</fact1> <fact2>have a significant effect</fact2> <fact3>on Dr. Seuss?</fact3>', 'answer': True, 'explanation': \"Yes. Dr. Seuss's real name was Theodor Geisel. Theodor Geisel was married to Helen Palmer at the time of her suicide. Theodor Geisel is quoted having said he considered suicide after the death of his wife.\"}, {'id': 122, 'question': '<fact1>Can fish</fact1> <fact2>get Tonsillitis?</fact2>', 'answer': False, 'explanation': 'No. Tonsils are a pair of soft tissue masses located at the rear of the throat. Tonsillitis is the inflammation of tonsils. Fish do not have tonsils. Tonsils are only found in mammals.'}, {'id': 123, 'question': '<fact1>Would it be impossible</fact1> <fact2>to get to Burning Man</fact2> <fact3>on the Mayflower?</fact3>', 'answer': True, 'explanation': 'Yes. The Mayflower was a seafaring vessel. Burning Man is held in Black Rock Desert. There are no bodies of water flowing into the Black Rock Desert.'}, {'id': 124, 'question': \"<fact1>Does Lupita Nyongo</fact1> <fact2>have citizenship</fact2> <fact3>in paternal Family</fact3> <fact4>of Barack Obama's origin country?</fact4>\", 'answer': True, 'explanation': \"Yes. Actress Lupita Nyongo has dual citizenship in Kenya and Mexico. Barack Obama's father was born in Nyang’oma Kogelo, Rachuonyo District, Kenya Colony. Barack Obama's father was a Kenyan of the Luo tribe.\"}, {'id': 125, 'question': '<fact1>Are there enough</fact1> <fact2>Johnny Cash records</fact2> <fact3>in the world</fact3> <fact4>to give one</fact4> <fact5>to each French citizen?</fact5>', 'answer': True, 'explanation': 'Yes. Johnny Cash has sold about 90 million albums. The population of France is around 66 million .'}, {'id': 126, 'question': '<fact1>Could someone</fact1> <fact2>mistake the smell</fact2> <fact3>of your brussels sprouts</fact3> <fact4>for a fart?</fact4>', 'answer': True, 'explanation': 'Yes. Brussels Sprouts are cruciferous vegetables. Cruciferous vegetables have a sulfur containing chemical called glucosinolate in them. When you cook brussels sprouts, their smell intensifies.'}, {'id': 127, 'question': '<fact1>Can Stone Cold Steve Austin</fact1> <fact2>apply his finisher</fact2> <fact3>to a mule deer?</fact3>', 'answer': True, 'explanation': \"Yes. Steve Austin's finisher, The Stone Cold Stunner, involves lifting the opponent and then pulling their head down. The largest male mule deer's are around 330 pounds. Steve Austin has applied his finisher to the wrestler The Big Show. The Big Show weighs 383 pounds.\"}, {'id': 128, 'question': '<fact1>Is the Joker</fact1> <fact2>in a healthy</fact2> <fact3>romantic relationship?</fact3>', 'answer': False, 'explanation': 'No. Healthy relationships are characterized by mutual trust and respect. The Joker is dating Harley Quinn. The Joker frequently abuses and talks down to Harley.'}, {'id': 129, 'question': '<fact1>Is the kayak</fact1> <fact2>a traditional boat</fact2> <fact3>in New Zealand?</fact3>', 'answer': False, 'explanation': 'No. Kayaks were developed by native peoples to hunt in northern waters of the Arctic Ocean, North Atlantic, Bering Sea and North Pacific.  New Zealand is in the Southern Hemisphere. The native Maori people of New Zealand arrived there in canoes.'}, {'id': 130, 'question': '<fact1>Did Boris Yeltsin</fact1> <fact2>watch</fact2> <fact3>the 2008 Summer Olympics?</fact3>', 'answer': False, 'explanation': 'No. The 2008 Summer Olympics were held Aug 08 - 24, 2008. Boris Yeltsin died on Apr 23, 2007.'}, {'id': 131, 'question': '<fact1>Did the swallow</fact1> <fact2>play a role</fact2> <fact3>in a famous film</fact3> <fact4>about King Arthur?</fact4>', 'answer': True, 'explanation': 'Yes. Monty Python and the Holy Grail was a famous film about King Arthur. In Monty Python and the Holy Grail, swallows are mentioned several times.'}, {'id': 132, 'question': '<fact1>Was Michael Crichton</fact1> <fact2>ever in danger</fact2> <fact3>of flunking out</fact3> <fact4>of Harvard</fact4> <fact5>as an undergraduate?</fact5>', 'answer': False, 'explanation': \"No. Scholastic probation or academic dismissal, sometimes known as flunking out, is the termination of students at a higher educational institution as the result of poor academic achievement. Michael Crichton obtained his bachelor's degree in biological anthropology summa cum laude in 1964. Summa cum laude is the highest distinction a person can achieve in college for academic success. Someone who achieves summa cum laude cannot have even a single semester of poor grades.\"}, {'id': 133, 'question': '<fact1>Was Surfing</fact1> <fact2>popular</fact2> <fact3>when pogs came out?</fact3>', 'answer': True, 'explanation': \"Yes. Pogs came out in the 1990's. The 90's saw a rise in 'Big Wave Culture', a practice involving finding the largest possible waves to surf on.\"}, {'id': 134, 'question': '<fact1>Could a student</fact1> <fact2>at the University of Houston</fact2> <fact3>see a caracal</fact3> <fact4>on campus?</fact4>', 'answer': False, 'explanation': 'No. The caracal is native to Africa, the Middle East, Central Asia, and India. The University of Houston is located in the United States. The United States is located in North America.'}, {'id': 135, 'question': '<fact1>Did the Coen brothers</fact1> <fact2>ever collaborate</fact2> <fact3>with the Brothers Grimm?</fact3>', 'answer': False, 'explanation': 'No. The Coen brothers were born in 1954 and 1957. The Brothers Grimm died in 1859 and 1863.'}, {'id': 136, 'question': '<fact1>Could</fact1> <fact2>a hundred thousand lolcats</fact2> <fact3>fit</fact3> <fact4>on a first generation iPhone?</fact4>', 'answer': True, 'explanation': 'Yes. Lolcat memes are often in jpeg form. The average size of a jpeg is 10 to 30 kb. One gigabyte is 1000000 kb. The first iPhone had 4GB to 16GB of storage .'}, {'id': 137, 'question': '<fact1>Are monks</fact1> <fact2>forbidden</fact2> <fact3>from engaging</fact3> <fact4>in warfare?</fact4>', 'answer': False, 'explanation': 'No. Monks are members of religious orders that usually take vows of poverty, chastity, and obedience. The Knights Templar were a religious order that fought during the Crusades and captured Jerusalem in 1099. Buddhist Shaolin monks developed very powerful martial arts skills, have defended temples during conquests.'}, {'id': 138, 'question': '<fact1>Is</fact1> <fact2>the BBC World Service</fact2> <fact3>hosted</fact3> <fact4>in Europe?</fact4>', 'answer': True, 'explanation': 'Yes. The BBC World Service is part of the BBC network. The BBC operates in England. England is part of Europe.'}, {'id': 139, 'question': '<fact1>Would</fact1> <fact2>1996 leap year baby</fact2> <fact3>technically be</fact3> <fact4>1 year old</fact4> <fact5>in 2000?</fact5>', 'answer': True, 'explanation': 'Yes. A leap year happens once every 4 years and has 29 days in February. The years 1996, 2000, 2004, 2008, 2012, 2016, and 2020 are the last 7 leap years that have happened. 1996 to 2000 is one leap year.'}, {'id': 140, 'question': '<fact1>Did Clark Gable</fact1> <fact2>marry more women</fact2> <fact3>once</fact3> <fact4>than Richard Burton?</fact4>', 'answer': True, 'explanation': 'Yes. Richard Burton was married to Elizabeth Taylor twice.  Richard Burton married Sally Burton, Suzy Hunt, and Sybil Williams once. Clark Gable was married to five different women, one time each.'}, {'id': 141, 'question': '<fact1>Is one blast</fact1> <fact2>from double-barreled shotgun</fact2> <fact3>likely to kill</fact3> <fact4>all squid brains?</fact4>', 'answer': False, 'explanation': 'No. A double-barreled shotgun fires two rounds in one single blast. Squids have three brains.'}, {'id': 142, 'question': '<fact1>Do women</fact1> <fact2>often need</fact2> <fact3>new shoes</fact3> <fact4>during their pregnancy?</fact4>', 'answer': True, 'explanation': 'Yes. Pregnancy can cause swelling in the feet and legs. For safety and comfort, one needs to get new shoes if the size of their feet change.'}, {'id': 143, 'question': '<fact1>Are some adherents</fact1> <fact2>to Christianity</fact2> <fact3>in China</fact3> <fact4>historic enemies</fact4> <fact5>of the Catholic Church?</fact5>', 'answer': True, 'explanation': \"Yes. Christianity in China is comprised of several different groups including: Protestants, Catholics, Evangelicals, and Orthodox Christians. Catholics have been at war with Protestants throughout history. The 1572 St. Bartholomew's Day Massacre saw thousands of Protestants killed by Catholic mobs. English Protestant rulers killed many Irish Catholics during the Reformation.\"}, {'id': 144, 'question': '<fact1>Is</fact1> <fact2>Chinese successor</fact2> <fact3>to Chevrolet Cruze</fact3> <fact4>name</fact4> <fact5>a town</fact5> <fact6>far from Milan?</fact6>', 'answer': False, 'explanation': 'No. The Chinese successor to Chevrolet Cruze is the Chevrolet Monza. Monza is a city near the north of Milan.'}, {'id': 145, 'question': \"<fact1>Could the Dominican Order</fact1> <fact2>hypothetically defeat</fact2> <fact3>Blessed Gerard's order?</fact3>\", 'answer': False, 'explanation': 'No. The Dominican Order is a Catholic group of friars that several priestly vows. Blessed Gerard was the founder of the Order of St John of Jerusalem (Knights Hospitaller).  The Order of St John of Jerusalem (Knights Hospitaller) were a well trained Catholic military order that fought in the Crusades.'}, {'id': 146, 'question': '<fact1>Is a pottery kiln</fact1> <fact2>inappropriate for use</fact2> <fact3>with glass blowing?</fact3>', 'answer': False, 'explanation': 'No. Pottery kilns heat from the sides. Glass kilns heat from the top. Glass can be fused in a ceramic pottery kiln without trouble.'}, {'id': 147, 'question': '<fact1>Are there enough people</fact1> <fact2>in the Balkans</fact2> <fact3>to match</fact3> <fact4>the population of Japan?</fact4>', 'answer': False, 'explanation': 'No. There are approximately 55 million people in the Balkans. There are more than 125 million people in Japan.'}, {'id': 148, 'question': '<fact1>Could Steven Spielberg</fact1> <fact2>send emails</fact2> <fact3>as a child?</fact3>', 'answer': False, 'explanation': 'No. Steven Spielberg was born in 1946. Email did not become available to the general public until 1995.'}, {'id': 149, 'question': \"<fact1>Was</fact1> <fact2>Charles Manson's body</fact2> <fact3>unwanted?</fact3>\", 'answer': False, 'explanation': \"No. Charles Manson's body was debated over for so long that he had to stay on ice. Charles Manson had four people fighting over his corpse.\"}, {'id': 150, 'question': \"<fact1>Would a Beaver's teeth</fact1> <fact2>rival</fact2> <fact3>that of a Smilodon?</fact3>\", 'answer': False, 'explanation': \"No. A beaver has teeth measuring 25 mm on average. The Smilodon was a prehistoric saber-toothed tiger. The Smilodon's teeth were around 11 inches long.\"}, {'id': 151, 'question': \"<fact1>Is dysphoria</fact1> <fact2>around one's pelvis</fact2> <fact3>treatable</fact3> <fact4>without surgery?</fact4>\", 'answer': True, 'explanation': 'Yes. For individuals experiencing dysphoria around having a vagina, packers can relieve symptoms. For people experiencing dysphoria about having a penis, there are tucking underwear and comfort gaffs available.'}, {'id': 152, 'question': '<fact1>Would members of Blue Lives Matter</fact1> <fact2>support every element</fact2> <fact3>of Grand Theft Auto III?</fact3>', 'answer': False, 'explanation': 'No. Blue Lives Matter is a countermovement in the United States that supports police officers and law enforcement personnel. Grand Theft Auto III allows for gratuitous violence against police officers in the game.'}, {'id': 153, 'question': '<fact1>Can</fact1> <fact2>the Dalai Lama</fact2> <fact3>fit</fact3> <fact4>in a car?</fact4>', 'answer': True, 'explanation': 'Yes. The Dalai Lama is a person. Cars are designed for people to sit in them.'}, {'id': 154, 'question': '<fact1>Can actress Dafne Keen</fact1> <fact2>win the Eurovision Song Contest finals</fact2> <fact3>in 2020?</fact3>', 'answer': False, 'explanation': 'No. Contestants must be at least 16 years of age to compete in the finals of Eurovision Song Contest. Dafne Keen is 15 years old in 2020.'}, {'id': 155, 'question': '<fact1>Does United Airlines</fact1> <fact2>have a perfect operation record?</fact2>', 'answer': False, 'explanation': 'No. An airline with a perfect operation record has no crashes or other damaging incidents. United Airlines has had over 30 crash incidents over several decades.'}, {'id': 156, 'question': '<fact1>Can</fact1> <fact2>United States Secretary of State</fact2> <fact3>do crimes</fact3> <fact4>in U.K.</fact4> <fact5>without being arrested?</fact5>', 'answer': True, 'explanation': 'Yes. Diplomatic Immunity allows for diplomats in other countries to not be tried for their transgressions. Countries that signed the Vienna Convention on Diplomatic Relations allow for Diplomatic Immunity. All UN member states besides Palau, The Solomon Islands, and South Sudan have signed the Vienna Convention on Diplomatic Relations treaty. The U.K. is one of the original UN member nations.'}, {'id': 157, 'question': '<fact1>Is</fact1> <fact2>myocardial infarction</fact2> <fact3>a brain problem</fact3>?', 'answer': False, 'explanation': 'No. Myocardial infarction is a problem in the heart. The equivalent in the brain would be similar to a stroke.'}, {'id': 158, 'question': '<fact1>Will Chick Fil A</fact1> <fact2>be open</fact2> <fact3>on Halloween</fact3> <fact4>2021?</fact4>', 'answer': False, 'explanation': 'No. Chick Fil A restaurants close on Sundays. Halloween 2021 falls on a Sunday.'}, {'id': 159, 'question': '<fact1>Was</fact1> <fact2>Godfrey of Bouillon</fact2> <fact3>an Islamaphobe?</fact3>', 'answer': True, 'explanation': 'Yes.  Godfrey of Bouillon lead troops during the Prince\\'s Crusade. The Prince\\'s Crusade was an attempt by Europeans to \"take back\" the city of Jerusalem from Islamic hands.'}, {'id': 160, 'question': '<fact1>Is an espresso</fact1> <fact2>likely</fact2> <fact3>to assuage fear?</fact3>', 'answer': False, 'explanation': 'No. Fear raises heart rate. Caffeine raises heart rate. Coffee may also increase symptoms such as jitteriness and nausea .'}, {'id': 161, 'question': '<fact1>If you add water</fact1> <fact2>to rice pudding</fact2> <fact3>is it horchata?</fact3>', 'answer': False, 'explanation': 'No. Horchata is a drink made from soaking dry rice in water for hours, then pulverizing and straining the mixture and adding spices and sweetener. Rice pudding includes ingredients like eggs, and whole grains of cooked rice.'}, {'id': 162, 'question': '<fact1>Did</fact1> <fact2>Bill Gates</fact2> <fact3>achieve</fact3> <fact4>Latin honors</fact4>?', 'answer': False, 'explanation': 'No. Bill Gates left Harvard after two years. Latin honors are Latin phrases used in some colleges and universities to indicate the level of distinction with which an academic degree has been earned. You cannot earn an academic degree if you drop out of the program.'}, {'id': 163, 'question': '<fact1>Was</fact1> <fact2>Superhero fiction</fact2> <fact3>invented</fact3> <fact4>in the digital format?</fact4>', 'answer': False, 'explanation': \"No. The Golden Age of comics occurred between the 1930's and the 1950's. Shatter was the first digitally drawn, commercially published comic.\"}, {'id': 164, 'question': '<fact1>Are any animals</fact1> <fact2>in Chinese calendar</fact2> <fact3>Chordata?</fact3>', 'answer': True, 'explanation': 'Yes. The chinese zodiac based on the Chinese calendar has a number of animals including dogs and pigs. Chordata is a scientific classification of an animals phylum. The phylum of pigs is Chordata.'}, {'id': 165, 'question': \"<fact1>Was Daniel</fact1> <fact2>thrown into the lion's den</fact2> <fact3>in the New Testament?</fact3>\", 'answer': False, 'explanation': 'No. The Book of Daniel is a book in the Old Testament of the Bible. The Bible is divided into the Old Testament and the New Testament. The New Testament focuses on four Gospels regarding the life of Jesus.'}, {'id': 166, 'question': '<fact1>Will a 2020 elephant pregnancy</fact1> <fact2>last past next year</fact2> <fact3>with 4 solar eclipses?</fact3>', 'answer': False, 'explanation': 'No. The gestation period of elephants are around 95 weeks. The year 2029 is the next year with 4 solar eclipses.'}, {'id': 167, 'question': '<fact1>Does the word swastika</fact1> <fact2>have meaning</fact2> <fact3>in Sanskrit?</fact3>', 'answer': True, 'explanation': 'Yes. In Sanskrit, the word swastika is a combination of ‘su’ (meaning ‘good’) and ‘asti’ (meaning ‘to exist’). This meaning of swastika  from Sanskrit this gets translated as ‘all is well.’ .'}, {'id': 168, 'question': '<fact1>Did Sony</fact1> <fact2>definitively win</fact2> <fact3>the video game war</fact3> <fact4>against Sega?</fact4>', 'answer': True, 'explanation': \"Yes. Sony is the maker of the Playstation which has sold over 108 million PS4 units by March 2020. Sega's last console, the Sega Dreamcast, was discontinued in 2001. Sony Playstation competed with Sega's Dreamcast and Saturn systems in the 1990s. Sega now makes games for its former competitor, Sony, including Team Sonic Racing in 2019. At the height of the console wars, Sega Saturn sold 9.5 million units while Sony Playstation sold 102 million units.\"}, {'id': 169, 'question': '<fact1>Would</fact1> <fact2>an ethics professor</fact2> <fact3>teach a class</fact3> <fact4>on Cezanne?</fact4>', 'answer': False, 'explanation': 'No. Cezanne was an Impressionist painter. Aesthetics is the branch of philosophy that deals with the arts.'}, {'id': 170, 'question': '<fact1>Can a dolphin</fact1> <fact2>keep a diary?</fact2>', 'answer': False, 'explanation': 'No. A diary is a written record. Dolphins cannot write.'}, {'id': 171, 'question': '<fact1>Would Columbus</fact1> <fact2>have discovered</fact2> <fact3>Durian trees</fact3> <fact4>during his 1492 expedition?</fact4>', 'answer': False, 'explanation': 'No. Columbus ended up in the Americas. Durian only exists in Southeast Asia.'}, {'id': 172, 'question': '<fact1>Can you</fact1> <fact2>carry</fact2> <fact3>a Chrysler</fact3> <fact4>in a laptop bag?</fact4>', 'answer': False, 'explanation': 'No. Chrysler manufactures automobiles, which weigh several thousand pounds. Laptop bags are designed to hold laptop computers, which typically weigh under ten pounds.'}, {'id': 173, 'question': '<fact1>Has</fact1> <fact2>every astronaut</fact2> <fact3>survived</fact3> <fact4>their space journey?</fact4>', 'answer': False, 'explanation': 'No. In 1986, the space shuttle Challenger exploded during launch, killing all astronauts aboard. In 2003, the space shuttle Columbia also exploded, again killing its entire crew. Various other space flights have resulted in fatal disasters.'}, {'id': 174, 'question': '<fact1>Do members of the Supreme Court of the United States</fact1> <fact2>have longer terms</fact2> <fact3>than most senators?</fact3>', 'answer': True, 'explanation': 'Yes. Senators, on average, serve for 10 years. Supreme Court Justices serve for their entire life. The average term for a Supreme court justice is 16 years.'}, {'id': 175, 'question': '<fact1>Would Dante</fact1> <fact2>have hypothetically placed</fact2> <fact3>Nostradamus</fact3> <fact4>in 3rd Circle of Hell?</fact4>', 'answer': False, 'explanation': \"No. Nostradamus was a famous seer and court astrologer. Dante's 3rd Circle of Hell is reserved for gluttons. The 8th Circle of Hell is reserved for frauds. Astrology was seen as a valuable skill during Dante's lifetime. Dante places the mystic and prophet Joachim of Flora in the heaven of the sun.\"}, {'id': 176, 'question': '<fact1>Was</fact1> <fact2>Mercedes-Benz</fact2> <fact3>associated</fact3> <fact4>with the Nazis?</fact4>', 'answer': True, 'explanation': 'Yes. During the 1930s, Mercedes-Benz produced the 770 model. The 770 was popular with Nazis, and Adolf Hitler used them as his personal vehicle.'}, {'id': 177, 'question': '<fact1>Is Disneyland Paris</fact1> <fact2>the largest</fact2> <fact3>Disney resort?</fact3>', 'answer': False, 'explanation': 'No. Disneyland Paris contains two parks, several hotels, and a shopping district. By comparison, Walt Disney World in Florida contains four parks, two waterparks, a shopping district, and many hotels. Disney World is bigger than Disneyland Paris Resort and Disneyland California Resort combined.'}, {'id': 178, 'question': '<fact1>Would a Rockette</fact1> <fact2>look odd</fact2> <fact3>with a moustache?</fact3>', 'answer': True, 'explanation': 'Yes. The Rockettes are an American dance group made up of all women. In America, it is uncommon to see a woman with a moustache.'}, {'id': 179, 'question': '<fact1>Can a sea turtle</fact1> <fact2>play tennis</fact2> <fact3>using a tennis racket?</fact3>', 'answer': False, 'explanation': 'No. to play tennis, a human-like hand is needed to properly hold the tennis racket. sea turtles have flippers and not human-like hands.'}, {'id': 180, 'question': '<fact1>Did Donald Trump</fact1> <fact2>come up with</fact2> <fact3>the idea</fact3> <fact4>for the New York Harbor?</fact4>', 'answer': False, 'explanation': 'No. The New York Harbor is at the mouth of the Hudson River. A harbor is a sheltered body of water where boats and ships can be docked. The New York Harbor has been used since colonial era of the 1500s. Donald Trump is a failed business man and 2016 president elect. Donald Trump makes outrageous deceitful claims .'}, {'id': 181, 'question': '<fact1>Would it</fact1> <fact2>be unusual</fact2> <fact3>to use PayPal</fact3> <fact4>for drug deals?</fact4>', 'answer': True, 'explanation': 'Yes. Paypal prohibits the use of their platform for drugs or drug paraphernalia.  Using paypal leaves a digital footprint of any drug purchase.'}, {'id': 182, 'question': '<fact1>Is it</fact1> <fact2>safe</fact2> <fact3>to eat</fact3> <fact4>kidney?</fact4>', 'answer': True, 'explanation': 'Yes. Traditional British cuisine includes \"Kidney Pie\", or \"Steak and Kidney Pie\". Kidney Pie contains kidney.'}, {'id': 183, 'question': '<fact1>Do shrimp</fact1> <fact2>taste best</fact2> <fact3>when cooked</fact3> <fact4>for a long time?</fact4>', 'answer': False, 'explanation': 'No. Shrimp becomes tough and rubbery if cooked for a long time. The ideal texture for shrimp is soft and easily chewed.'}, {'id': 184, 'question': '<fact1>Hypothetically</fact1>, <fact2>will an African elephant</fact2> <fact3>be crushed</fact3> <fact4>by Hulk</fact4> <fact5>on its back?</fact5>', 'answer': False, 'explanation': 'No. The Hulk is a Marvel comics character. The Hulk is said to be around 8 feet tall and weigh around 1400 pounds. An African elephant can carry up to 9,000 kg, or 19,841 pounds.'}, {'id': 185, 'question': \"<fact1>Does Rupert Murdoch's alma mater</fact1> <fact2>have more history</fact2> <fact3>than the USA?</fact3>\", 'answer': True, 'explanation': \"Yes. Rupert Murdoch's alma mater is Worcester College. Worcester College was founded in 1714. The first documented use of the term the United States of America was in a January 2, 1776 letter.\"}, {'id': 186, 'question': '<fact1>Is Saturn</fact1> <fact2>named after</fact2> <fact3>king of gods</fact3> <fact4>in Greek mythology?</fact4>', 'answer': False, 'explanation': \"No. Saturn, the sixth planet from the sun is named after the Roman god Saturn. The Roman god Saturn is derived from its Greek equivalent, Kronos. The king of the gods in Greek mythology was Zeus. Kronos was Zeus's father, and was the leader of the Titans.\"}, {'id': 187, 'question': '<fact1>Does</fact1> <fact2>American Independence Day</fact2> <fact3>occur</fact3> <fact4>during autumn</fact4>?', 'answer': False, 'explanation': 'No. Autumn runs from about September 20 to about December 20. American Independence Day is July 4, over two months before autumn begins.'}, {'id': 188, 'question': '<fact1>Is Albany, Georgia</fact1> <fact2>the most populous</fact2> <fact3>US Albany?</fact3>', 'answer': False, 'explanation': 'No. Albany, Georgia had a population of 75,249 in 2018. Albany, New York had a population of 97,279 in 2018.'}, {'id': 189, 'question': '<fact1>Are</fact1> <fact2>the founders</fact2> <fact3>of Skype</fact3> <fact4>from Asia?</fact4>', 'answer': False, 'explanation': 'No. Skype was created by the Niklas Zennström and the Dane Janus Friis. Niklas Zennström and Dane Janus Friis are from Sweden. Sweden is located in Europe, not Asia.'}, {'id': 190, 'question': '<fact1>Are all Wednesdays</fact1> <fact2>in a year</fact2> <fact3>enough</fact3> <fact4>to read Bible</fact4> <fact5>15 times?</fact5>', 'answer': True, 'explanation': 'Yes. There are 52 Wednesdays in a year. There are 1,248 hours over all the Wednesdays in a year. The Old Testament of the Bible takes an average of 52 hours to read. The New Testament of the Bible takes an average of 18 hours to read.'}, {'id': 191, 'question': '<fact1>Did the butler Eugene Allen</fact1> <fact2>retire</fact2> <fact3>the same year</fact3> <fact4>a centuries-old war ended?</fact4>', 'answer': True, 'explanation': \"Yes. Eugene Allen was a butler at the White House for 34 years until 1986. The United Kingdom and the Kingdom of the Netherlands ended the Three Hundred and Thirty Five Years' Warnin 1986.\"}, {'id': 192, 'question': '<fact1>Did the writer of Christmas Carol</fact1> <fact2>fast</fact2> <fact3>during Ramadan?</fact3>', 'answer': False, 'explanation': 'No. The writer of Christmas carol is Charles Dickens, who is a Christian.  Christians do not fast during Ramadan.'}, {'id': 193, 'question': '<fact1>Is entire Common Era</fact1> <fact2>minuscule</fact2> <fact3>to lifespan</fact3> <fact4>of some trees?</fact4>', 'answer': True, 'explanation': \"Yes. The Common Era has lasted for over 2,000 years as of 2020. A tree named Methuselah, from California's White Mountains, is almost 5,000 years old.\"}, {'id': 194, 'question': '<fact1>Is Tange Sazen</fact1> <fact2>hypothetically an ideal choice</fact2> <fact3>for a secretary job?</fact3>', 'answer': False, 'explanation': 'No. Secretaries are required to type and also read copious amounts of notes. Tange Sazen is a one-eyed, one-armed swordsman in Japanese literature.'}, {'id': 195, 'question': '<fact1>Are Brussels sprouts</fact1> <fact2>particularly good</fact2> <fact3>for adrenal fatigue?</fact3>', 'answer': True, 'explanation': 'Yes. Adenal fatigue is a disorder in which the body does not produce enough hormones and people get tired. Brussels sprout are foods rich in vitamin C. When stress levels rise, the adrenal glands require more Vitamin C and it is used very quickly.'}, {'id': 196, 'question': '<fact1>Does</fact1> <fact2>Adam Sandler</fact2> <fact3>skip</fact3> <fact4>celebrating</fact4> <fact5>Easter?</fact5>', 'answer': True, 'explanation': 'Yes. Adam Sandler is Jewish. Jewish religious people do not celebrate Easter.'}, {'id': 197, 'question': '<fact1>Does March</fact1> <fact2>begin on the same day of the week</fact2> <fact3>as February</fact3> <fact4>during leap years?</fact4>', 'answer': False, 'explanation': 'No. During normal years, February has exactly 28 days, so March begins on the same day of the week as February. However, on leap years, February has an extra day, so March begins the next day of the week from whichever day started February.'}, {'id': 198, 'question': \"<fact1>Is Shiva's divine dance</fact1> <fact2>an ancient physical fitness pose?</fact2>\", 'answer': False, 'explanation': \"No. In yoga as exercise, the pose called Natarajasara represents Shiva's divine dance. This pose is new, originating in the early 20th century. Exercise is an activity that maintains physical fitness.\"}, {'id': 199, 'question': '<fact1>Was</fact1> <fact2>Land of Israel</fact2> <fact3>in possession</fact3> <fact4>of an Islamic empire</fact4> <fact5>in 16th century?</fact5>', 'answer': True, 'explanation': 'Yes. Land of Israel was controlled by the Ottoman Empire in 16th century.  The religion of Ottoman Empire was Sunni Islam.'}]\n",
      "Question: <fact1>At a presentation about post traumatic stress disorder</fact1>, <fact2>would Ariana Grande</fact2> <fact3>be a topic of relevance?</fact3>\n",
      "Answer: <fact2>Ariana Grande</fact2> was performing in Manchester in 2017 when explosives were set off in an act of terrorism.\n",
      "<fact2>Ariana Grande</fact2> has spoken openly about her trauma experience and her <fact1>PTSD</fact1> regarding the Manchester Bombing.\n",
      "So the answer is {true}.\n",
      "\n",
      "Question: <fact1>Has the Indian Ocean garbage patch</fact1> <fact2>not completed</fact2> <fact3>two full rotations of debris</fact3> <fact4>since its discovery?</fact4>\n",
      "Answer: The <fact1>Indian Ocean garbage patch</fact1> was discovered in 2010\n",
      "The Indian Ocean garbage patch takes 6 years to complete a <fact3>circulation</fact3> \n",
      "So the answer is {true}.\n",
      "\n",
      "Question: <fact1>Was the Second Amendment</fact1> <fact2>to the United States Constitution</fact2> <fact3>written without consideration</fact3> <fact4>for black Americans?</fact4>\n",
      "Answer: The writers of the <fact2>Constitutional Amendments</fact2> did not view <fact4>black people</fact4> as legitimate human beings.\n",
      "The writers of the <fact2>Constitutional Amendments</fact2> believed that slavery benefited <fact2>black slaves</fact2>.\n",
      "The <fact2>Constitutional Amendments</fact2> were written for people that the writers considered human.\n",
      "So the answer is {true}.\n",
      "\n",
      "Question: <fact1>Are Doctors of Homeopathy</fact1> <fact2>more likely</fact2> <fact3>than Doctors of Internal Medicine</fact3> <fact4>to recommend Quartz as a treatment?</fact4>\n",
      "Answer: <fact1>Doctors of Homeopathy</fact1> are practitioners of \"<fact1>alternative medicine</fact1>\" \n",
      "In <fact1>alternative medicine</fact1> practices, <fact4>Quartz</fact4> is believed to have powers.\n",
      "<fact3>Doctors of Internal Medicine</fact3> have completed a medical residency and do not recommend <fact1>alternative medicine</fact1>.\n",
      "So the answer is {true}.\n",
      "\n",
      "Question: <fact1>Is Benjamin Franklin</fact1> <fact2>a prime candidate</fact2> <fact3>to have his statues removed</fact3> <fact4>by Black Lives Matter movement?</fact4>\n",
      "Answer: The <fact4>Black Lives Matter movement</fact4> is a social movement advocating for racial equality.\n",
      "<fact1>Benjamin Franklin</fact1>, a famous founding father, has <fact3>his image on many monuments</fact3> and on American currency.\n",
      "Members of the <fact4>Black Lives Matter movement</fact4> petitioned for statues of Christopher Columbus to be removed due to his subjugation of Native Americans.\n",
      "<fact1>Benjamin Franklin's</fact1> 1730s newspaper, The Philadelphia Gazette, posted ads for black slaves.\n",
      "<fact1>Benjamin Franklin</fact1> owned two slaves, George and King, who worked as personal servants.\n",
      "So the answer is {true}.\n",
      "\n",
      "Question: <fact1>Does the United States Department of Education</fact1> <fact2>oversee services</fact2> <fact3>benefiting undocumented migrants?</fact3> \n",
      "Answer: The <fact1>United States Department of Education</fact1> <fact2>oversees public education</fact2> across the United States.\n",
      "Public education is a service.\n",
      "Public education services are given to students of migrant families that may be <fact3>undocumented</fact3>.\n",
      "So the answer is {true}.\n",
      "\n",
      "Question: <fact1>Could Bernie Sanders</fact1> <fact2>visit the Metropolitan Museum of Art</fact2> <fact3>twenty times</fact3> <fact4>for under two hundred dollars?</fact4>\n",
      "Answer: <fact1>Bernie Sanders</fact1> is a senior citizen\n",
      "Senior citizens from outside NY, NJ, or CT must pay $17 per <fact2>visit</fact2>\n",
      "So the answer is {false}.\n",
      "\n",
      "Question: <fact1>Would Eric Clapton's mother</fact1> <fact2>hypothetically be unable</fact2> <fact3>to legally purchase cigarettes</fact3> <fact4>in the USA</fact4> <fact5>at his birth?</fact5>\n",
      "Answer: <fact1>Eric Clapton's mother</fact1> was 16 years old <fact5>at the time of his birth.</fact5>\n",
      "As of 2020, federal law required states comply with a <fact3>minimum age of 21 years for sale/purchase of tobacco products.</fact3>\n",
      "So the answer is {true}.\n",
      "\n",
      "Question: <fact1>Would a 900,000 pound net worth person</fact1> <fact2>be an American billionaire</fact2> <fact3>if they exchange currency</fact3> <fact4>June 2020?</fact4>\n",
      "Answer: The <fact3>exchange rate</fact3> in <fact4>June of 2020</fact4> between dollars and pounds is 1 Euro= 1.23 dollar.\n",
      "<fact1>900000 pounds</fact1> is equal to about <fact2>1,107,000.00</fact2>\n",
      "So the answer is {true}.\n",
      "\n",
      "\n",
      "Initialized new save file with headers at: /Users/log/Github/textual_grounding/logan/results/StrategyQA/4o/grounded_fact/fewshot_clause.txt_4o_1017_214836.csv\n",
      "Already answered IDs: set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [05:42<00:00,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. 200 new answers saved to /Users/log/Github/textual_grounding/logan/results/StrategyQA/4o/grounded_fact/fewshot_clause.txt_4o_1017_214836.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "time = datetime.datetime.now().strftime(\"%m%d_%H%M%S\")\n",
    "# time = '1003_002310'\n",
    "project_root = '/Users/log/Github/textual_grounding/'\n",
    "dataset = 'StrategyQA'\n",
    "\n",
    "llm_model = '4o'\n",
    "# prompt_type = 'log_cot'\n",
    "prompt_type = 'stripped_clause'\n",
    "# prompt_type = 'fs_clause_inst'\n",
    "# few_shot_txt = 'clause_fs.txt'\n",
    "few_shot_txt = 'fewshot_clause.txt'\n",
    "\n",
    "# Paths\n",
    "data_path = os.path.join(project_root, 'data', dataset, 'test_modified.jsonl')\n",
    "# data_path = os.path.join(project_root, 'data', dataset, 'test.json')\n",
    "# data_path = '/Users/log/Github/textual_grounding/logan/results/GSM8K/llama/mermaid/mermaid_get_graph_llama3.1_20240924_001821.csv'\n",
    "\n",
    "fewshot_prompt_path = os.path.join(project_root, \"prompt\", dataset, few_shot_txt)\n",
    "# fewshot_prompt_path = '/Users/log/Github/textual_grounding/prompt/GSM8K/fewshot_mermaid_full.txt'\n",
    "save_dir = os.path.join(project_root, 'logan/results', dataset, f'{llm_model}/grounded_fact')\n",
    "os.makedirs(save_dir, exist_ok=True)  # Ensure the directory exists\n",
    "save_path = os.path.join(save_dir, f'{few_shot_txt}_{llm_model}_{time}.csv')\n",
    "\n",
    "# ids, questions = load_data_deterministic(data_path, sample_size=200)\n",
    "ids, questions = load_data_size_specific(data_path, sample_size=200)\n",
    "few_shot_prompt = load_few_shot_prompt(fewshot_prompt_path)\n",
    "\n",
    "print(few_shot_prompt)\n",
    "# raise ValueError('stop')\n",
    "\n",
    "initialize_save_file(save_path)\n",
    "already_answered_ids = load_already_answered_ids(save_path)\n",
    "\n",
    "\n",
    "ids_answered, questions_answered, answers = query_llm(\n",
    "    llm_model=llm_model,\n",
    "    ids=ids,\n",
    "    questions=questions,\n",
    "    few_shot_prompt=few_shot_prompt,\n",
    "    prompt_type=prompt_type,\n",
    "    save_path=save_path,\n",
    "    already_answered_ids=already_answered_ids\n",
    ")\n",
    "\n",
    "print(f\"Processing complete. {len(ids_answered)} new answers saved to {save_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogiQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Add the directory containing logiqa.py to the Python path\n",
    "logiqa_path = \"/Users/log/Github/textual_grounding/data/logiqa\"\n",
    "sys.path.append(logiqa_path)\n",
    "\n",
    "# Import the LogiQA class from the logiqa module if needed\n",
    "from logiqa import LogiQA\n",
    "\n",
    "# Load the dataset using Hugging Face load_dataset method\n",
    "dataset = load_dataset('/Users/log/Github/textual_grounding/data/logiqa/logiqa.py', split='test')\n",
    "\n",
    "# Print out the first 5 examples from the test set\n",
    "for idx in range(5):\n",
    "    example = dataset[idx]\n",
    "    print(f\"Example {idx + 1}:\")\n",
    "    print(f\"Context: {example['context']}\")\n",
    "    print(f\"Query: {example['query']}\")\n",
    "    print(f\"Options: {example['options']}\")\n",
    "    print(f\"Correct Option Index: {example['correct_option']}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset (adjust the path as needed)\n",
    "dataset = load_dataset('/Users/log/Github/textual_grounding/data/logiqa/logiqa.py', split='test')\n",
    "\n",
    "# Prepare to write the first 300 examples to a JSONL file\n",
    "output_file = 'logiqa_300_examples.jsonl'\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for idx, example in enumerate(dataset):\n",
    "        if idx >= 300:\n",
    "            break\n",
    "        \n",
    "        # Create the \"question\" field by concatenating context, query, and options\n",
    "        context = example['context']\n",
    "        query = example['query']\n",
    "        options = example['options']\n",
    "        options_str = \" \".join([f\"({chr(65 + i)}) {opt}\" for i, opt in enumerate(options)])\n",
    "        question = f\"{context} {query}\\n{options_str}\"\n",
    "        \n",
    "        # Create the dictionary for the current example\n",
    "        example_dict = {\n",
    "            \"id\": idx,\n",
    "            \"question\": question,\n",
    "            \"answer\": chr(65 + example['correct_option'])  # Convert index to letter (A, B, C, D)\n",
    "        }\n",
    "        \n",
    "        # Write the example as a JSON object to the JSONL file\n",
    "        f.write(json.dumps(example_dict) + '\\n')\n",
    "\n",
    "print(f\"Saved 300 examples to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grounded Visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total QA Pairs Parsed: 200\n",
      "Total Ground Truth Entries: 2290\n",
      "id: 0: true\n",
      "id: 1: false\n",
      "id: 2: false\n",
      "id: 3: true\n",
      "id: 4: false\n",
      "id: 5: false\n",
      "id: 6: false\n",
      "id: 7: true\n",
      "id: 8: false\n",
      "id: 9: true\n",
      "id: 10: false\n",
      "id: 11: false\n",
      "id: 12: false\n",
      "id: 13: false\n",
      "id: 14: true\n",
      "id: 15: false\n",
      "id: 16: true\n",
      "id: 17: true\n",
      "id: 18: true\n",
      "id: 19: false\n",
      "id: 20: true\n",
      "id: 21: false\n",
      "id: 22: true\n",
      "id: 23: false\n",
      "id: 24: false\n",
      "id: 25: false\n",
      "id: 26: false\n",
      "id: 27: false\n",
      "id: 28: true\n",
      "id: 29: true\n",
      "id: 30: false\n",
      "id: 31: true\n",
      "id: 32: true\n",
      "id: 33: false\n",
      "id: 34: true\n",
      "id: 35: false\n",
      "id: 36: true\n",
      "id: 37: false\n",
      "id: 38: false\n",
      "id: 39: true\n",
      "id: 40: true\n",
      "id: 41: true\n",
      "id: 42: true\n",
      "id: 43: true\n",
      "id: 44: true\n",
      "id: 45: true\n",
      "id: 46: false\n",
      "id: 47: true\n",
      "id: 48: false\n",
      "id: 49: true\n",
      "id: 50: true\n",
      "id: 51: false\n",
      "id: 52: false\n",
      "id: 53: true\n",
      "id: 54: true\n",
      "id: 55: true\n",
      "id: 56: false\n",
      "id: 57: false\n",
      "id: 58: false\n",
      "id: 59: false\n",
      "id: 60: false\n",
      "id: 61: false\n",
      "id: 62: false\n",
      "id: 63: true\n",
      "id: 64: false\n",
      "id: 65: false\n",
      "id: 66: false\n",
      "id: 67: false\n",
      "id: 68: true\n",
      "id: 69: true\n",
      "id: 70: false\n",
      "id: 71: false\n",
      "id: 72: true\n",
      "id: 73: true\n",
      "id: 74: false\n",
      "id: 75: true\n",
      "id: 76: false\n",
      "id: 77: true\n",
      "id: 78: true\n",
      "id: 79: false\n",
      "id: 80: false\n",
      "id: 81: true\n",
      "id: 82: false\n",
      "id: 83: false\n",
      "id: 84: false\n",
      "id: 85: true\n",
      "id: 86: false\n",
      "id: 87: false\n",
      "id: 88: true\n",
      "id: 89: true\n",
      "id: 90: true\n",
      "id: 91: true\n",
      "id: 92: false\n",
      "id: 93: false\n",
      "id: 94: false\n",
      "id: 95: true\n",
      "id: 96: false\n",
      "id: 97: false\n",
      "id: 98: false\n",
      "id: 99: false\n",
      "id: 100: true\n",
      "id: 101: true\n",
      "id: 102: false\n",
      "id: 103: true\n",
      "id: 104: false\n",
      "id: 105: false\n",
      "id: 106: true\n",
      "id: 107: false\n",
      "id: 108: false\n",
      "id: 109: true\n",
      "id: 110: true\n",
      "id: 111: true\n",
      "id: 112: true\n",
      "id: 113: true\n",
      "id: 114: false\n",
      "id: 115: false\n",
      "id: 116: false\n",
      "id: 117: true\n",
      "id: 118: true\n",
      "id: 119: false\n",
      "id: 120: false\n",
      "id: 121: true\n",
      "id: 122: false\n",
      "id: 123: true\n",
      "id: 124: true\n",
      "id: 125: true\n",
      "id: 126: true\n",
      "id: 127: true\n",
      "id: 128: false\n",
      "id: 129: false\n",
      "id: 130: false\n",
      "id: 131: true\n",
      "id: 132: false\n",
      "id: 133: true\n",
      "id: 134: false\n",
      "id: 135: false\n",
      "id: 136: true\n",
      "id: 137: false\n",
      "id: 138: true\n",
      "id: 139: true\n",
      "id: 140: true\n",
      "id: 141: false\n",
      "id: 142: true\n",
      "id: 143: true\n",
      "id: 144: false\n",
      "id: 145: false\n",
      "id: 146: false\n",
      "id: 147: false\n",
      "id: 148: false\n",
      "id: 149: false\n",
      "id: 150: false\n",
      "id: 151: true\n",
      "id: 152: false\n",
      "id: 153: true\n",
      "id: 154: false\n",
      "id: 155: false\n",
      "id: 156: true\n",
      "id: 157: false\n",
      "id: 158: false\n",
      "id: 159: true\n",
      "id: 160: false\n",
      "id: 161: false\n",
      "id: 162: false\n",
      "id: 163: false\n",
      "id: 164: true\n",
      "id: 165: false\n",
      "id: 166: false\n",
      "id: 167: true\n",
      "id: 168: true\n",
      "id: 169: false\n",
      "id: 170: false\n",
      "id: 171: false\n",
      "id: 172: false\n",
      "id: 173: false\n",
      "id: 174: true\n",
      "id: 175: false\n",
      "id: 176: true\n",
      "id: 177: false\n",
      "id: 178: true\n",
      "id: 179: false\n",
      "id: 180: false\n",
      "id: 181: true\n",
      "id: 182: true\n",
      "id: 183: false\n",
      "id: 184: false\n",
      "id: 185: true\n",
      "id: 186: false\n",
      "id: 187: false\n",
      "id: 188: false\n",
      "id: 189: false\n",
      "id: 190: true\n",
      "id: 191: true\n",
      "id: 192: false\n",
      "id: 193: true\n",
      "id: 194: false\n",
      "id: 195: true\n",
      "id: 196: true\n",
      "id: 197: false\n",
      "id: 198: false\n",
      "id: 199: true\n",
      "HTML content has been successfully written to stripped_response_StrategyQA_4o.html\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import json  # For handling JSONL\n",
    "import os\n",
    "\n",
    "def add_color_to_tags_new(text):\n",
    "    # Find all unique tags in the text using regex\n",
    "    tags = set(re.findall(r'<([A-Za-z]+\\d*)>', text))\n",
    "\n",
    "    # Predefined color palette\n",
    "    color_palette = [\n",
    "        'lightyellow', 'lightblue', 'lightgreen', 'lightcoral',\n",
    "        'lightcyan', 'lightpink', 'lightsalmon', 'lightgray',\n",
    "        'lightgoldenrodyellow', 'lightseagreen', 'lightskyblue',\n",
    "        'lightsteelblue'\n",
    "    ]\n",
    "\n",
    "    # Dictionary to hold tag-color mapping\n",
    "    tag_color_mapping = {}\n",
    "\n",
    "    # Assign colors to tags, cycling through the color palette if necessary\n",
    "    for i, tag in enumerate(sorted(tags)):\n",
    "        color = color_palette[i % len(color_palette)]\n",
    "        tag_color_mapping[tag] = color\n",
    "\n",
    "    # Function to replace tags with styled spans including class names\n",
    "    def replace_tag(match):\n",
    "        tag = match.group(1)\n",
    "        content = match.group(2)\n",
    "        color = tag_color_mapping.get(tag, 'lightgray')  # Default color if not found\n",
    "        return f'<span class=\"{tag}\" style=\"background-color: {color}; font-weight: bold;\">{content}</span>'\n",
    "\n",
    "    # Regex to find tags and replace them with styled spans\n",
    "    tag_regex = re.compile(r'<([A-Za-z]+\\d*)>\\s*([\\s\\S]*?)\\s*</\\1>')\n",
    "\n",
    "    # Replace all tags with styled spans\n",
    "    text = tag_regex.sub(replace_tag, text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def parse_csv_file(file_path):\n",
    "    qa_pairs = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            question = row.get('question', 'No question found.').strip()\n",
    "            answer_text = row.get('answer', 'No answer found.').strip()\n",
    "            id_ = row.get('id')\n",
    "            if id_ is not None:\n",
    "                try:\n",
    "                    id_int = int(id_)\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping a row due to invalid 'id' (not an integer): {id_}\")\n",
    "                    continue\n",
    "                qa_pairs.append((id_int, question, answer_text))\n",
    "            else:\n",
    "                # Handle cases without 'id' by skipping\n",
    "                print(f\"Skipping a row due to missing 'id': {row}\")\n",
    "    return qa_pairs\n",
    "\n",
    "\n",
    "def read_ground_truth(jsonl_path):\n",
    "    ground_truth = {}\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            id_ = data.get('id')\n",
    "            answer = str(data.get('answer')).lower()\n",
    "            if id_ is not None and answer is not None:\n",
    "\n",
    "                ground_truth[id_] = answer\n",
    "            else:\n",
    "                print(f\"Invalid ground truth entry: {data}\")\n",
    "    return ground_truth\n",
    "\n",
    "\n",
    "def create_highlight_html_new(qa_pairs, ground_truth):\n",
    "    html_content = \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <title>Question and Answer Highlights</title>\n",
    "        <style>\n",
    "            body {\n",
    "                font-family: Arial, sans-serif;\n",
    "                margin: 20px;\n",
    "                background-color: #f0f0f0;\n",
    "            }\n",
    "            .container {\n",
    "                background-color: #ffffff;\n",
    "                padding: 20px;\n",
    "                margin-bottom: 20px;\n",
    "                border-radius: 8px;\n",
    "                box-shadow: 0 2px 5px rgba(0,0,0,0.1);\n",
    "            }\n",
    "            .question {\n",
    "                font-size: 1.2em;\n",
    "                margin-bottom: 10px;\n",
    "            }\n",
    "            .full-response, .final-answer, .ground-truth-answer {\n",
    "                margin-bottom: 10px;\n",
    "                white-space: pre-wrap; /* Add this line to preserve newlines */\n",
    "            }\n",
    "            .final-answer {\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            .ground-truth-answer {\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            /* Styles for the highlighted spans */\n",
    "            .highlighted {\n",
    "                padding: 2px 4px;\n",
    "                border-radius: 3px;\n",
    "                display: inline-block;\n",
    "            }\n",
    "            /* Styles for the summary section */\n",
    "            .summary {\n",
    "                background-color: #e0ffe0;\n",
    "                padding: 15px;\n",
    "                border: 2px solid #00cc00;\n",
    "                border-radius: 8px;\n",
    "                font-size: 1.2em;\n",
    "                margin-top: 30px;\n",
    "            }\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "    <h1>Question and Answer Highlights</h1>\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize counters for correct and total answers\n",
    "    correct_answers = 0\n",
    "    total_answers = 0\n",
    "\n",
    "    for i, (id_, question, answer_text) in enumerate(qa_pairs, 1):\n",
    "        try:\n",
    "            full_response = answer_text.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot process answer for question ID {id_}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Apply color to tags in the full_response\n",
    "        highlighted_response = add_color_to_tags_new(full_response)\n",
    "        \n",
    "        # Replace newline characters with <br> tags to ensure they are rendered in HTML\n",
    "        highlighted_response = highlighted_response.replace('\\n', '<br>')\n",
    "\n",
    "        # Extract the final answer within curly brackets {}\n",
    "        final_answer_match = re.search(r'\\{([^}]+)\\}', full_response)\n",
    "        \n",
    "        if final_answer_match:\n",
    "            final_answer = final_answer_match.group(1).replace(',', '').replace('$', '').strip().lower()\n",
    "            if \"no\" in final_answer:\n",
    "                final_answer = \"false\"\n",
    "            elif \"yes\" in final_answer:\n",
    "                final_answer = \"true\"\n",
    "        else:\n",
    "            final_answer = \"\"\n",
    "\n",
    "        # Retrieve ground truth answer\n",
    "        gt_answer = str(ground_truth.get(id_))\n",
    "        print(f\"id: {id_}: {gt_answer}\")\n",
    "        if gt_answer is None:\n",
    "            gt_answer_display = \"<span style='color: gray;'>Ground truth not available.</span>\"\n",
    "            is_correct = False\n",
    "        else:\n",
    "            # Normalize both final_answer and gt_answer for comparison\n",
    "            try:\n",
    "                # Attempt to convert to float for numerical comparison\n",
    "                final_answer_num = float(final_answer.replace(',', '').replace('$', ''))\n",
    "                if isinstance(gt_answer, list):\n",
    "                    # If ground truth is a list, take the first element\n",
    "                    gt_answer_num = float(gt_answer[0].replace(',', '').replace('$', ''))\n",
    "                else:\n",
    "                    gt_answer_num = float(gt_answer.replace(',', '').replace('$', ''))\n",
    "                is_correct = final_answer_num == gt_answer_num\n",
    "                # Format numbers with commas and two decimal places if needed\n",
    "                if final_answer_num.is_integer():\n",
    "                    final_answer_display = f\"{int(final_answer_num):,}\"\n",
    "                else:\n",
    "                    final_answer_display = f\"{final_answer_num:,.2f}\"\n",
    "                if gt_answer_num.is_integer():\n",
    "                    gt_answer_display = f\"{int(gt_answer_num):,}\"\n",
    "                else:\n",
    "                    gt_answer_display = f\"{gt_answer_num:,.2f}\"\n",
    "            except ValueError:\n",
    "                # In case conversion fails, fallback to string comparison\n",
    "                is_correct = final_answer == gt_answer\n",
    "                final_answer_display = final_answer\n",
    "                gt_answer_display = gt_answer\n",
    "\n",
    "        # Style the final answer based on correctness\n",
    "        if is_correct:\n",
    "            highlighted_final_answer = f\"<span style='font-size:1.1em; color: green;'>{final_answer_display}</span>\"\n",
    "            correct_answers += 1\n",
    "        else:\n",
    "            highlighted_final_answer = f\"<span style='font-size:1.1em; color: red;'>{final_answer_display}</span>\"\n",
    "        total_answers += 1\n",
    "\n",
    "        # Display ground truth answer\n",
    "        if gt_answer is not None:\n",
    "            ground_truth_html = f\"<div class='ground-truth-answer'><strong>Ground Truth Answer:</strong> {gt_answer_display}</div>\"\n",
    "        else:\n",
    "            ground_truth_html = f\"<div class='ground-truth-answer'><strong>Ground Truth Answer:</strong> Not available.</div>\"\n",
    "\n",
    "        # Build the HTML structure\n",
    "        html_content += f\"<div class='container'>\"\n",
    "        html_content += f\"<div class='question'><strong>Question:</strong> {question}</div>\"\n",
    "        html_content += f\"<div class='full-response'>{highlighted_response}</div>\"\n",
    "        html_content += f\"<div class='final-answer'><strong>Final Answer:</strong> {highlighted_final_answer}</div>\"\n",
    "        html_content += f\"{ground_truth_html}\"\n",
    "        html_content += \"</div>\\n\"\n",
    "\n",
    "    # After processing all QA pairs, add the summary section\n",
    "    summary_percentage = (correct_answers / total_answers * 100) if total_answers > 0 else 0\n",
    "    summary_html = f\"\"\"\n",
    "    <div class='summary'>\n",
    "        <strong>Summary:</strong> Correct Answers: {correct_answers} / {total_answers} ({summary_percentage:.2f}%)\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    # Close the HTML tags\n",
    "    html_content += \"\"\"\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    output_html = summary_html + html_content\n",
    "    return output_html\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Replace these paths with your actual file paths\n",
    "    # input_csv = '/Users/log/Github/textual_grounding/logan/results/StrategyQA/4o/grounded_fact/fewshot_cot.txt_4o_1017_203601.csv'  \n",
    "    input_csv = '/Users/log/Github/textual_grounding/logan/results/StrategyQA/4o/grounded_fact/stripped_clause.txt_4o_1017_214836.csv'  \n",
    "    ground_truth_file = '/Users/log/Github/textual_grounding/data/StrategyQA/test.jsonl'  # Path to the ground truth JSONL file\n",
    "    output_file = 'stripped_response_StrategyQA_4o.html'  # Desired output HTML file path\n",
    "\n",
    "    # Check if input files exist\n",
    "    if not os.path.isfile(input_csv):\n",
    "        print(f\"Input CSV file not found: {input_csv}\")\n",
    "        return\n",
    "    if not os.path.isfile(ground_truth_file):\n",
    "        print(f\"Ground truth JSON file not found: {ground_truth_file}\")\n",
    "        return\n",
    "\n",
    "    # Parse the input CSV file to extract IDs, questions, and answers\n",
    "    qa_pairs = parse_csv_file(input_csv)\n",
    "    print(f\"Total QA Pairs Parsed: {len(qa_pairs)}\")  # Debug: Print the number of QA pairs parsed\n",
    "\n",
    "    # Read the ground truth answers\n",
    "    ground_truth = read_ground_truth(ground_truth_file)\n",
    "    print(f\"Total Ground Truth Entries: {len(ground_truth)}\")  # Debug: Print the number of ground truth entries\n",
    "\n",
    "    # Check if any QA pairs were found\n",
    "    if not qa_pairs:\n",
    "        print(\"No question-answer pairs were found in the input file.\")\n",
    "        return\n",
    "\n",
    "    # Generate the HTML content\n",
    "    html_content = create_highlight_html_new(qa_pairs, ground_truth)\n",
    "\n",
    "    # Write the HTML content to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(html_content)\n",
    "\n",
    "    print(f\"HTML content has been successfully written to {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoT - Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total QA Pairs Parsed: 200\n",
      "Total Ground Truth Entries: 220\n",
      "HTML content has been successfully written to stripped_response_4o_rGSMK.html\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_final_answer(answer_text):\n",
    "    # Regex pattern to match anything inside curly braces\n",
    "    final_answer_pattern = re.compile(r'\\{([^}]+)\\}')\n",
    "    \n",
    "    # Find all matches of text inside curly braces\n",
    "    matches = list(final_answer_pattern.finditer(answer_text))\n",
    "    \n",
    "    # If we have at least one match, get the content of the last match\n",
    "    if matches:\n",
    "        final_answer = matches[-1].group(1).strip()\n",
    "        return final_answer\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def parse_csv_file(file_path):\n",
    "    qa_pairs = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            question = row.get('question', 'No question found.').strip()\n",
    "            answer_text = row.get('answer', 'No answer found.').strip()\n",
    "            id_ = row.get('id')\n",
    "            if id_ is not None:\n",
    "                try:\n",
    "                    id_int = int(id_)\n",
    "                    qa_pairs.append((id_int, question, answer_text))\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping a row due to invalid 'id' (not an integer): {id_}\")\n",
    "            else:\n",
    "                print(f\"Skipping a row due to missing 'id': {row}\")\n",
    "    return qa_pairs\n",
    "\n",
    "def read_ground_truth(jsonl_path):\n",
    "    ground_truth = {}\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            id_ = data.get('index')\n",
    "            answer = data.get('answer')\n",
    "            if id_ is not None and answer is not None:\n",
    "                ground_truth[id_] = answer\n",
    "            else:\n",
    "                print(f\"Invalid ground truth entry: {data}\")\n",
    "    return ground_truth\n",
    "\n",
    "def create_simple_html(qa_pairs, ground_truth):\n",
    "    html_content = \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <title>Question and Answer Comparison</title>\n",
    "        <style>\n",
    "            body {\n",
    "                font-family: Arial, sans-serif;\n",
    "                margin: 20px;\n",
    "                background-color: #f9f9f9;\n",
    "            }\n",
    "            .container {\n",
    "                background-color: #ffffff;\n",
    "                padding: 15px 20px;\n",
    "                margin-bottom: 15px;\n",
    "                border-radius: 6px;\n",
    "                box-shadow: 0 1px 3px rgba(0,0,0,0.1);\n",
    "            }\n",
    "            .question {\n",
    "                font-size: 1.1em;\n",
    "                margin-bottom: 10px;\n",
    "                color: #333333;\n",
    "            }\n",
    "            .answer-text {\n",
    "                background-color: #f4f4f4;\n",
    "                padding: 10px;\n",
    "                border-left: 4px solid #2196F3;\n",
    "                margin-bottom: 10px;\n",
    "                white-space: pre-wrap;\n",
    "                font-family: Consolas, \"Courier New\", monospace;\n",
    "            }\n",
    "            .final-answer, .ground-truth-answer {\n",
    "                margin-bottom: 5px;\n",
    "            }\n",
    "            .final-answer span.correct {\n",
    "                color: green;\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            .final-answer span.incorrect {\n",
    "                color: red;\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            .ground-truth-answer {\n",
    "                color: #555555;\n",
    "            }\n",
    "            .summary {\n",
    "                background-color: #e0ffe0;\n",
    "                padding: 15px;\n",
    "                border: 2px solid #00cc00;\n",
    "                border-radius: 8px;\n",
    "                font-size: 1.2em;\n",
    "                margin-top: 30px;\n",
    "            }\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "    <h1>Question and Answer Comparison</h1>\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize counters for correct and total answers\n",
    "    correct_answers = 0\n",
    "    total_answers = 0\n",
    "\n",
    "    for id_, question, answer_text in qa_pairs:\n",
    "        final_answer = extract_final_answer(answer_text)\n",
    "        gt_answer = ground_truth.get(id_)\n",
    "\n",
    "        if gt_answer is None:\n",
    "            gt_answer_display = \"<span style='color: gray;'>Ground truth not available.</span>\"\n",
    "            is_correct = False\n",
    "        else:\n",
    "            # Normalize both final_answer and gt_answer for comparison\n",
    "            try:\n",
    "                final_answer_num = float(final_answer.replace(',', '').replace('$', ''))\n",
    "                if isinstance(gt_answer, list):  # Handle list of answers if applicable\n",
    "                    gt_answer_num = float(gt_answer[0].replace(',', '').replace('$', ''))\n",
    "                else:\n",
    "                    gt_answer_num = float(str(gt_answer).replace(',', '').replace('$', ''))\n",
    "                is_correct = final_answer_num == gt_answer_num\n",
    "                # Format numbers with commas and two decimal places if needed\n",
    "                final_answer_display = f\"{final_answer_num:,.2f}\" if not final_answer_num.is_integer() else f\"{int(final_answer_num):,}\"\n",
    "                gt_answer_display = f\"{gt_answer_num:,.2f}\" if not gt_answer_num.is_integer() else f\"{int(gt_answer_num):,}\"\n",
    "            except (ValueError, TypeError):\n",
    "                # Fallback to string comparison if conversion fails\n",
    "                is_correct = final_answer.strip().lower() == str(gt_answer).strip().lower()\n",
    "                final_answer_display = final_answer\n",
    "                gt_answer_display = gt_answer\n",
    "\n",
    "        # Style the final answer based on correctness\n",
    "        if is_correct:\n",
    "            final_answer_html = f\"<span class='correct'>{final_answer_display}</span>\"\n",
    "            correct_answers += 1\n",
    "        else:\n",
    "            final_answer_html = f\"<span class='incorrect'>{final_answer_display}</span>\"\n",
    "        total_answers += 1\n",
    "\n",
    "        # Display ground truth answer\n",
    "        if gt_answer is not None:\n",
    "            ground_truth_html = f\"<div class='ground-truth-answer'><strong>Ground Truth Answer:</strong> {gt_answer_display}</div>\"\n",
    "        else:\n",
    "            ground_truth_html = f\"<div class='ground-truth-answer'><strong>Ground Truth Answer:</strong> Not available.</div>\"\n",
    "\n",
    "        # Build the HTML structure for each QA pair\n",
    "        html_content += f\"<div class='container'>\"\n",
    "        html_content += f\"<div class='question'><strong>Question:</strong> {question}</div>\"\n",
    "        html_content += f\"<div class='answer-text'><strong>Model Response:</strong><br>{answer_text}</div>\"\n",
    "        html_content += f\"<div class='final-answer'><strong>Final Answer:</strong> {final_answer_html}</div>\"\n",
    "        html_content += f\"{ground_truth_html}\"\n",
    "        html_content += \"</div>\\n\"\n",
    "\n",
    "    # Add the summary section\n",
    "    summary_percentage = (correct_answers / total_answers * 100) if total_answers > 0 else 0\n",
    "    summary_html = f\"\"\"\n",
    "    <div class='summary'>\n",
    "        <strong>Summary:</strong> Correct Answers: {correct_answers} / {total_answers} ({summary_percentage:.2f}%)\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    # html_content += summary_html\n",
    "\n",
    "    # Close the HTML tags\n",
    "    html_content += \"\"\"\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    final_content = summary_html + html_content\n",
    "    return final_content\n",
    "\n",
    "def main():\n",
    "    input_csv = '/Users/log/Github/textual_grounding/logan/results/StrategyQA/4o/grounded_fact/stripped_clause.txt_4o_1017_214836.csv'  \n",
    "    # input_csv = '/Users/log/Github/textual_grounding/logan/results/p_GSM8K/4o/grounded_fact/3shot_grounded.txt_4o_1013_233035.csv'  \n",
    "    ground_truth_file = '/Users/log/Github/textual_grounding/data/p_GSM8K/r-gsm.jsonl'  # Path to the ground truth JSONL file\n",
    "    output_file = 'stripped_response_4o_rGSMK.html'  # Desired output HTML file path\n",
    "\n",
    "    # Check if input files exist\n",
    "    if not os.path.isfile(input_csv):\n",
    "        print(f\"Input CSV file not found: {input_csv}\")\n",
    "        return\n",
    "    if not os.path.isfile(ground_truth_file):\n",
    "        print(f\"Ground truth JSONL file not found: {ground_truth_file}\")\n",
    "        return\n",
    "\n",
    "    # Parse the input CSV file to extract IDs, questions, and answers\n",
    "    qa_pairs = parse_csv_file(input_csv)\n",
    "    print(f\"Total QA Pairs Parsed: {len(qa_pairs)}\")\n",
    "\n",
    "    # Read the ground truth answers\n",
    "    ground_truth = read_ground_truth(ground_truth_file)\n",
    "    print(f\"Total Ground Truth Entries: {len(ground_truth)}\")\n",
    "\n",
    "    # Check if any QA pairs were found\n",
    "    if not qa_pairs:\n",
    "        print(\"No question-answer pairs were found in the input file.\")\n",
    "        return\n",
    "\n",
    "    # Generate the HTML content\n",
    "    html_content = create_simple_html(qa_pairs, ground_truth)\n",
    "\n",
    "    # Write the HTML content to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(html_content)\n",
    "\n",
    "    print(f\"HTML content has been successfully written to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"tasksource/spartqa-mchoice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total QA Pairs Parsed: 200\n",
      "No ground truth answer found for ID 489\n",
      "No ground truth answer found for ID 1113\n",
      "Total Ground Truth Entries: 1317\n",
      "\n",
      "===== Analysis Statistics =====\n",
      "\n",
      "Total Responses Analyzed: 200\n",
      "Responses with Final Answer in Curly Brackets: 136 (68.00%)\n",
      "Responses without Final Answer in Curly Brackets: 64 (32.00%)\n",
      "Responses with Ground Truth Available: 200 (100.00%)\n",
      "Correct Answers: 94\n",
      "Incorrect Answers: 106\n",
      "Accuracy: 47.00%\n",
      "Responses without Ground Truth: 0\n",
      "\n",
      "----- Tag Statistics -----\n",
      "Total Tags Found: 501\n",
      "Average Number of Tags per Response: 2.50\n",
      "Average Length of Tag Content: 8.68 characters\n",
      "--------------------------\n",
      "\n",
      "===== End of Statistics =====\n",
      "\n",
      "Statistics analysis completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import json  # For handling JSONL\n",
    "import os\n",
    "\n",
    "def extract_parts_regular_cot(answer_text):\n",
    "    # Attempt to extract Final Answer from 'Final Answer:'\n",
    "    final_match = re.search(r'Final Answer:\\s*(\\S+)', answer_text, re.IGNORECASE)\n",
    "    if final_match and final_match.group(1).strip():\n",
    "        final_answer = final_match.group(1).strip()\n",
    "        has_curly = False\n",
    "    else:\n",
    "        # Fallback: Extract Final Answer from '{...}' in the reasoning\n",
    "        curly_match = re.search(r'\\{([\\d.]+)\\}', answer_text)\n",
    "        final_answer = curly_match.group(1).strip() if curly_match else \"\"\n",
    "        has_curly = bool(curly_match)\n",
    "\n",
    "    return answer_text.strip(), final_answer, has_curly\n",
    "\n",
    "def parse_csv_file(file_path):\n",
    "    qa_pairs = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            question = row.get('question', 'No question found.').strip()\n",
    "            answer_text = row.get('answer', 'No answer found.').strip()\n",
    "            id_ = row.get('id')\n",
    "            if id_ is not None:\n",
    "                try:\n",
    "                    id_int = int(id_)\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping a row due to invalid 'id' (not an integer): {id_}\")\n",
    "                    continue\n",
    "                qa_pairs.append((id_int, question, answer_text))\n",
    "            else:\n",
    "                # Handle cases without 'id' by skipping\n",
    "                print(f\"Skipping a row due to missing 'id': {row}\")\n",
    "    return qa_pairs\n",
    "\n",
    "def read_ground_truth(jsonl_path):\n",
    "    ground_truth = {}\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            id_ = data.get('id')\n",
    "            answer = data.get('answer')\n",
    "            if id_ is not None and answer is not None:\n",
    "                # Extract the last number or text after '####'\n",
    "                match = re.search(r'####\\s*([\\d.]+)', answer)\n",
    "                if match:\n",
    "                    ground_truth[id_] = match.group(1).strip()\n",
    "                else:\n",
    "                    print(f\"No ground truth answer found for ID {id_}\")\n",
    "            else:\n",
    "                print(f\"Invalid ground truth entry: {data}\")\n",
    "    return ground_truth\n",
    "\n",
    "def create_statistics(qa_pairs, ground_truth):\n",
    "    total_responses = len(qa_pairs)\n",
    "    responses_with_curly = 0\n",
    "    responses_without_curly = 0\n",
    "    correct_answers = 0\n",
    "    incorrect_answers = 0\n",
    "    no_ground_truth = 0\n",
    "\n",
    "    # Variables for tag statistics\n",
    "    total_tags = 0\n",
    "    total_tag_length = 0\n",
    "    tag_counts = []  # List to store number of tags per response\n",
    "    tag_lengths = []  # List to store lengths of tag content across all responses\n",
    "\n",
    "    for id_, question, answer_text in qa_pairs:\n",
    "        try:\n",
    "            answer_reasoning, final_answer, has_curly = extract_parts_regular_cot(answer_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot extract parts for question ID {id_}: {e}\")\n",
    "            continue\n",
    "\n",
    "        if has_curly:\n",
    "            responses_with_curly += 1\n",
    "        else:\n",
    "            responses_without_curly += 1\n",
    "\n",
    "        # Extract tags and their content\n",
    "        tags_in_response = re.findall(r'<([A-Za-z]+\\d*)>(.*?)</\\1>', answer_text)\n",
    "        number_of_tags = len(tags_in_response)\n",
    "        tag_counts.append(number_of_tags)\n",
    "        total_tags += number_of_tags\n",
    "\n",
    "        for tag, content in tags_in_response:\n",
    "            content_length = len(content)\n",
    "            tag_lengths.append(content_length)\n",
    "            total_tag_length += content_length\n",
    "\n",
    "        # Retrieve ground truth answer\n",
    "        gt_answer = ground_truth.get(id_)\n",
    "        if gt_answer is None:\n",
    "            no_ground_truth += 1\n",
    "            continue\n",
    "\n",
    "        # Compare final_answer with ground truth\n",
    "        if final_answer == gt_answer:\n",
    "            correct_answers += 1\n",
    "        else:\n",
    "            incorrect_answers += 1\n",
    "\n",
    "    # Calculate additional metrics\n",
    "    accuracy_percentage = (correct_answers / (correct_answers + incorrect_answers) * 100) if (correct_answers + incorrect_answers) > 0 else 0\n",
    "    curly_percentage = (responses_with_curly / total_responses * 100) if total_responses > 0 else 0\n",
    "    no_curly_percentage = (responses_without_curly / total_responses * 100) if total_responses > 0 else 0\n",
    "    ground_truth_available = total_responses - no_ground_truth\n",
    "    ground_truth_available_percentage = (ground_truth_available / total_responses * 100) if total_responses > 0 else 0\n",
    "\n",
    "    # Calculate tag statistics\n",
    "    average_tags_per_response = (total_tags / total_responses) if total_responses > 0 else 0\n",
    "    average_tag_length = (total_tag_length / total_tags) if total_tags > 0 else 0\n",
    "\n",
    "    # Print the statistics\n",
    "    print(\"\\n===== Analysis Statistics =====\\n\")\n",
    "    print(f\"Total Responses Analyzed: {total_responses}\")\n",
    "    print(f\"Responses with Final Answer in Curly Brackets: {responses_with_curly} ({curly_percentage:.2f}%)\")\n",
    "    print(f\"Responses without Final Answer in Curly Brackets: {responses_without_curly} ({no_curly_percentage:.2f}%)\")\n",
    "    print(f\"Responses with Ground Truth Available: {ground_truth_available} ({ground_truth_available_percentage:.2f}%)\")\n",
    "    print(f\"Correct Answers: {correct_answers}\")\n",
    "    print(f\"Incorrect Answers: {incorrect_answers}\")\n",
    "    print(f\"Accuracy: {accuracy_percentage:.2f}%\")\n",
    "    print(f\"Responses without Ground Truth: {no_ground_truth}\")\n",
    "\n",
    "    # Tag Statistics\n",
    "    print(\"\\n----- Tag Statistics -----\")\n",
    "    print(f\"Total Tags Found: {total_tags}\")\n",
    "    print(f\"Average Number of Tags per Response: {average_tags_per_response:.2f}\")\n",
    "    print(f\"Average Length of Tag Content: {average_tag_length:.2f} characters\")\n",
    "    print(\"--------------------------\\n\")\n",
    "    print(\"===== End of Statistics =====\\n\")\n",
    "\n",
    "def main():\n",
    "    input_csv = '/Users/log/Github/textual_grounding/logan/results/GSM8K/llama/mermaid/mermaid_get_answer_llama3.1_20240926_215344.csv'  # Replace with your input CSV file path\n",
    "    ground_truth_file = '/Users/log/Github/textual_grounding/data/GSM8K/test.jsonl'  # Path to the ground truth JSONL file\n",
    "\n",
    "    # Check if input files exist\n",
    "    if not os.path.isfile(input_csv):\n",
    "        print(f\"Input CSV file not found: {input_csv}\")\n",
    "        return\n",
    "    if not os.path.isfile(ground_truth_file):\n",
    "        print(f\"Ground truth JSONL file not found: {ground_truth_file}\")\n",
    "        return\n",
    "\n",
    "    # Parse the input CSV file to extract IDs, questions, and answers\n",
    "    qa_pairs = parse_csv_file(input_csv)\n",
    "    print(f\"Total QA Pairs Parsed: {len(qa_pairs)}\")  # Debug: Print the number of QA pairs parsed\n",
    "\n",
    "    # Read the ground truth answers\n",
    "    ground_truth = read_ground_truth(ground_truth_file)\n",
    "    print(f\"Total Ground Truth Entries: {len(ground_truth)}\")  # Debug: Print the number of ground truth entries\n",
    "\n",
    "    # Check if any QA pairs were found\n",
    "    if not qa_pairs:\n",
    "        print(\"No question-answer pairs were found in the input file.\")\n",
    "        return\n",
    "\n",
    "    # Generate and print the statistics\n",
    "    create_statistics(qa_pairs, ground_truth)\n",
    "\n",
    "    print(\"Statistics analysis completed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 'prompt' is a list in entry ID 110. Attempting to join into a string.\n",
      "Warning: 'prompt' is a list in entry ID 111. Attempting to join into a string.\n",
      "Warning: 'prompt' is a list in entry ID 112. Attempting to join into a string.\n",
      "Warning: 'prompt' is a list in entry ID 113. Attempting to join into a string.\n",
      "JSON file has been updated successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Define the path to your JSON file\n",
    "input_file = '/Users/log/Github/textual_grounding/data/AIW/test.json'\n",
    "\n",
    "# Load the JSON data from the file\n",
    "with open(input_file, 'r', encoding='utf-8') as file:\n",
    "    try:\n",
    "        data = json.load(file)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "# Process each entry in the JSON data\n",
    "for idx, entry in enumerate(data):\n",
    "    # Get the current prompt\n",
    "    prompt = entry.get('prompt', '')\n",
    "\n",
    "    # Check if prompt is a string\n",
    "    if isinstance(prompt, str):\n",
    "        delimiter = 'have?'\n",
    "        index = prompt.find(delimiter)\n",
    "\n",
    "        if index != -1:\n",
    "            # Truncate the prompt after \"have?\"\n",
    "            truncated_prompt = prompt[:index + len(delimiter)]\n",
    "            entry['prompt'] = truncated_prompt\n",
    "        else:\n",
    "            print(f\"Warning: 'have?' not found in prompt of entry ID {entry.get('id', 'Unknown')}. Prompt left unchanged.\")\n",
    "    elif isinstance(prompt, list):\n",
    "        print(f\"Warning: 'prompt' is a list in entry ID {entry.get('id', 'Unknown')}. Attempting to join into a string.\")\n",
    "        # Attempt to join the list into a single string\n",
    "        joined_prompt = ' '.join(str(item) for item in prompt)\n",
    "        delimiter = 'have?'\n",
    "        index = joined_prompt.find(delimiter)\n",
    "\n",
    "        if index != -1:\n",
    "            truncated_prompt = joined_prompt[:index + len(delimiter)]\n",
    "            entry['prompt'] = truncated_prompt\n",
    "        else:\n",
    "            print(f\"Warning: 'have?' not found after joining prompt in entry ID {entry.get('id', 'Unknown')}. Prompt left unchanged.\")\n",
    "    else:\n",
    "        print(f\"Warning: 'prompt' is neither a string nor a list in entry ID {entry.get('id', 'Unknown')}. Prompt left unchanged.\")\n",
    "\n",
    "    # Rename 'right_answer' to 'answer' if it exists\n",
    "    if 'right_answer' in entry:\n",
    "        entry['answer'] = entry.pop('right_answer')\n",
    "\n",
    "# Save the updated data back to the same JSON file\n",
    "with open(input_file, 'w', encoding='utf-8') as file:\n",
    "    json.dump(data, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"JSON file has been updated successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
