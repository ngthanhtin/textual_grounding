{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import anthropic\n",
    "# import ollama\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from google.generativeai.types import RequestOptions\n",
    "from google.api_core import retry\n",
    "from typing import List, Tuple\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import datetime\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mega_prompt = \"\"\"\n",
    "You are a helpful assistant tasked with tagging key facts in the text and then using those facts to answer the final question. Your goal is to analyze the input question, identify distinct key points needed to answer the question, and then wrap each of these points in custom HTML-like tags like <fact1>, <fact2>, <fact3>, etc and then use these tags to answer the question. After you have tagged the question in tags, use these tags in your reasoning process to answer the question. The <fact> tags should be interweaved in the sentences in your reasoning.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "1. **Read and Understand the Input Question**:\n",
    "   - Carefully analyze the input question to understand its context.\n",
    "   - Identify distinct facts, entities, or concepts that contribute to the meaning of the question.\n",
    "\n",
    "2. **Identify Key Points (Facts)**:\n",
    "   - Each distinct and meaningful segment of the question that provides important information should be considered a \"fact.\"\n",
    "   - This can include the subject, object, context, or any qualifiers that make the question specific.\n",
    "\n",
    "3. **Tag Each Fact**:\n",
    "   - Assign a unique tag to each fact in the form <fact1>, <fact2>, etc.\n",
    "   - Wrap each identified key point in these tags.\n",
    "   - The tags should start from <fact1> for the first key point, and increment for each new fact identified.\n",
    "\n",
    "4. **Formatting Requirements**:\n",
    "   - Maintain the original structure of the question as much as possible.\n",
    "   - Make sure each tag encapsulates the entire key point clearly without splitting phrases unnecessarily.\n",
    "\n",
    "### Example\n",
    "\n",
    "#### Input Question:\n",
    "\"Does climate change positively affect polar bear populations in the Arctic? Anwser Options: Yes, No, Depends\"\n",
    "\n",
    "#### Step-by-Step Identification:\n",
    "1. Fact 1: \"Climate change\" (main topic causing the effect)\n",
    "2. Fact 2: \"Polar bear populations\" (who is affected)\n",
    "3. Fact 3: \"In the Arctic\" (the location/context)\n",
    "\n",
    "#### Reformatted Output:\n",
    "\"How does <fact1>climate change</fact1> affect <fact2>polar bear populations</fact2> in <fact3>the Arctic</fact3>?\"\n",
    "\n",
    "#### Answer Reasoning\n",
    "\"<fact1>Climate change</fact1> primarily affects <fact2>polar bears</fact2> through the loss of <fact3>sea ice</fact3>, which is crucial for their hunting and survival. As <fact3>Arctic ice</fact3> melts earlier and forms later due to <fact1>rising temperatures</fact1>, <fact2>polar bears</fact2> face reduced access to their main prey (seals), leading to nutritional stress, longer fasting periods, and <fact2>overall population decline</fact2>. The impact of <fact1>climate change</fact1> on <fact2>polar bear populations</fact2> is therefore negative, so the answer is {No}\"\n",
    "\n",
    "### Final Output Format\n",
    "\n",
    "Ensure that the final output is:\n",
    "- Grammatically correct.\n",
    "- Properly formatted with each key point enclosed in <fact> tags.\n",
    "- Consistent with the original meaning of the input question.\n",
    "- Final answer is enclosed in curly braces e.g. {answer}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_zero_prompt = \"\"\"\n",
    "# General Instructions\n",
    "You are a helpful assistant tasked with tagging key facts in the text and then using those facts to answer the final question. Your first goal is to analyze the input question and identify distinct key points needed to answer the question and then wrap each of these points in custom HTML-like tags like <fact1>, <fact2>, <fact3>. After you have tagged the reformatted question in tags, use these tags in your reasoning process to answer the question. The <tags> should be interweaved in the sentences in your reasoning. You are only concerned with highlighting facts that are essential to answering the question. You should not tag irrelevant information. Once you have determined the answer to the question, you should put the concise version of your final answer in curly braces. For example, {3} or {True} or {A}.\n",
    "\n",
    "Your response should follow this format:\n",
    "\n",
    "Reformatted Question:\n",
    ".... (reformatted question with tags)\n",
    "\n",
    "Answer Reasoning:\n",
    ".... (reasoning with tags)\n",
    "\n",
    "Final Answer: {answer}\n",
    "\n",
    "# Tagging Information\n",
    "Each tag should be named according to the type of information it represents. There are multiple distinct types of tags:\n",
    "\n",
    "- <person> for people or characters\n",
    "- <location> for places or regions\n",
    "- <number> for numerical values\n",
    "- <time> for time-related information\n",
    "- <concept> for abstract ideas\n",
    "- <object> for physical things\n",
    "\n",
    "\n",
    "## Tagging Examples \n",
    "\n",
    "### Person Example\n",
    "For the text \"Adam worked as a volunteer firefighter in order to help serve his community.\", you would tag the text as follows: \"<person>Adam worked as a volunteer firefighter</person> in order to help serve his community.\"\n",
    "\n",
    "The text in the <person> tag should include key information relevant to that person. While acceptable, it does not always need to be just their name. \n",
    "\n",
    "### Location Example\n",
    "For the text \"What is the capital of France?\", you would tag the question as follows: \"What is the <location>capital of France</location>?\"\n",
    "\n",
    "### Number Example\n",
    "For the text \"The average temperature in the desert is 110 degrees Fahrenheit.\", you would tag the text as follows: \"The average temperature in the desert is <number>110 degrees Fahrenheit</number>.\"\n",
    "\n",
    "The text in the <number> tag should include any information relevant to a quantity of something. \n",
    "\n",
    "### Time Example\n",
    "For the text \"The earliest recorded human writing was composed nearly 4,000 years ago, sometime around 2000 B.C.\" would be tagged as \"The earliest recorded human writing was composed nearly <time1>4,000 years</time1> ago, sometime around <time2>2000 B.C.</time2>\"\n",
    "\n",
    "### Concept Example\n",
    "For the text \"The field of machine learning is a rapidly growing area of study.\", you would tag the text as follows: \"The field of <concept>machine learning</concept> is a rapidly growing area of study.\"\n",
    "\n",
    "### Object Example\n",
    "For the text \"The astronaut used a special tool to repair the damaged satellite.\", you would tag the text as follows: \"The astronaut used a <object1>special tool</object2> to repair the <object2>damaged satellite</object2>.\"\n",
    "\n",
    "## What NOT to Tag\n",
    "Given an inputted question, there are large amounts of possible tags that could be used. However, not all of these tags are relevant to the final question. You should only tag information that is ESSENTIAL to answering the question. Here is an example of a question that has been over-tagged:\n",
    "\n",
    "Original Question:\n",
    "Kaden is a computer science major at NYU where he studies subjects such as physics, math, and the principles of programming languages. He has a pet dog named Max who is 3 years old. Kaden has a part-time job at the local grocery store where he works every Saturday and Sunday. What major ocean does Kaden live near?\n",
    "\n",
    "Reformatted Question:\n",
    "\"<person1>Kaden</person1> is a <concept1>computer science major</concept1> at <location1>NYU</location1> where he studies subjects such as <concept2>physics</concept2>, <concept3>math</concept3>, and the <concept4>principles of programming languages</concept4>. <person2>He has a pet dog</person2> named <person3>Max</person3> who is <time1>3 years old</time1>. <person4>Kaden has a part-time job</person4> at the <location2>local grocery store</location2> where he works every <time2>Saturday</time2> and <time3>Sunday</time3>. What <object1>major ocean</object1> does <person1>Kaden</person1> live near?\"\n",
    "\n",
    "While this question does have valid tags, almost all of them are not relevant to the question. Here is an exmaple of the same question with only the relevant tags:\n",
    "\n",
    "Reformatted Question:\n",
    "\"<person1>Kaden</person1> is a computer science major at <location1>NYU</location1> where he studies subjects such as physics, math, and the principles of programming languages. He has a pet dog named Max who is 3 years old. Kaden has a part-time job at the local grocery store where he works every Saturday and Sunday. What <object1>major ocean</object1> does <person1>Kaden</person1> live near?\"\n",
    "\n",
    "Here is an example answer for the properly formatted question:\n",
    "\"Since <person1>Kaden</person1> attends <location1>NYU</location1>, he lives in New York City. Given that New York City is on the east coast of the United States, the nearest major ocean that <person1>Kaden</person1> lives near is the Atlantic Ocean. \n",
    "Final answer: {Atlantic Ocean}\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_facts = \"\"\"\n",
    "# General Instructions\n",
    "You are a helpful assistant tasked with extracting key facts from the text. Your goal is to analyze the input question and identify distinct sections of the text needed to answer the question. You are only concerned with highlighting facts that are ESSENTIAL to answering the question. You should ignore irrelevant information that does not help answer the final question. \n",
    "\n",
    "Reasoning Process:\n",
    "....\n",
    "\n",
    "Key Facts:\n",
    "....\n",
    "\n",
    "# Fact Extraction Details\n",
    "\n",
    "## Properly Done Example\n",
    "Here is an example of properly extracting the information needed to answer the final question:\n",
    "\n",
    "Original Question:\n",
    "Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?\n",
    "\n",
    "Reasoning Process:\n",
    "Identifying the important information in this problem centers on determining what leads to finding the difference between the wallet's cost and Betty's total available money.\n",
    "The wallet's cost of $100 serves as the target amount, establishing the baseline for all calculations. Understanding Betty's available money requires summing all her financial sources. Her initial savings, described as half the needed amount, translates to $50. The parents' contribution of $15 directly adds to this sum. The grandparents' contribution, defined as twice the parents' amount, equals $30 and represents another essential component.\n",
    "Each numerical value and relationship presented in the problem contributes directly to calculating Betty's total available funds. The absence of any single piece - the wallet cost, initial savings, parental contribution, or the relationship between parental and grandparental gifts - would make it impossible to determine the remaining amount needed.\n",
    "\n",
    "Key Facts:\n",
    "- The wallet costs $100\n",
    "- Betty has half of the money needed\n",
    "- Bett's parents gave her $15\n",
    "- Betty's Grandparents gave twice as much as parents\n",
    "\n",
    "## What NOT to Extract\n",
    "Given an inputted question, there are large amounts of possible facts that could be used. However, not all of these citations are relevant to the final question. You should only extract information that is ESSENTIAL to answering the question. Here is an example of a question that has too many extracted facts:\n",
    "\n",
    "Original Question:\n",
    "Kaden is a computer science major at NYU where he studies subjects such as physics, math, and the principles of programming languages. He has a pet dog named Max who is 3 years old. Kaden has a part-time job at the local grocery store where he works every Saturday and Sunday. What major ocean does Kaden live near?\n",
    "\n",
    "Reasoning Process:\n",
    "....\n",
    "\n",
    "Key Facts:\n",
    "- Kaden is a computer science major\n",
    "- Kaden attends NYU\n",
    "- Kaden studies physics, math and programming languages\n",
    "- Kaden has a dog\n",
    "- Max, the dog, is 3 years old\n",
    "- Kaden has a part time job\n",
    "- Kaden works at a grocery store\n",
    "- Kaden works every Saturday and Sunday\n",
    "\n",
    "While these are all valid pieces of information, almost all of them are not relevant to the question. Here is what the key facts section should actually look like:\n",
    "\n",
    "Key Facts:\n",
    "- Kaden attends NYU\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_facts_quote = \"\"\"\n",
    "# General Instructions\n",
    "You are a helpful assistant tasked with extracting key facts from the text. Your goal is to analyze the input question and identify distinct sections of the text needed to answer the question. You are only concerned with identifying quotes that are ESSENTIAL to answering the question. You should ignore irrelevant information that does not help answer the final question. You should extract the exact parts of the text, not summarized versions of the text. After you have decided what specific quotes to use, tag the original question with xml tags around those quotes. Here is what your response should look like:\n",
    "\n",
    "```\n",
    "### Fact Extraction Reasoning:\n",
    "....\n",
    "\n",
    "### Key Facts:\n",
    "....\n",
    "\n",
    "### Reformatted Question:\n",
    "...\n",
    "```\n",
    "\n",
    "# Fact Extraction Details\n",
    "Each block of text has lots of information that could be a valid fact. However, you should only extract the most important quotes. Try to keep each quote as short as possible while still maintaining the essential information.\n",
    "\n",
    "## Properly Done Example\n",
    "Here is an example of properly extracting the information needed to answer the final question:\n",
    "\n",
    "```\n",
    "### Original Question:\n",
    "Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?\n",
    "\n",
    "### Fact Extraction Reasoning:\n",
    "Identifying the important information in this problem centers on determining what leads to finding the difference between the wallet's cost and Betty's total available money.\n",
    "The wallet's cost of $100 serves as the target amount, establishing the baseline for all calculations. Understanding Betty's available money requires summing all her financial sources. Her initial savings, described as half the needed amount, translates to $50. The parents' contribution of $15 directly adds to this sum. The grandparents' contribution, defined as twice the parents' amount, equals $30 and represents another essential component.\n",
    "Each numerical value and relationship presented in the problem contributes directly to calculating Betty's total available funds. The absence of any single piece - the wallet cost, initial savings, parental contribution, or the relationship between parental and grandparental gifts - would make it impossible to determine the remaining amount needed.\n",
    "\n",
    "### Key Facts:\n",
    "<fact1>wallet which costs $100</fact1>\n",
    "<fact2>Betty has only half of the money</fact2>\n",
    "<fact3>parents decided to give her $15</fact3>\n",
    "<fact4>grandparents twice as much as her parents</fact4>\n",
    "\n",
    "### Reformatted Question:\n",
    "Betty is saving money for a new <fact1>wallet which costs $100</fact1>. <fact2>Betty has only half of the money</fact2> she needs. Her <fact3>parents decided to give her $15</fact3> for that purpose, and her <fact4>grandparents twice as much as her parents</fact4>. How much more money does Betty need to buy the wallet?\n",
    "```\n",
    "\n",
    "## What NOT to Extract\n",
    "Given an inputted question, there are large amounts of possible facts that could be used. However, not all of these citations are relevant to the final question. You should only extract information that is essential to answering the question. Here is an example of a question that has too many extracted facts:\n",
    "\n",
    "```\n",
    "### Original Question:\n",
    "Kaden is a computer science major at NYU where he studies subjects such as physics, math, and the principles of programming languages. He has a pet dog named Max who is 3 years old. Kaden has a part-time job at the local grocery store where he works every Saturday and Sunday. What major ocean does Kaden live near?\n",
    "\n",
    "### Fact Extraction Reasoning:\n",
    "....\n",
    "\n",
    "### Key Facts:\n",
    "<fact1>Kaden is a computer science major</fact1>\n",
    "<fact2>NYU</fact2>\n",
    "<fact3>he studies subjects such as physics, math, and the principles of programming languages</fact3>\n",
    "<fact4>pet dog named Max</fact4>\n",
    "<fact5>Max, the dog, is 3 years old</fact5>\n",
    "<fact6>Kaden has a part-time job</fact6>\n",
    "<fact7>local grocery store</fact7>\n",
    "<fact8>he works every Saturday and Sunday</fact8>\n",
    "\n",
    "### Reformatted Question:\n",
    "...\n",
    "```\n",
    "\n",
    "While these are all valid pieces of information, almost none of them are relevant to the question. We only want information that is relevant to the final question, which in this case is \"What major ocean does Kaden live near?\" This is what the key facts section and the associated reformatted question should actually look like:\n",
    "\n",
    "```\n",
    "### Key Facts:\n",
    "<fact1>Kaden is a computer science major at NYU</fact1>\n",
    "\n",
    "### Reformatted Question \n",
    "<fact1>Kaden is a computer science major at NYU</fact1> where he studies subjects such as physics, math, and the principles of programming languages. He has a pet dog named Max who is 3 years old. Kaden has a part-time job at the local grocery store where he works every Saturday and Sunday. What major ocean does Kaden live near?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_turn_facts = \"\"\"\n",
    "Now that you have put the key facts in tags, use these tags in your reasoning process to answer the question. The <fact...> tags should be interweaved in the sentences in your reasoning. Put your final answer in curly braces. For example, {3} or {True} or {A}.\n",
    "\n",
    "Here is an example of how you should answer the question based off your previous work:\n",
    "\n",
    "```\n",
    "### Reformatted Question:\n",
    "\"How does <fact1>climate change</fact1> affect <fact2>polar bear populations</fact2> in <fact3>the Arctic</fact3>?\"\n",
    "\n",
    "#### Final Answer Reasoning\n",
    "\"<fact1>Climate change</fact1> primarily affects <fact2>polar bears</fact2> through the loss of <fact3>sea ice</fact3>, which is crucial for their hunting and survival. As <fact3>Arctic ice</fact3> melts earlier and forms later due to <fact1>rising temperatures</fact1>, <fact2>polar bears</fact2> face reduced access to their main prey (seals), leading to nutritional stress, longer fasting periods, and <fact2>overall population decline</fact2>. The impact of <fact1>climate change</fact1> on <fact2>polar bear populations</fact2> is therefore negative, so the answer is {No}\"\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_4o_multiturn(prompt: str) -> str:\n",
    "    client = OpenAI()\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # First turn\n",
    "        completion1 = client.chat.completions.create(\n",
    "            model=\"gpt-4o-2024-08-06\",\n",
    "            messages=messages,\n",
    "            temperature=0\n",
    "        )\n",
    "        response1 = completion1.choices[0].message.content.strip()\n",
    "        \n",
    "        # Append assistant's first response\n",
    "        messages.append({\"role\": \"assistant\", \"content\": response1})\n",
    "        \n",
    "        # Second turn: follow-up prompt\n",
    "        follow_up_prompt = second_turn_facts\n",
    "        messages.append({\"role\": \"user\", \"content\": follow_up_prompt})\n",
    "        \n",
    "        completion2 = client.chat.completions.create(\n",
    "            model=\"gpt-4o-2024-08-06\",\n",
    "            messages=messages,\n",
    "            temperature=0\n",
    "        )\n",
    "        response2 = completion2.choices[0].message.content.strip()\n",
    "        \n",
    "        # Combine both responses\n",
    "        combined_response = f\"{response1}\\n{response2}\"\n",
    "        print(f\"Combined response: {combined_response}\")\n",
    "        return combined_response\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in multiturn query_4o: {str(e)}\")\n",
    "        return \"\"\n",
    "    \n",
    "# def query_gemini(prompt: str, problem_id) -> str:\n",
    "#     \"\"\"\n",
    "#     Queries the Gemini LLM with the given prompt and returns the response text.\n",
    "#     \"\"\"\n",
    "#     genai.configure(api_key=get_gemini_key(problem_id))\n",
    "#     model = genai.GenerativeModel('gemini-1.5-pro-latest')\n",
    "#     response = model.generate_content(prompt, request_options=RequestOptions(retry=retry.Retry(initial=20, multiplier=3, maximum=121, timeout=60)))\n",
    "#     text = response.candidates[0].content.parts[0].text\n",
    "#     return text\n",
    "\n",
    "def query_claude(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Queries the Claude LLM with the given prompt and returns the response text.\n",
    "    \"\"\"\n",
    "    client = anthropic.Anthropic(api_key=API_KEYS['claude'])\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20240620\",\n",
    "        max_tokens=1024,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.content[0].text\n",
    "\n",
    "def query_4o(prompt: str) -> str:\n",
    "    client = OpenAI()\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-2024-08-06\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{prompt}\"\n",
    "            }\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(save_path: str, ids: List[str], questions: List[str], answers: List[str], append: bool = False):\n",
    "    \"\"\"\n",
    "    Saves the results to a CSV file. If append is True and the file exists, it appends without headers.\n",
    "    Otherwise, it writes a new file with headers.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({'id': ids, 'question': questions, 'answer': answers})\n",
    "    if append and os.path.exists(save_path):\n",
    "        df.to_csv(save_path, mode='a', index=False, header=False)\n",
    "    else:\n",
    "        df.to_csv(save_path, index=False)\n",
    "\n",
    "def read_jsonl_file(filepath: str) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Reads a JSONL file and returns a list of JSON objects.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            json_obj = json.loads(line)\n",
    "            data.append(json_obj)\n",
    "    return data\n",
    "\n",
    "def get_prompt(prompt_type: str, few_shot_prompt: str, question: str) -> str:\n",
    "    \"\"\"\n",
    "    Constructs the prompt based on the prompt type.\n",
    "    \"\"\"\n",
    "    prompts = {\n",
    "        \"cot\": f\"{few_shot_prompt}\\n{question}\\nPlease generate your explanation first, then generate the answer in the bracket as follow:\\n\" +\"Answer: {}\",\n",
    "        \"log_cot_mcq\": f\"{few_shot_prompt}\\n{question}\\nThink through your answer step by step and then choose the answer option that is the most correct. Then put your final answer in curly brackets. For example, Final_Answer:{{3}}\",\n",
    "        \"log_cot\": f\"{question}\\nThink through your answer step by step. Then put your final answer in curly brackets. For example, Final answer:{{3}}\",\n",
    "        \"fs\": f\"{few_shot_prompt}\\n{question}\",\n",
    "        \"fs_inst\": f\"{few_shot_prompt}\\n{question}\\nI want you to answer this question but your explanation should contain references referring back to the information in the question. To do that, first, re-generate the question with proper tags and then generate your answers. The output format is as follow:\\n\\\n",
    "            Reformatted Question: \\\n",
    "                Answer:\",\n",
    "        \"zs\": f\"{question}\\nI want you to answer this question but your explanation should contain references referring back to the information in the question. To do that, first, re-generate the question with proper tags (<a>, <b>, <c>, etc) for refered information and then generate your answers that also have the tag (<a>, <b>, <c>, etc) for the grounded information. Give your answer by analyzing step by step, and give only numbers in the final answer. The output format is as follow:\\n\\\n",
    "            Reformatted Question: \\\n",
    "                Answer:\\\n",
    "                    Final answer:\",\n",
    "        \"fs_xml\": f\"{few_shot_prompt}\\n\\nRecreate the following question in the style of the correctly formatted examples shown previously. Make sure that your response has all its information inclosed in the proper <tags>. Begin your response with the <key_facts> section. Make sure that every fact in <key_facts> is very concise and contains a very short reference to the <question>. Do not include a <question> section in your response\\n\\n<question>\\n{question}\\n</question>\",\n",
    "        \"fs_log_inst\": f\"{few_shot_prompt}\\n\\n{question}\\nTo answer this question, your explanation should contain references referring back to the information in the question. To do that, first, re-generate the question with proper tags and then generate your answers based off the tags. Put your final answer in curly brackets e.g. Final_Answer: {{false}}. Your final answer should only be \\\"true\\\" or \\\"false\\\".\",\n",
    "        \"fs_clause_inst\": f\"{few_shot_prompt}\\n\\n{question}\\nTo answer this question, first regenerate the question with <fact> tags around each clause or phrase in the text. Each clause or phrase should be as concise as possible so that long sentences will be broken up into multiple segments. Then, to answer the original question, your explanation should contain references back to the information in the tagged question. After you have generated the reformatted question and your reasoning which contains references to the tagged reformatted question, put your answer in curly brackets e.g. Final_Answer: {{false}}. Your final answer should either be \\\"true\\\" or \\\"false\\\".\",\n",
    "        \"stripped_clause\": f\"{few_shot_prompt}\\n\\n{question}\\nTo answer this question, your explanation should contain references referring back to the information in the question. Generate your answers based off the tags in the question. Use the example answers as a guide for what your answer format should look like. The <fact> tags should be interweaved in the sentences in your reasoning. Put your final answer in curly brackets e.g. Final_Answer: {{false}}. Your final answer should either be \\\"true\\\" or \\\"false\\\".\",\n",
    "        \"mermaid_get_answer\": f\"{few_shot_prompt}\\n\\n Your job is to extract the key facts from a question relevant to answering the question. The facts should be represented in a hierarchal format through a mermaid diagram. Do not create duplicate facts across multiple branches that represent the same information. Create a mermaid diagram that represents the key facts in the following question. Then, use the nodes from this graph to cite specific facts in your answer reasoning. Put your final answer in curly brackets e.g. Final_Answer: {{30}} \\n\\nquestion: {question}\", \n",
    "        # \"mega_prompt\": f\"{mega_prompt}\\nYour final answer to this question should ONLY be {{true}} or {{false}} \\n\\n{question}\",\n",
    "        # \"semantic_zero_prompt\": f\"{semantic_zero_prompt}\\n\\n{question}\\n\\n ONLY include the number in your final answer. For example, {3}\",\n",
    "        \"extract_facts\": f\"{extract_facts_quote}\\n\\n{question}\",\n",
    "    }\n",
    "    return prompts.get(prompt_type, \"\")\n",
    "\n",
    "\n",
    "def query_llm(llm_model: str, ids: List[str], questions: List[str], few_shot_prompt: str, prompt_type: str, save_path: str, already_answered_ids: set) -> Tuple[List[str], List[str], List[str]]:\n",
    "    answers = []\n",
    "    ids_can_be_answered = []\n",
    "    questions_can_be_answered = []\n",
    "    \n",
    "    for id, q in tqdm(zip(ids, questions), total=len(ids)):\n",
    "        # print(f\"Processing ID: {id}\")\n",
    "        if id in already_answered_ids:\n",
    "            print(f\"Skipping already answered ID: {id}\")\n",
    "            continue\n",
    "        \n",
    "        prompt = get_prompt(prompt_type, few_shot_prompt, q)\n",
    "        try:\n",
    "            if llm_model == 'gemini':\n",
    "                answer = query_gemini(prompt, id)\n",
    "            elif llm_model == 'claude':\n",
    "                answer = query_claude(prompt)\n",
    "            elif llm_model == '4o':\n",
    "                # answer = query_4o(prompt)\n",
    "                answer = query_4o_multiturn(prompt)\n",
    "            elif llm_model == 'llama3.1':\n",
    "                answer = ollama.generate(model='llama3.1', prompt=prompt)['response']\n",
    "                print(f\"Processed ID: {id}\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported LLM model: {llm_model}\")\n",
    "            # print(f\"Answer for ID {id}: {answer}\")\n",
    "            \n",
    "            answers.append(answer)\n",
    "            questions_can_be_answered.append(q)\n",
    "            ids_can_be_answered.append(id)\n",
    "\n",
    "            # Save after each answer\n",
    "            save_results(save_path, [id], [q], [answer], append=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question {id}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return ids_can_be_answered, questions_can_be_answered, answers\n",
    "\n",
    "def load_data_size_specific(data_path: str, sample_size: int = 0, random_seed: int = 0):\n",
    "    random.seed(random_seed)\n",
    "\n",
    "    data = read_jsonl_file(data_path)\n",
    "    question_length = 0 # 336  # 526 # 800\n",
    "    eligible_data = [x for x in data if len(x[\"question\"]) >= question_length]\n",
    "    \n",
    "    if sample_size > 0 and sample_size < len(eligible_data):\n",
    "        sampled_data = random.sample(eligible_data, sample_size)\n",
    "    else:\n",
    "        sampled_data = eligible_data\n",
    "    \n",
    "    ids = [x[\"id\"] for x in sampled_data]\n",
    "    questions = [x[\"question\"] for x in sampled_data]\n",
    "    \n",
    "    return ids, questions\n",
    "\n",
    "def load_few_shot_prompt(prompt_path: str) -> str:\n",
    "    with open(prompt_path, 'r') as file:\n",
    "        prompt = file.read()\n",
    "    # print(f\"Loaded few-shot prompt from: {prompt_path}\")\n",
    "    return prompt\n",
    "\n",
    "def load_already_answered_ids(save_path: str) -> set:\n",
    "    if os.path.exists(save_path):\n",
    "        df = pd.read_csv(save_path)\n",
    "        answered_ids = set(df['id'].astype(int).tolist())\n",
    "        # print(f\"Loaded {len(answered_ids)} already answered IDs from: {save_path}\")\n",
    "        print(f\"Already answered IDs: {answered_ids}\")\n",
    "        return answered_ids\n",
    "    else:\n",
    "        print(f\"No existing save file found at: {save_path}. Starting fresh.\")\n",
    "        return set()\n",
    "\n",
    "def initialize_save_file(save_path: str):\n",
    "    if not os.path.exists(save_path):\n",
    "        # Create an empty DataFrame with headers and save\n",
    "        df = pd.DataFrame(columns=['id', 'question', 'answer'])\n",
    "        df.to_csv(save_path, index=False)\n",
    "        print(f\"Initialized new save file with headers at: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized new save file with headers at: /Users/log/Github/textual_grounding/logan/results/p_GSM8k/4o/grounded_fact/extract_facts_3shot_cot.txt_4o_1022_233035.csv\n",
      "Already answered IDs: set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:10<00:10, 10.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined response: ```\n",
      "### Fact Extraction Reasoning:\n",
      "To determine the minimum temperature of the second bottle, we need to focus on the conditions that allow Donny to drink the water. The key information includes the temperature requirement for Donny to drink water, the temperature of the first mug, and the fact that after mixing water from both mugs, the water is drinkable. These details are essential to calculate the minimum temperature of the second bottle.\n",
      "\n",
      "### Key Facts:\n",
      "<fact1>Donny can only drink water if it's at least 40 degrees</fact1>\n",
      "<fact2>One mug is 33 degrees</fact2>\n",
      "<fact3>he pours 4 ounces of water from the 33-degree mug</fact3>\n",
      "<fact4>one ounce from the other bottle</fact4>\n",
      "<fact5>he is now able to drink the water</fact5>\n",
      "\n",
      "### Reformatted Question:\n",
      "Donny can only drink water if it's <fact1>at least 40 degrees</fact1>. He has two mugs of water. <fact2>One mug is 33 degrees</fact2>. The other is an unknown temperature. If <fact3>he pours 4 ounces of water from the 33-degree mug</fact3> into his water bottle and <fact4>one ounce from the other bottle</fact4>, <fact5>he is now able to drink the water</fact5>. At least how many degrees is the second bottle?\n",
      "```\n",
      "```\n",
      "### Reformatted Question:\n",
      "Donny can only drink water if it's <fact1>at least 40 degrees</fact1>. He has two mugs of water. <fact2>One mug is 33 degrees</fact2>. The other is an unknown temperature. If <fact3>he pours 4 ounces of water from the 33-degree mug</fact3> into his water bottle and <fact4>one ounce from the other bottle</fact4>, <fact5>he is now able to drink the water</fact5>. At least how many degrees is the second bottle?\n",
      "\n",
      "#### Final Answer Reasoning\n",
      "To find the minimum temperature of the second bottle, we need to ensure that the mixed water reaches <fact1>at least 40 degrees</fact1>. Donny mixes <fact3>4 ounces of water from the 33-degree mug</fact3> with <fact4>one ounce from the other bottle</fact4>. The total volume of the mixed water is 5 ounces. The condition that <fact5>he is now able to drink the water</fact5> implies that the average temperature of the mixture must be at least 40 degrees.\n",
      "\n",
      "The equation for the average temperature of the mixture is:\n",
      "\\[\n",
      "\\frac{(4 \\text{ ounces} \\times 33 \\text{ degrees}) + (1 \\text{ ounce} \\times T)}{5 \\text{ ounces}} \\geq 40 \\text{ degrees}\n",
      "\\]\n",
      "\n",
      "Solving for \\(T\\), the temperature of the second bottle:\n",
      "\\[\n",
      "\\frac{132 + T}{5} \\geq 40\n",
      "\\]\n",
      "\\[\n",
      "132 + T \\geq 200\n",
      "\\]\n",
      "\\[\n",
      "T \\geq 68\n",
      "\\]\n",
      "\n",
      "Therefore, the minimum temperature of the second bottle must be {68} degrees.\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:20<00:00, 10.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined response: ```\n",
      "### Fact Extraction Reasoning:\n",
      "To determine how much John pays a year for driving, we need to focus on the distance he travels, the cost per mile, and any additional charges. The key elements include the distance to work, the frequency of his trips, the cost per mile, and the monthly bonus he gives to his driver. These components will allow us to calculate the total annual cost.\n",
      "\n",
      "### Key Facts:\n",
      "<fact1>His work is 30 miles away</fact1>\n",
      "<fact2>he has to go there and back each day</fact2>\n",
      "<fact3>He goes to work 5 days a week</fact3>\n",
      "<fact4>for 50 weeks a year</fact4>\n",
      "<fact5>charged $2 per mile driven</fact5>\n",
      "<fact6>$150 bonus per month</fact6>\n",
      "\n",
      "### Reformatted Question:\n",
      "John hires a driving service to get him to work each day. <fact1>His work is 30 miles away</fact1> and <fact2>he has to go there and back each day</fact2>. <fact3>He goes to work 5 days a week</fact3> <fact4>for 50 weeks a year</fact4>. He gets <fact5>charged $2 per mile driven</fact5> and he also gives his driver a <fact6>$150 bonus per month</fact6>. How much does he pay a year for driving?\n",
      "```\n",
      "```\n",
      "### Reformatted Question:\n",
      "John hires a driving service to get him to work each day. <fact1>His work is 30 miles away</fact1> and <fact2>he has to go there and back each day</fact2>. <fact3>He goes to work 5 days a week</fact3> <fact4>for 50 weeks a year</fact4>. He gets <fact5>charged $2 per mile driven</fact5> and he also gives his driver a <fact6>$150 bonus per month</fact6>. How much does he pay a year for driving?\n",
      "\n",
      "#### Final Answer Reasoning\n",
      "John's daily round trip is <fact1>30 miles away</fact1> each way, so he travels 60 miles per day. Since <fact2>he has to go there and back each day</fact2>, he travels 60 miles per day. He works <fact3>5 days a week</fact3>, which means he travels 300 miles per week. Over the course of <fact4>50 weeks a year</fact4>, he travels 15,000 miles annually. At a rate of <fact5>$2 per mile driven</fact5>, his annual mileage cost is 15,000 miles * $2/mile = $30,000. Additionally, he gives his driver a <fact6>$150 bonus per month</fact6>, which totals $1,800 annually (12 months * $150/month). Therefore, the total annual cost is $30,000 + $1,800 = {31,800}.\n",
      "```\n",
      "Processing complete. 2 new answers saved to /Users/log/Github/textual_grounding/logan/results/p_GSM8k/4o/grounded_fact/extract_facts_3shot_cot.txt_4o_1022_233035.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "time = datetime.datetime.now().strftime(\"%m%d_%H%M%S\")\n",
    "# time = '1022_002130'\n",
    "# /Users/log/Github/textual_grounding/logan/results/GSM8K/4o/grounded_fact/semantic_zero_prompt_clause_fs.txt_4o_1020_225628.csv\n",
    "project_root = '/Users/log/Github/textual_grounding/'\n",
    "dataset = 'p_GSM8k'\n",
    "\n",
    "llm_model = '4o'\n",
    "prompt_type = 'extract_facts'\n",
    "# prompt_type = 'semantic_zero_prompt'\n",
    "# prompt_type = 'fs_clause_inst'\n",
    "# few_shot_txt = 'clause_fs.txt'\n",
    "few_shot_txt = '3shot_cot.txt'\n",
    "\n",
    "# Paths\n",
    "data_path = os.path.join(project_root, 'data', dataset, 'test.jsonl')\n",
    "# data_path = os.path.join(project_root, 'data', dataset, 'test.json')\n",
    "# data_path = '/Users/log/Github/textual_grounding/logan/results/GSM8K/llama/mermaid/mermaid_get_graph_llama3.1_20240924_001821.csv'\n",
    "\n",
    "fewshot_prompt_path = os.path.join(project_root, \"prompt\", dataset, few_shot_txt)\n",
    "# fewshot_prompt_path = '/Users/log/Github/textual_grounding/prompt/GSM8K/fewshot_mermaid_full.txt'\n",
    "save_dir = os.path.join(project_root, 'logan/results', dataset, f'{llm_model}/grounded_fact')\n",
    "os.makedirs(save_dir, exist_ok=True)  # Ensure the directory exists\n",
    "save_path = os.path.join(save_dir, f'{prompt_type}_{few_shot_txt}_{llm_model}_{time}.csv')\n",
    "\n",
    "# ids, questions = load_data_deterministic(data_path, sample_size=200)\n",
    "ids, questions = load_data_size_specific(data_path, sample_size=2)\n",
    "few_shot_prompt = load_few_shot_prompt(fewshot_prompt_path)\n",
    "\n",
    "# print(few_shot_prompt)\n",
    "# raise ValueError('stop')\n",
    "\n",
    "initialize_save_file(save_path)\n",
    "already_answered_ids = load_already_answered_ids(save_path)\n",
    "\n",
    "\n",
    "ids_answered, questions_answered, answers = query_llm(\n",
    "    llm_model=llm_model,\n",
    "    ids=ids,\n",
    "    questions=questions,\n",
    "    few_shot_prompt=few_shot_prompt,\n",
    "    prompt_type=prompt_type,\n",
    "    save_path=save_path,\n",
    "    already_answered_ids=already_answered_ids\n",
    ")\n",
    "\n",
    "print(f\"Processing complete. {len(ids_answered)} new answers saved to {save_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogiQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Add the directory containing logiqa.py to the Python path\n",
    "logiqa_path = \"/Users/log/Github/textual_grounding/data/logiqa\"\n",
    "sys.path.append(logiqa_path)\n",
    "\n",
    "# Import the LogiQA class from the logiqa module if needed\n",
    "from logiqa import LogiQA\n",
    "\n",
    "# Load the dataset using Hugging Face load_dataset method\n",
    "dataset = load_dataset('/Users/log/Github/textual_grounding/data/logiqa/logiqa.py', split='test')\n",
    "\n",
    "# Print out the first 5 examples from the test set\n",
    "for idx in range(5):\n",
    "    example = dataset[idx]\n",
    "    print(f\"Example {idx + 1}:\")\n",
    "    print(f\"Context: {example['context']}\")\n",
    "    print(f\"Query: {example['query']}\")\n",
    "    print(f\"Options: {example['options']}\")\n",
    "    print(f\"Correct Option Index: {example['correct_option']}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset (adjust the path as needed)\n",
    "dataset = load_dataset('/Users/log/Github/textual_grounding/data/logiqa/logiqa.py', split='test')\n",
    "\n",
    "# Prepare to write the first 300 examples to a JSONL file\n",
    "output_file = 'logiqa_300_examples.jsonl'\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for idx, example in enumerate(dataset):\n",
    "        if idx >= 300:\n",
    "            break\n",
    "        \n",
    "        # Create the \"question\" field by concatenating context, query, and options\n",
    "        context = example['context']\n",
    "        query = example['query']\n",
    "        options = example['options']\n",
    "        options_str = \" \".join([f\"({chr(65 + i)}) {opt}\" for i, opt in enumerate(options)])\n",
    "        question = f\"{context} {query}\\n{options_str}\"\n",
    "        \n",
    "        # Create the dictionary for the current example\n",
    "        example_dict = {\n",
    "            \"id\": idx,\n",
    "            \"question\": question,\n",
    "            \"answer\": chr(65 + example['correct_option'])  # Convert index to letter (A, B, C, D)\n",
    "        }\n",
    "        \n",
    "        # Write the example as a JSON object to the JSONL file\n",
    "        f.write(json.dumps(example_dict) + '\\n')\n",
    "\n",
    "print(f\"Saved 300 examples to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grounded Visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total QA Pairs Parsed: 2\n",
      "Total Ground Truth Entries: 220\n",
      "id: 225: 68\n",
      "id: 98: 31800\n",
      "HTML content has been successfully written to extract_adv_GSM8k_4o.html\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import json  # For handling JSONL\n",
    "import os\n",
    "\n",
    "def add_color_to_tags_new(text):\n",
    "    # Find all unique tags in the text using regex\n",
    "    tags = set(re.findall(r'<([A-Za-z]+\\d*)>', text))\n",
    "\n",
    "    # Predefined color palette\n",
    "    color_palette = [\n",
    "        'lightyellow', 'lightblue', 'lightgreen', 'lightcoral',\n",
    "        'lightcyan', 'lightpink', 'lightsalmon', 'lightgray',\n",
    "        'lightgoldenrodyellow', 'lightseagreen', 'lightskyblue',\n",
    "        'lightsteelblue',\n",
    "        'lavender', 'peachpuff', 'paleturquoise', 'wheat', 'mistyrose'\n",
    "    ]\n",
    "\n",
    "    # Dictionary to hold tag-color mapping\n",
    "    tag_color_mapping = {}\n",
    "\n",
    "    # Assign colors to tags, cycling through the color palette if necessary\n",
    "    for i, tag in enumerate(sorted(tags)):\n",
    "        color = color_palette[i % len(color_palette)]\n",
    "        tag_color_mapping[tag] = color\n",
    "\n",
    "    # Function to replace tags with styled spans including class names\n",
    "    def replace_tag(match):\n",
    "        tag = match.group(1)\n",
    "        content = match.group(2)\n",
    "        color = tag_color_mapping.get(tag, 'lightgray')  # Default color if not found\n",
    "        return f'<span class=\"{tag}\" style=\"background-color: {color}; font-weight: bold;\">{content}</span>'\n",
    "\n",
    "    # Regex to find tags and replace them with styled spans\n",
    "    tag_regex = re.compile(r'<([A-Za-z]+\\d*)>\\s*([\\s\\S]*?)\\s*</\\1>')\n",
    "\n",
    "    # Replace all tags with styled spans\n",
    "    text = tag_regex.sub(replace_tag, text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def parse_csv_file(file_path):\n",
    "    qa_pairs = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            question = row.get('question', 'No question found.').strip()\n",
    "            answer_text = row.get('answer', 'No answer found.').strip()\n",
    "            id_ = row.get('id')\n",
    "            if id_ is not None:\n",
    "                try:\n",
    "                    id_int = int(id_)\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping a row due to invalid 'id' (not an integer): {id_}\")\n",
    "                    continue\n",
    "                qa_pairs.append((id_int, question, answer_text))\n",
    "            else:\n",
    "                # Handle cases without 'id' by skipping\n",
    "                print(f\"Skipping a row due to missing 'id': {row}\")\n",
    "    return qa_pairs\n",
    "\n",
    "\n",
    "def read_ground_truth(jsonl_path):\n",
    "    ground_truth = {}\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            id_ = data.get('id')\n",
    "            answer = str(data.get('answer')).lower()\n",
    "            if id_ is not None and answer is not None:\n",
    "\n",
    "                # GSM ONLY\n",
    "                # answer = answer.split('####')[1].strip()\n",
    "                ground_truth[id_] = answer\n",
    "            else:\n",
    "                print(f\"Invalid ground truth entry: {data}\")\n",
    "    return ground_truth\n",
    "\n",
    "\n",
    "def create_highlight_html_new(qa_pairs, ground_truth):\n",
    "    html_content = \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <title>Question and Answer Highlights</title>\n",
    "        <style>\n",
    "            body {\n",
    "                font-family: Arial, sans-serif;\n",
    "                margin: 20px;\n",
    "                background-color: #f0f0f0;\n",
    "            }\n",
    "            .container {\n",
    "                background-color: #ffffff;\n",
    "                padding: 20px;\n",
    "                margin-bottom: 20px;\n",
    "                border-radius: 8px;\n",
    "                box-shadow: 0 2px 5px rgba(0,0,0,0.1);\n",
    "            }\n",
    "            .question {\n",
    "                font-size: 1.2em;\n",
    "                margin-bottom: 10px;\n",
    "            }\n",
    "            .full-response, .final-answer, .ground-truth-answer {\n",
    "                margin-bottom: 10px;\n",
    "                white-space: pre-wrap; /* Add this line to preserve newlines */\n",
    "            }\n",
    "            .final-answer {\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            .ground-truth-answer {\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            /* Styles for the highlighted spans */\n",
    "            .highlighted {\n",
    "                padding: 2px 4px;\n",
    "                border-radius: 3px;\n",
    "                display: inline-block;\n",
    "            }\n",
    "            /* Styles for the summary section */\n",
    "            .summary {\n",
    "                background-color: #e0ffe0;\n",
    "                padding: 15px;\n",
    "                border: 2px solid #00cc00;\n",
    "                border-radius: 8px;\n",
    "                font-size: 1.2em;\n",
    "                margin-top: 30px;\n",
    "            }\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "    <h1>Question and Answer Highlights</h1>\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize counters for correct and total answers\n",
    "    correct_answers = 0\n",
    "    total_answers = 0\n",
    "\n",
    "    for i, (id_, question, answer_text) in enumerate(qa_pairs, 1):\n",
    "        try:\n",
    "            full_response = answer_text.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot process answer for question ID {id_}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Apply color to tags in the full_response\n",
    "        highlighted_response = add_color_to_tags_new(full_response)\n",
    "        \n",
    "        # Replace newline characters with <br> tags to ensure they are rendered in HTML\n",
    "        highlighted_response = highlighted_response.replace('\\n', '<br>')\n",
    "\n",
    "        # Extract the final answer within curly brackets {}\n",
    "        # final_answer_match = re.search(r'\\{([^}]+)\\}', full_response)\n",
    "        final_answer_match = re.search(r'\\{([^}]+)\\}(?=[^}]*$)', full_response, re.DOTALL)\n",
    "        \n",
    "        if final_answer_match:\n",
    "            final_answer = final_answer_match.group(1).replace(',', '').replace('$', '').strip().lower()\n",
    "            if \"no\" in final_answer or \"false\" in final_answer:\n",
    "                final_answer = \"false\"\n",
    "            elif \"yes\" in final_answer or \"true\" in final_answer:\n",
    "                final_answer = \"true\"\n",
    "            \n",
    "        else:\n",
    "            final_answer = \"\"\n",
    "\n",
    "        # Retrieve ground truth answer\n",
    "        gt_answer = str(ground_truth.get(id_))\n",
    "        print(f\"id: {id_}: {gt_answer}\")\n",
    "        if gt_answer is None:\n",
    "            gt_answer_display = \"<span style='color: gray;'>Ground truth not available.</span>\"\n",
    "            is_correct = False\n",
    "        else:\n",
    "            # Normalize both final_answer and gt_answer for comparison\n",
    "            try:\n",
    "                # Attempt to convert to float for numerical comparison\n",
    "                final_answer_num = float(final_answer.replace(',', '').replace('$', ''))\n",
    "                if isinstance(gt_answer, list):\n",
    "                    # If ground truth is a list, take the first element\n",
    "                    gt_answer_num = float(gt_answer[0].replace(',', '').replace('$', ''))\n",
    "                else:\n",
    "                    gt_answer_num = float(gt_answer.replace(',', '').replace('$', ''))\n",
    "                is_correct = final_answer_num == gt_answer_num\n",
    "                # Format numbers with commas and two decimal places if needed\n",
    "                if final_answer_num.is_integer():\n",
    "                    final_answer_display = f\"{int(final_answer_num):,}\"\n",
    "                else:\n",
    "                    final_answer_display = f\"{final_answer_num:,.2f}\"\n",
    "                if gt_answer_num.is_integer():\n",
    "                    gt_answer_display = f\"{int(gt_answer_num):,}\"\n",
    "                else:\n",
    "                    gt_answer_display = f\"{gt_answer_num:,.2f}\"\n",
    "            except ValueError:\n",
    "                # In case conversion fails, fallback to string comparison\n",
    "                is_correct = final_answer == gt_answer\n",
    "                final_answer_display = final_answer\n",
    "                gt_answer_display = gt_answer\n",
    "\n",
    "        # Style the final answer based on correctness\n",
    "        if is_correct:\n",
    "            highlighted_final_answer = f\"<span style='font-size:1.1em; color: green;'>{final_answer_display}</span>\"\n",
    "            correct_answers += 1\n",
    "        else:\n",
    "            highlighted_final_answer = f\"<span style='font-size:1.1em; color: red;'>{final_answer_display}</span>\"\n",
    "        total_answers += 1\n",
    "\n",
    "        # Display ground truth answer\n",
    "        if gt_answer is not None:\n",
    "            ground_truth_html = f\"<div class='ground-truth-answer'><strong>Ground Truth Answer:</strong> {gt_answer_display}</div>\"\n",
    "        else:\n",
    "            ground_truth_html = f\"<div class='ground-truth-answer'><strong>Ground Truth Answer:</strong> Not available.</div>\"\n",
    "\n",
    "        # Build the HTML structure\n",
    "        html_content += f\"<div class='container'>\"\n",
    "        html_content += f\"<div class='question'><strong>Question:</strong> {question}</div>\"\n",
    "        html_content += f\"<div class='full-response'>{highlighted_response}</div>\"\n",
    "        html_content += f\"<div class='final-answer'><strong>Final Answer:</strong> {highlighted_final_answer}</div>\"\n",
    "        html_content += f\"{ground_truth_html}\"\n",
    "        html_content += \"</div>\\n\"\n",
    "\n",
    "    # After processing all QA pairs, add the summary section\n",
    "    summary_percentage = (correct_answers / total_answers * 100) if total_answers > 0 else 0\n",
    "    summary_html = f\"\"\"\n",
    "    <div class='summary'>\n",
    "        <strong>Summary:</strong> Correct Answers: {correct_answers} / {total_answers} ({summary_percentage:.2f}%)\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    # Close the HTML tags\n",
    "    html_content += \"\"\"\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    output_html = summary_html + html_content\n",
    "    return output_html\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Replace these paths with your actual file paths\n",
    "    # input_csv = '/Users/log/Github/textual_grounding/logan/results/StrategyQA/4o/grounded_fact/fewshot_cot.txt_4o_1017_203601.csv'  \n",
    "    input_csv = '/Users/log/Github/textual_grounding/logan/results/p_GSM8K/4o/grounded_fact/extract_facts_3shot_cot.txt_4o_1022_224021.csv'  \n",
    "    ground_truth_file = '/Users/log/Github/textual_grounding/data/p_GSM8K/test.jsonl'  # Path to the ground truth JSONL file\n",
    "    output_file = 'extract_adv_GSM8k_4o.html'  # Desired output HTML file path\n",
    "\n",
    "    # Check if input files exist\n",
    "    if not os.path.isfile(input_csv):\n",
    "        print(f\"Input CSV file not found: {input_csv}\")\n",
    "        return\n",
    "    if not os.path.isfile(ground_truth_file):\n",
    "        print(f\"Ground truth JSON file not found: {ground_truth_file}\")\n",
    "        return\n",
    "\n",
    "    # Parse the input CSV file to extract IDs, questions, and answers\n",
    "    qa_pairs = parse_csv_file(input_csv)\n",
    "    print(f\"Total QA Pairs Parsed: {len(qa_pairs)}\")  # Debug: Print the number of QA pairs parsed\n",
    "\n",
    "    # Read the ground truth answers\n",
    "    ground_truth = read_ground_truth(ground_truth_file)\n",
    "    print(f\"Total Ground Truth Entries: {len(ground_truth)}\")  # Debug: Print the number of ground truth entries\n",
    "\n",
    "    # Check if any QA pairs were found\n",
    "    if not qa_pairs:\n",
    "        print(\"No question-answer pairs were found in the input file.\")\n",
    "        return\n",
    "\n",
    "    # Generate the HTML content\n",
    "    html_content = create_highlight_html_new(qa_pairs, ground_truth)\n",
    "\n",
    "    # Write the HTML content to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(html_content)\n",
    "\n",
    "    print(f\"HTML content has been successfully written to {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoT - Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total QA Pairs Parsed: 25\n",
      "Total Ground Truth Entries: 3591\n",
      "HTML content has been successfully written to extract_facts_SPARTQA_4o.html\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_final_answer(answer_text):\n",
    "    # Regex pattern to match anything inside curly braces\n",
    "    final_answer_pattern = re.compile(r'\\{([^}]+)\\}')\n",
    "    \n",
    "    # Find all matches of text inside curly braces\n",
    "    matches = list(final_answer_pattern.finditer(answer_text))\n",
    "    \n",
    "    # If we have at least one match, get the content of the last match\n",
    "    if matches:\n",
    "        final_answer = matches[-1].group(1).strip()\n",
    "        if final_answer[-1] == '\\\\':\n",
    "            final_answer = final_answer[:-1]\n",
    "        return final_answer\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def parse_csv_file(file_path):\n",
    "    qa_pairs = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            question = row.get('question', 'No question found.').strip()\n",
    "            answer_text = row.get('answer', 'No answer found.').strip()\n",
    "            id_ = row.get('id')\n",
    "            if id_ is not None:\n",
    "                try:\n",
    "                    id_int = int(id_)\n",
    "                    qa_pairs.append((id_int, question, answer_text))\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping a row due to invalid 'id' (not an integer): {id_}\")\n",
    "            else:\n",
    "                print(f\"Skipping a row due to missing 'id': {row}\")\n",
    "    return qa_pairs\n",
    "\n",
    "def read_ground_truth(jsonl_path):\n",
    "    ground_truth = {}\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            id_ = data.get('id')\n",
    "            answer = data.get('answer')\n",
    "            if id_ is not None and answer is not None:\n",
    "                # GSM ONLY\n",
    "                # answer = answer.split('####')[1].strip()\n",
    "                ground_truth[id_] = answer\n",
    "            else:\n",
    "                print(f\"Invalid ground truth entry: {data}\")\n",
    "    return ground_truth\n",
    "\n",
    "def create_simple_html(qa_pairs, ground_truth):\n",
    "    html_content = \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <title>Question and Answer Comparison</title>\n",
    "        <style>\n",
    "            body {\n",
    "                font-family: Arial, sans-serif;\n",
    "                margin: 20px;\n",
    "                background-color: #f9f9f9;\n",
    "            }\n",
    "            .container {\n",
    "                background-color: #ffffff;\n",
    "                padding: 15px 20px;\n",
    "                margin-bottom: 15px;\n",
    "                border-radius: 6px;\n",
    "                box-shadow: 0 1px 3px rgba(0,0,0,0.1);\n",
    "            }\n",
    "            .question {\n",
    "                font-size: 1.1em;\n",
    "                margin-bottom: 10px;\n",
    "                color: #333333;\n",
    "            }\n",
    "            .answer-text {\n",
    "                background-color: #f4f4f4;\n",
    "                padding: 10px;\n",
    "                border-left: 4px solid #2196F3;\n",
    "                margin-bottom: 10px;\n",
    "                white-space: pre-wrap;\n",
    "                font-family: Consolas, \"Courier New\", monospace;\n",
    "            }\n",
    "            .final-answer, .ground-truth-answer {\n",
    "                margin-bottom: 5px;\n",
    "            }\n",
    "            .final-answer span.correct {\n",
    "                color: green;\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            .final-answer span.incorrect {\n",
    "                color: red;\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            .ground-truth-answer {\n",
    "                color: #555555;\n",
    "            }\n",
    "            .summary {\n",
    "                background-color: #e0ffe0;\n",
    "                padding: 15px;\n",
    "                border: 2px solid #00cc00;\n",
    "                border-radius: 8px;\n",
    "                font-size: 1.2em;\n",
    "                margin-top: 30px;\n",
    "            }\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "    <h1>Question and Answer Comparison</h1>\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize counters for correct and total answers\n",
    "    correct_answers = 0\n",
    "    total_answers = 0\n",
    "\n",
    "    for id_, question, answer_text in qa_pairs:\n",
    "        final_answer = extract_final_answer(answer_text)\n",
    "        gt_answer = ground_truth.get(id_)\n",
    "\n",
    "        if gt_answer is None:\n",
    "            gt_answer_display = \"<span style='color: gray;'>Ground truth not available.</span>\"\n",
    "            is_correct = False\n",
    "        else:\n",
    "            # Normalize both final_answer and gt_answer for comparison\n",
    "            try:\n",
    "                final_answer_num = float(final_answer.replace(',', '').replace('$', ''))\n",
    "                if isinstance(gt_answer, list):  # Handle list of answers if applicable\n",
    "                    gt_answer_num = float(gt_answer[0].replace(',', '').replace('$', ''))\n",
    "                else:\n",
    "                    gt_answer_num = float(str(gt_answer).replace(',', '').replace('$', ''))\n",
    "                is_correct = final_answer_num == gt_answer_num\n",
    "                # Format numbers with commas and two decimal places if needed\n",
    "                final_answer_display = f\"{final_answer_num:,.2f}\" if not final_answer_num.is_integer() else f\"{int(final_answer_num):,}\"\n",
    "                gt_answer_display = f\"{gt_answer_num:,.2f}\" if not gt_answer_num.is_integer() else f\"{int(gt_answer_num):,}\"\n",
    "            except (ValueError, TypeError):\n",
    "                # Fallback to string comparison if conversion fails\n",
    "                is_correct = final_answer.strip().lower() == str(gt_answer).strip().lower()\n",
    "                final_answer_display = final_answer\n",
    "                gt_answer_display = gt_answer\n",
    "\n",
    "        # Style the final answer based on correctness\n",
    "        if is_correct:\n",
    "            final_answer_html = f\"<span class='correct'>{final_answer_display}</span>\"\n",
    "            correct_answers += 1\n",
    "        else:\n",
    "            final_answer_html = f\"<span class='incorrect'>{final_answer_display}</span>\"\n",
    "        total_answers += 1\n",
    "\n",
    "        # Display ground truth answer\n",
    "        if gt_answer is not None:\n",
    "            ground_truth_html = f\"<div class='ground-truth-answer'><strong>Ground Truth Answer:</strong> {gt_answer_display}</div>\"\n",
    "        else:\n",
    "            ground_truth_html = f\"<div class='ground-truth-answer'><strong>Ground Truth Answer:</strong> Not available.</div>\"\n",
    "\n",
    "        # Build the HTML structure for each QA pair\n",
    "        html_content += f\"<div class='container'>\"\n",
    "        html_content += f\"<div class='question'><strong>Question:</strong> {question}</div>\"\n",
    "        html_content += f\"<div class='answer-text'><strong>Model Response:</strong><br>{answer_text}</div>\"\n",
    "        html_content += f\"<div class='final-answer'><strong>Final Answer:</strong> {final_answer_html}</div>\"\n",
    "        html_content += f\"{ground_truth_html}\"\n",
    "        html_content += \"</div>\\n\"\n",
    "\n",
    "    # Add the summary section\n",
    "    summary_percentage = (correct_answers / total_answers * 100) if total_answers > 0 else 0\n",
    "    summary_html = f\"\"\"\n",
    "    <div class='summary'>\n",
    "        <strong>Summary:</strong> Correct Answers: {correct_answers} / {total_answers} ({summary_percentage:.2f}%)\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    # html_content += summary_html\n",
    "\n",
    "    # Close the HTML tags\n",
    "    html_content += \"\"\"\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    final_content = summary_html + html_content\n",
    "    return final_content\n",
    "\n",
    "def main():\n",
    "    input_csv = '/Users/log/Github/textual_grounding/logan/results/SPARTQA/4o/grounded_fact/extract_facts_1_shot_cot.txt_4o_1022_002130.csv'  \n",
    "    ground_truth_file = '/Users/log/Github/textual_grounding/data/SPARTQA/test.jsonl'  # Path to the ground truth JSONL file\n",
    "    output_file = 'extract_facts_SPARTQA_4o.html'  # Desired output HTML file path\n",
    "\n",
    "    # Check if input files exist\n",
    "    if not os.path.isfile(input_csv):\n",
    "        print(f\"Input CSV file not found: {input_csv}\")\n",
    "        return\n",
    "    if not os.path.isfile(ground_truth_file):\n",
    "        print(f\"Ground truth JSONL file not found: {ground_truth_file}\")\n",
    "        return\n",
    "\n",
    "    # Parse the input CSV file to extract IDs, questions, and answers\n",
    "    qa_pairs = parse_csv_file(input_csv)\n",
    "    print(f\"Total QA Pairs Parsed: {len(qa_pairs)}\")\n",
    "\n",
    "    # Read the ground truth answers\n",
    "    ground_truth = read_ground_truth(ground_truth_file)\n",
    "    print(f\"Total Ground Truth Entries: {len(ground_truth)}\")\n",
    "\n",
    "    # Check if any QA pairs were found\n",
    "    if not qa_pairs:\n",
    "        print(\"No question-answer pairs were found in the input file.\")\n",
    "        return\n",
    "\n",
    "    # Generate the HTML content\n",
    "    html_content = create_simple_html(qa_pairs, ground_truth)\n",
    "\n",
    "    # Write the HTML content to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(html_content)\n",
    "\n",
    "    print(f\"HTML content has been successfully written to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"tasksource/spartqa-mchoice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total QA Pairs Parsed: 200\n",
      "No ground truth answer found for ID 489\n",
      "No ground truth answer found for ID 1113\n",
      "Total Ground Truth Entries: 1317\n",
      "\n",
      "===== Analysis Statistics =====\n",
      "\n",
      "Total Responses Analyzed: 200\n",
      "Responses with Final Answer in Curly Brackets: 136 (68.00%)\n",
      "Responses without Final Answer in Curly Brackets: 64 (32.00%)\n",
      "Responses with Ground Truth Available: 200 (100.00%)\n",
      "Correct Answers: 94\n",
      "Incorrect Answers: 106\n",
      "Accuracy: 47.00%\n",
      "Responses without Ground Truth: 0\n",
      "\n",
      "----- Tag Statistics -----\n",
      "Total Tags Found: 501\n",
      "Average Number of Tags per Response: 2.50\n",
      "Average Length of Tag Content: 8.68 characters\n",
      "--------------------------\n",
      "\n",
      "===== End of Statistics =====\n",
      "\n",
      "Statistics analysis completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import json  # For handling JSONL\n",
    "import os\n",
    "\n",
    "def extract_parts_regular_cot(answer_text):\n",
    "    # Attempt to extract Final Answer from 'Final Answer:'\n",
    "    final_match = re.search(r'Final Answer:\\s*(\\S+)', answer_text, re.IGNORECASE)\n",
    "    if final_match and final_match.group(1).strip():\n",
    "        final_answer = final_match.group(1).strip()\n",
    "        has_curly = False\n",
    "    else:\n",
    "        # Fallback: Extract Final Answer from '{...}' in the reasoning\n",
    "        curly_match = re.search(r'\\{([\\d.]+)\\}', answer_text)\n",
    "        final_answer = curly_match.group(1).strip() if curly_match else \"\"\n",
    "        has_curly = bool(curly_match)\n",
    "\n",
    "    return answer_text.strip(), final_answer, has_curly\n",
    "\n",
    "def parse_csv_file(file_path):\n",
    "    qa_pairs = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            question = row.get('question', 'No question found.').strip()\n",
    "            answer_text = row.get('answer', 'No answer found.').strip()\n",
    "            id_ = row.get('id')\n",
    "            if id_ is not None:\n",
    "                try:\n",
    "                    id_int = int(id_)\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping a row due to invalid 'id' (not an integer): {id_}\")\n",
    "                    continue\n",
    "                qa_pairs.append((id_int, question, answer_text))\n",
    "            else:\n",
    "                # Handle cases without 'id' by skipping\n",
    "                print(f\"Skipping a row due to missing 'id': {row}\")\n",
    "    return qa_pairs\n",
    "\n",
    "def read_ground_truth(jsonl_path):\n",
    "    ground_truth = {}\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            id_ = data.get('id')\n",
    "            answer = data.get('answer')\n",
    "            if id_ is not None and answer is not None:\n",
    "                # Extract the last number or text after '####'\n",
    "                match = re.search(r'####\\s*([\\d.]+)', answer)\n",
    "                if match:\n",
    "                    ground_truth[id_] = match.group(1).strip()\n",
    "                else:\n",
    "                    print(f\"No ground truth answer found for ID {id_}\")\n",
    "            else:\n",
    "                print(f\"Invalid ground truth entry: {data}\")\n",
    "    return ground_truth\n",
    "\n",
    "def create_statistics(qa_pairs, ground_truth):\n",
    "    total_responses = len(qa_pairs)\n",
    "    responses_with_curly = 0\n",
    "    responses_without_curly = 0\n",
    "    correct_answers = 0\n",
    "    incorrect_answers = 0\n",
    "    no_ground_truth = 0\n",
    "\n",
    "    # Variables for tag statistics\n",
    "    total_tags = 0\n",
    "    total_tag_length = 0\n",
    "    tag_counts = []  # List to store number of tags per response\n",
    "    tag_lengths = []  # List to store lengths of tag content across all responses\n",
    "\n",
    "    for id_, question, answer_text in qa_pairs:\n",
    "        try:\n",
    "            answer_reasoning, final_answer, has_curly = extract_parts_regular_cot(answer_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot extract parts for question ID {id_}: {e}\")\n",
    "            continue\n",
    "\n",
    "        if has_curly:\n",
    "            responses_with_curly += 1\n",
    "        else:\n",
    "            responses_without_curly += 1\n",
    "\n",
    "        # Extract tags and their content\n",
    "        tags_in_response = re.findall(r'<([A-Za-z]+\\d*)>(.*?)</\\1>', answer_text)\n",
    "        number_of_tags = len(tags_in_response)\n",
    "        tag_counts.append(number_of_tags)\n",
    "        total_tags += number_of_tags\n",
    "\n",
    "        for tag, content in tags_in_response:\n",
    "            content_length = len(content)\n",
    "            tag_lengths.append(content_length)\n",
    "            total_tag_length += content_length\n",
    "\n",
    "        # Retrieve ground truth answer\n",
    "        gt_answer = ground_truth.get(id_)\n",
    "        if gt_answer is None:\n",
    "            no_ground_truth += 1\n",
    "            continue\n",
    "\n",
    "        # Compare final_answer with ground truth\n",
    "        if final_answer == gt_answer:\n",
    "            correct_answers += 1\n",
    "        else:\n",
    "            incorrect_answers += 1\n",
    "\n",
    "    # Calculate additional metrics\n",
    "    accuracy_percentage = (correct_answers / (correct_answers + incorrect_answers) * 100) if (correct_answers + incorrect_answers) > 0 else 0\n",
    "    curly_percentage = (responses_with_curly / total_responses * 100) if total_responses > 0 else 0\n",
    "    no_curly_percentage = (responses_without_curly / total_responses * 100) if total_responses > 0 else 0\n",
    "    ground_truth_available = total_responses - no_ground_truth\n",
    "    ground_truth_available_percentage = (ground_truth_available / total_responses * 100) if total_responses > 0 else 0\n",
    "\n",
    "    # Calculate tag statistics\n",
    "    average_tags_per_response = (total_tags / total_responses) if total_responses > 0 else 0\n",
    "    average_tag_length = (total_tag_length / total_tags) if total_tags > 0 else 0\n",
    "\n",
    "    # Print the statistics\n",
    "    print(\"\\n===== Analysis Statistics =====\\n\")\n",
    "    print(f\"Total Responses Analyzed: {total_responses}\")\n",
    "    print(f\"Responses with Final Answer in Curly Brackets: {responses_with_curly} ({curly_percentage:.2f}%)\")\n",
    "    print(f\"Responses without Final Answer in Curly Brackets: {responses_without_curly} ({no_curly_percentage:.2f}%)\")\n",
    "    print(f\"Responses with Ground Truth Available: {ground_truth_available} ({ground_truth_available_percentage:.2f}%)\")\n",
    "    print(f\"Correct Answers: {correct_answers}\")\n",
    "    print(f\"Incorrect Answers: {incorrect_answers}\")\n",
    "    print(f\"Accuracy: {accuracy_percentage:.2f}%\")\n",
    "    print(f\"Responses without Ground Truth: {no_ground_truth}\")\n",
    "\n",
    "    # Tag Statistics\n",
    "    print(\"\\n----- Tag Statistics -----\")\n",
    "    print(f\"Total Tags Found: {total_tags}\")\n",
    "    print(f\"Average Number of Tags per Response: {average_tags_per_response:.2f}\")\n",
    "    print(f\"Average Length of Tag Content: {average_tag_length:.2f} characters\")\n",
    "    print(\"--------------------------\\n\")\n",
    "    print(\"===== End of Statistics =====\\n\")\n",
    "\n",
    "def main():\n",
    "    input_csv = '/Users/log/Github/textual_grounding/logan/results/GSM8K/llama/mermaid/mermaid_get_answer_llama3.1_20240926_215344.csv'  # Replace with your input CSV file path\n",
    "    ground_truth_file = '/Users/log/Github/textual_grounding/data/GSM8K/test.jsonl'  # Path to the ground truth JSONL file\n",
    "\n",
    "    # Check if input files exist\n",
    "    if not os.path.isfile(input_csv):\n",
    "        print(f\"Input CSV file not found: {input_csv}\")\n",
    "        return\n",
    "    if not os.path.isfile(ground_truth_file):\n",
    "        print(f\"Ground truth JSONL file not found: {ground_truth_file}\")\n",
    "        return\n",
    "\n",
    "    # Parse the input CSV file to extract IDs, questions, and answers\n",
    "    qa_pairs = parse_csv_file(input_csv)\n",
    "    print(f\"Total QA Pairs Parsed: {len(qa_pairs)}\")  # Debug: Print the number of QA pairs parsed\n",
    "\n",
    "    # Read the ground truth answers\n",
    "    ground_truth = read_ground_truth(ground_truth_file)\n",
    "    print(f\"Total Ground Truth Entries: {len(ground_truth)}\")  # Debug: Print the number of ground truth entries\n",
    "\n",
    "    # Check if any QA pairs were found\n",
    "    if not qa_pairs:\n",
    "        print(\"No question-answer pairs were found in the input file.\")\n",
    "        return\n",
    "\n",
    "    # Generate and print the statistics\n",
    "    create_statistics(qa_pairs, ground_truth)\n",
    "\n",
    "    print(\"Statistics analysis completed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 'prompt' is a list in entry ID 110. Attempting to join into a string.\n",
      "Warning: 'prompt' is a list in entry ID 111. Attempting to join into a string.\n",
      "Warning: 'prompt' is a list in entry ID 112. Attempting to join into a string.\n",
      "Warning: 'prompt' is a list in entry ID 113. Attempting to join into a string.\n",
      "JSON file has been updated successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Define the path to your JSON file\n",
    "input_file = '/Users/log/Github/textual_grounding/data/AIW/test.json'\n",
    "\n",
    "# Load the JSON data from the file\n",
    "with open(input_file, 'r', encoding='utf-8') as file:\n",
    "    try:\n",
    "        data = json.load(file)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "# Process each entry in the JSON data\n",
    "for idx, entry in enumerate(data):\n",
    "    # Get the current prompt\n",
    "    prompt = entry.get('prompt', '')\n",
    "\n",
    "    # Check if prompt is a string\n",
    "    if isinstance(prompt, str):\n",
    "        delimiter = 'have?'\n",
    "        index = prompt.find(delimiter)\n",
    "\n",
    "        if index != -1:\n",
    "            # Truncate the prompt after \"have?\"\n",
    "            truncated_prompt = prompt[:index + len(delimiter)]\n",
    "            entry['prompt'] = truncated_prompt\n",
    "        else:\n",
    "            print(f\"Warning: 'have?' not found in prompt of entry ID {entry.get('id', 'Unknown')}. Prompt left unchanged.\")\n",
    "    elif isinstance(prompt, list):\n",
    "        print(f\"Warning: 'prompt' is a list in entry ID {entry.get('id', 'Unknown')}. Attempting to join into a string.\")\n",
    "        # Attempt to join the list into a single string\n",
    "        joined_prompt = ' '.join(str(item) for item in prompt)\n",
    "        delimiter = 'have?'\n",
    "        index = joined_prompt.find(delimiter)\n",
    "\n",
    "        if index != -1:\n",
    "            truncated_prompt = joined_prompt[:index + len(delimiter)]\n",
    "            entry['prompt'] = truncated_prompt\n",
    "        else:\n",
    "            print(f\"Warning: 'have?' not found after joining prompt in entry ID {entry.get('id', 'Unknown')}. Prompt left unchanged.\")\n",
    "    else:\n",
    "        print(f\"Warning: 'prompt' is neither a string nor a list in entry ID {entry.get('id', 'Unknown')}. Prompt left unchanged.\")\n",
    "\n",
    "    # Rename 'right_answer' to 'answer' if it exists\n",
    "    if 'right_answer' in entry:\n",
    "        entry['answer'] = entry.pop('right_answer')\n",
    "\n",
    "# Save the updated data back to the same JSON file\n",
    "with open(input_file, 'w', encoding='utf-8') as file:\n",
    "    json.dump(data, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"JSON file has been updated successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
